{"cells":[{"cell_type":"code","execution_count":null,"id":"6d30e714","metadata":{"tags":[],"id":"6d30e714"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import random,os\n","import torch.utils.data as Data\n","from sklearn.metrics import f1_score,recall_score,precision_score,roc_curve,auc,accuracy_score,confusion_matrix,r2_score, mean_squared_error\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import torch\n","# import torch.nn as nn\n","from torch import nn,optim\n","import torch.nn.functional as F\n","from torch.optim.optimizer import Optimizer, required\n","import gc\n","import time\n","import math\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"id":"f02da321","metadata":{"id":"f02da321"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"02360f62","metadata":{"tags":[],"id":"02360f62"},"outputs":[],"source":["'''ResNet in PyTorch.\n","\n","For Pre-activation ResNet, see 'preact_resnet.py'.\n","\n","Reference:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","'''\n","\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","        def __init__(self, in_planes, planes, stride=2):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv1d(in_planes, planes, kernel_size=5, stride=stride, padding=2, bias=False)\n","        self.bn1 = nn.BatchNorm1d(planes)\n","        self.conv2 = nn.Conv1d(planes, planes, kernel_size=5, stride=1, padding=2, bias=False)\n","        self.bn2 = nn.BatchNorm1d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion * planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv1d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm1d(self.expansion * planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        x1 = self.shortcut(x)\n","        if x1.shape[-1] != out.shape[-1]:\n","            x1 = x1[:, :, :out.shape[-1]]\n","        out += x1\n","        out = F.relu(out)\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv1d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm1d(planes)\n","        self.conv2 = nn.Conv1d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm1d(planes)\n","        self.conv3 = nn.Conv1d(planes, self.expansion * planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm1d(self.expansion * planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion * planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv1d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm1d(self.expansion * planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=5):\n","        super(ResNet, self).__init__()\n","        self.input_channel = 1\n","        self.in_planes = 64\n","\n","        self.conv1 = nn.Conv1d(self.input_channel, self.in_planes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm1d(self.in_planes)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=2)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512 * block.expansion, num_classes)\n","        self.dropout = nn.Dropout(0.2)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1] * (num_blocks - 1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.adaptive_avg_pool1d(out, 1).view(out.size(0), -1)\n","        out = self.dropout(out)\n","        out = self.linear(out)\n","        return out\n","\n","    def reg_loss(self):\n","        reg_loss = 0.0\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv1d):\n","                reg_loss += torch.sum(torch.abs(m.weight))\n","        return reg_loss\n","\n","    def l1reg_loss(self):\n","        reg_loss = 0.0\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv1d):\n","                reg_loss += torch.sum(torch.abs(m.weight))\n","        return reg_loss\n","\n","    def l12reg_loss(self):\n","        reg_loss = 0.0\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv1d):\n","                reg_loss += torch.sum((torch.abs(m.weight) + 1e-6).sqrt())\n","        return reg_loss\n","\n","    def l23reg_loss(self):\n","        reg_loss = 0.0\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv1d):\n","                reg_loss += torch.sum((torch.abs(m.weight) + 1e-6).pow(2 / 3))\n","        return reg_loss\n","\n","    def exact_sparsity(self):\n","        nnz = 0\n","        total_param = 0.0\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv1d):\n","                total_param += np.prod(m.weight.data.shape)\n","                nnz += torch.sum(m.weight.data != 0).detach().item()\n","        ratio = nnz / total_param\n","        return ratio\n","\n","    def sparsity_level(self):\n","        nnz_2 = 0.0\n","        nnz_3 = 0.0\n","        nnz_4 = 0.0\n","        total_param = 0.0\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv1d):\n","                total_param += np.prod(m.weight.data.shape)\n","                nnz_2 += torch.sum(m.weight.data.abs() >= 0.01).detach().item()\n","                nnz_3 += torch.sum(m.weight.data.abs() >= 0.001).detach().item()\n","                nnz_4 += torch.sum(m.weight.data.abs() >= 0.0001).detach().item()\n","        ratio_2 = nnz_2 / total_param\n","        ratio_3 = nnz_3 / total_param\n","        ratio_4 = nnz_4 / total_param\n","        return ratio_2, ratio_3\n","\n","def ResNet18():\n","    return ResNet(BasicBlock, [2, 2, 2, 2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3, 4, 6, 3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3, 4, 6, 3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3, 4, 23, 3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3, 8, 36, 3])\n","\n","def test():\n","    net = ResNet18()\n","    y = net(torch.randn(1, 1, 1000))  # Example input for 1D genomic data with 1000 features\n","    print(y.size())\n"]},{"cell_type":"code","execution_count":null,"id":"218a2306-0631-4c66-a17f-cdac76369d08","metadata":{"scrolled":true,"tags":[],"id":"218a2306-0631-4c66-a17f-cdac76369d08","outputId":"cd2996c1-71ac-4cc0-a915-e10b38d0b1a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["1 torch.Size([35, 64, 263, 263])\n","2 torch.Size([35, 64, 131, 131])\n","3 torch.Size([35, 64, 131, 131])\n","1 torch.Size([35, 64, 129, 129])\n","2 torch.Size([35, 64, 127, 127])\n","3 torch.Size([35, 64, 127, 127])\n","1 torch.Size([35, 128, 63, 63])\n","2 torch.Size([35, 128, 31, 31])\n","3 torch.Size([35, 128, 31, 31])\n","1 torch.Size([35, 128, 29, 29])\n","2 torch.Size([35, 128, 27, 27])\n","3 torch.Size([35, 128, 27, 27])\n","1 torch.Size([35, 256, 13, 13])\n","2 torch.Size([35, 256, 6, 6])\n","3 torch.Size([35, 256, 6, 6])\n","1 torch.Size([35, 256, 4, 4])\n","2 torch.Size([35, 256, 2, 2])\n","3 torch.Size([35, 256, 2, 2])\n"]},{"data":{"text/plain":["tensor([[ 6.0897e-01,  3.4467e-01,  1.4449e+00,  1.3051e-01, -3.9126e-01],\n","        [ 6.1392e-01, -2.9817e-01,  2.2365e+00, -7.5860e-01, -4.7162e-01],\n","        [ 1.4457e-01, -7.9887e-01,  3.5700e-01, -2.2242e-02,  1.0272e+00],\n","        [ 4.9928e-01, -1.1885e+00,  1.6506e+00, -4.1080e-01, -2.3591e-01],\n","        [ 7.5289e-02, -1.4801e+00,  1.6055e+00, -1.9498e-01,  1.1269e+00],\n","        [ 7.9856e-01, -3.9796e-01,  1.8062e+00, -3.6544e-01,  5.9264e-01],\n","        [ 1.2730e+00, -5.9346e-01,  2.9772e-01, -2.0840e-01, -7.4229e-02],\n","        [ 3.0167e-01, -1.3968e+00,  1.8140e+00,  3.3971e-01,  1.2598e+00],\n","        [ 1.2760e+00, -5.6288e-01,  8.1168e-01, -8.6617e-01,  2.2294e-01],\n","        [-5.8046e-01, -9.0921e-01,  1.5764e+00, -1.2824e+00,  3.0161e-01],\n","        [ 3.8200e-01, -1.3019e+00,  5.2936e-01, -8.9636e-01,  1.5326e+00],\n","        [ 6.4161e-01, -2.2584e-01,  2.0175e+00, -2.5758e-01,  9.9670e-01],\n","        [ 9.8744e-01,  3.0334e-02,  1.4274e+00, -1.0625e-01,  2.8331e-01],\n","        [ 9.9430e-01, -1.7786e-01,  1.7381e+00, -4.3652e-01, -1.6162e-01],\n","        [ 3.4583e-01,  5.2585e-01,  9.7142e-01, -2.9351e-01,  8.7729e-01],\n","        [ 1.5884e+00, -6.5217e-01,  7.6794e-01, -1.1847e-01,  1.3200e-01],\n","        [-4.0133e-01, -9.3969e-02,  3.3193e-01,  5.5493e-01,  8.3425e-01],\n","        [ 8.0707e-01, -8.1773e-01,  7.6738e-01, -5.2194e-01, -2.7700e-01],\n","        [-5.6927e-02, -1.3484e+00,  1.6408e+00, -6.4157e-01, -2.4746e-01],\n","        [-3.9857e-01, -3.3234e-01,  4.0071e-01,  8.8527e-04,  8.3692e-02],\n","        [ 8.5016e-01, -7.1939e-01,  1.5589e+00, -1.9841e-02,  1.9532e-01],\n","        [ 7.5767e-02, -2.1814e-01,  1.8047e+00, -1.0190e+00, -2.3087e-02],\n","        [ 9.6731e-01, -9.3892e-01,  2.0516e+00, -9.0322e-01, -3.2023e-01],\n","        [ 1.1845e+00, -6.7182e-01,  1.8253e-01,  2.3000e-01,  5.6577e-01],\n","        [ 5.8224e-01, -3.8206e-01,  1.1709e+00, -6.2269e-01,  3.4378e-01],\n","        [-2.8242e-03, -6.5237e-01,  1.0071e+00, -4.9835e-01,  4.8234e-01],\n","        [ 2.2896e-01, -1.3574e+00,  1.5121e+00, -1.2525e+00,  8.3128e-01],\n","        [-2.1731e-01, -3.9133e-01,  7.7354e-01, -3.0282e-01,  3.5240e-01],\n","        [ 2.4706e-01, -3.1905e-01,  2.4244e+00,  3.7196e-01,  2.2466e-01],\n","        [-4.5427e-01, -7.9367e-01,  1.2811e+00, -4.0816e-01,  1.4569e-01],\n","        [ 7.4417e-01, -1.2039e+00,  1.2974e+00, -1.3837e-02, -6.2159e-01],\n","        [-2.4362e-01, -8.9930e-01,  1.0335e+00, -1.2620e+00, -2.4682e-01],\n","        [-6.4108e-02, -1.1708e+00,  1.3774e+00, -9.1671e-01,  8.1321e-01],\n","        [ 1.2081e+00, -7.7354e-01,  1.8690e+00, -3.9724e-01,  8.7860e-01],\n","        [-2.7706e-02, -1.1411e+00,  7.7978e-01,  4.9194e-02,  8.2470e-01]],\n","       grad_fn=<AddmmBackward0>)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["net = ResNet18()\n","net(torch.rand([35,1,529,529]))"]},{"cell_type":"code","execution_count":null,"id":"dbf65941","metadata":{"tags":[],"id":"dbf65941","outputId":"a665db09-b169-464e-9670-4ebb5908cf6e"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_27884/1669737403.py:3: DtypeWarning: Columns (0,1,2,3,4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv(path_name,header=None)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>52839</th>\n","      <th>52840</th>\n","      <th>52841</th>\n","      <th>52842</th>\n","      <th>52843</th>\n","      <th>52844</th>\n","      <th>52845</th>\n","      <th>52846</th>\n","      <th>52847</th>\n","      <th>52848</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ID</td>\n","      <td>t1</td>\n","      <td>t2</td>\n","      <td>t3</td>\n","      <td>t4</td>\n","      <td>t5</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>...</td>\n","      <td>52834.0</td>\n","      <td>52835.0</td>\n","      <td>52836.0</td>\n","      <td>52837.0</td>\n","      <td>52838.0</td>\n","      <td>52839.0</td>\n","      <td>52840.0</td>\n","      <td>52841.0</td>\n","      <td>52842.0</td>\n","      <td>52843.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1136</td>\n","      <td>-0.8687035</td>\n","      <td>0.28158495</td>\n","      <td>1.71659645</td>\n","      <td>-0.8861064</td>\n","      <td>47.6114438</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>...</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1318</td>\n","      <td>0.13144417</td>\n","      <td>-1.7468354</td>\n","      <td>0.95272593</td>\n","      <td>0.15843581</td>\n","      <td>-41.940212</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1319</td>\n","      <td>-0.8536013</td>\n","      <td>-2.9968045</td>\n","      <td>-0.0871332</td>\n","      <td>-1.2177578</td>\n","      <td>100.653792</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1321</td>\n","      <td>0.03416785</td>\n","      <td>-0.320321</td>\n","      <td>-0.071189</td>\n","      <td>-1.4672803</td>\n","      <td>13.8808255</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2310</th>\n","      <td>6466</td>\n","      <td>0.045294</td>\n","      <td>1.386794</td>\n","      <td>2.291255</td>\n","      <td>0.226449</td>\n","      <td>-23.831346</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2311</th>\n","      <td>6467</td>\n","      <td>-1.199029</td>\n","      <td>-0.565144</td>\n","      <td>2.225259</td>\n","      <td>-5.672833</td>\n","      <td>85.793067</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2312</th>\n","      <td>6469</td>\n","      <td>-0.964449</td>\n","      <td>0.673534</td>\n","      <td>1.754374</td>\n","      <td>-1.489569</td>\n","      <td>-8.871632</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2313</th>\n","      <td>6471</td>\n","      <td>-0.832078</td>\n","      <td>-1.40681</td>\n","      <td>2.368887</td>\n","      <td>-1.517962</td>\n","      <td>51.492272</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2314</th>\n","      <td>6473</td>\n","      <td>0.340885</td>\n","      <td>-0.195353</td>\n","      <td>0.372499</td>\n","      <td>-1.375068</td>\n","      <td>114.904649</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2315 rows × 52849 columns</p>\n","</div>"],"text/plain":["     0           1           2           3           4           5      6      \\\n","0       ID          t1          t2          t3          t4          t5    1.0   \n","1     1136  -0.8687035  0.28158495  1.71659645  -0.8861064  47.6114438    1.0   \n","2     1318  0.13144417  -1.7468354  0.95272593  0.15843581  -41.940212    1.0   \n","3     1319  -0.8536013  -2.9968045  -0.0871332  -1.2177578  100.653792    1.0   \n","4     1321  0.03416785   -0.320321   -0.071189  -1.4672803  13.8808255    1.0   \n","...    ...         ...         ...         ...         ...         ...    ...   \n","2310  6466    0.045294    1.386794    2.291255    0.226449  -23.831346    0.0   \n","2311  6467   -1.199029   -0.565144    2.225259   -5.672833   85.793067    1.0   \n","2312  6469   -0.964449    0.673534    1.754374   -1.489569   -8.871632    2.0   \n","2313  6471   -0.832078    -1.40681    2.368887   -1.517962   51.492272    1.0   \n","2314  6473    0.340885   -0.195353    0.372499   -1.375068  114.904649    0.0   \n","\n","      7      8      9      ...    52839    52840    52841    52842    52843  \\\n","0       2.0    3.0    4.0  ...  52834.0  52835.0  52836.0  52837.0  52838.0   \n","1       2.0    1.0    2.0  ...      2.0      1.0      1.0      2.0      1.0   \n","2       2.0    1.0    0.0  ...      0.0      1.0      1.0      2.0      2.0   \n","3       2.0    1.0    1.0  ...      0.0      1.0      1.0      2.0      0.0   \n","4       2.0    0.0    0.0  ...      1.0      1.0      1.0      2.0      1.0   \n","...     ...    ...    ...  ...      ...      ...      ...      ...      ...   \n","2310    1.0    2.0    1.0  ...      0.0      0.0      1.0      0.0      2.0   \n","2311    2.0    1.0    2.0  ...      1.0      1.0      2.0      0.0      0.0   \n","2312    2.0    2.0    1.0  ...      0.0      1.0      1.0      1.0      1.0   \n","2313    1.0    2.0    1.0  ...      1.0      1.0      2.0      1.0      1.0   \n","2314    2.0    2.0    1.0  ...      1.0      0.0      1.0      1.0      1.0   \n","\n","        52844    52845    52846    52847    52848  \n","0     52839.0  52840.0  52841.0  52842.0  52843.0  \n","1         1.0      0.0      2.0      0.0      1.0  \n","2         2.0      1.0      2.0      1.0      2.0  \n","3         2.0      1.0      2.0      1.0      2.0  \n","4         2.0      1.0      2.0      1.0      2.0  \n","...       ...      ...      ...      ...      ...  \n","2310      2.0      0.0      2.0      1.0      2.0  \n","2311      2.0      0.0      2.0      2.0      2.0  \n","2312      2.0      1.0      2.0      2.0      0.0  \n","2313      2.0      0.0      2.0      0.0      2.0  \n","2314      2.0      2.0      2.0      0.0      2.0  \n","\n","[2315 rows x 52849 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["path1='/root/ResNet/'\n","path_name = os.path.join(path1,'pig_data.csv')\n","df = pd.read_csv(path_name,header=None)\n","df"]},{"cell_type":"code","execution_count":null,"id":"8a69809d-5ae5-46a8-aefa-903f085df1a7","metadata":{"tags":[],"id":"8a69809d-5ae5-46a8-aefa-903f085df1a7","outputId":"2ab17702-ebff-46cb-b55d-4c3f0b1b2ea2"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>t1</th>\n","      <th>t2</th>\n","      <th>t3</th>\n","      <th>t4</th>\n","      <th>t5</th>\n","      <th>1.0</th>\n","      <th>2.0</th>\n","      <th>3.0</th>\n","      <th>4.0</th>\n","      <th>...</th>\n","      <th>52834.0</th>\n","      <th>52835.0</th>\n","      <th>52836.0</th>\n","      <th>52837.0</th>\n","      <th>52838.0</th>\n","      <th>52839.0</th>\n","      <th>52840.0</th>\n","      <th>52841.0</th>\n","      <th>52842.0</th>\n","      <th>52843.0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1136</td>\n","      <td>-0.8687035</td>\n","      <td>0.28158495</td>\n","      <td>1.71659645</td>\n","      <td>-0.8861064</td>\n","      <td>47.6114438</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>...</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1318</td>\n","      <td>0.13144417</td>\n","      <td>-1.7468354</td>\n","      <td>0.95272593</td>\n","      <td>0.15843581</td>\n","      <td>-41.940212</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1319</td>\n","      <td>-0.8536013</td>\n","      <td>-2.9968045</td>\n","      <td>-0.0871332</td>\n","      <td>-1.2177578</td>\n","      <td>100.653792</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1321</td>\n","      <td>0.03416785</td>\n","      <td>-0.320321</td>\n","      <td>-0.071189</td>\n","      <td>-1.4672803</td>\n","      <td>13.8808255</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1322</td>\n","      <td>0.57391141</td>\n","      <td>-2.0458374</td>\n","      <td>0.054743</td>\n","      <td>0.16402708</td>\n","      <td>-58.738674</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2309</th>\n","      <td>6466</td>\n","      <td>0.045294</td>\n","      <td>1.386794</td>\n","      <td>2.291255</td>\n","      <td>0.226449</td>\n","      <td>-23.831346</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2310</th>\n","      <td>6467</td>\n","      <td>-1.199029</td>\n","      <td>-0.565144</td>\n","      <td>2.225259</td>\n","      <td>-5.672833</td>\n","      <td>85.793067</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2311</th>\n","      <td>6469</td>\n","      <td>-0.964449</td>\n","      <td>0.673534</td>\n","      <td>1.754374</td>\n","      <td>-1.489569</td>\n","      <td>-8.871632</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2312</th>\n","      <td>6471</td>\n","      <td>-0.832078</td>\n","      <td>-1.40681</td>\n","      <td>2.368887</td>\n","      <td>-1.517962</td>\n","      <td>51.492272</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2313</th>\n","      <td>6473</td>\n","      <td>0.340885</td>\n","      <td>-0.195353</td>\n","      <td>0.372499</td>\n","      <td>-1.375068</td>\n","      <td>114.904649</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2314 rows × 52849 columns</p>\n","</div>"],"text/plain":["0       ID          t1          t2          t3          t4          t5  1.0  \\\n","0     1136  -0.8687035  0.28158495  1.71659645  -0.8861064  47.6114438  1.0   \n","1     1318  0.13144417  -1.7468354  0.95272593  0.15843581  -41.940212  1.0   \n","2     1319  -0.8536013  -2.9968045  -0.0871332  -1.2177578  100.653792  1.0   \n","3     1321  0.03416785   -0.320321   -0.071189  -1.4672803  13.8808255  1.0   \n","4     1322  0.57391141  -2.0458374    0.054743  0.16402708  -58.738674  2.0   \n","...    ...         ...         ...         ...         ...         ...  ...   \n","2309  6466    0.045294    1.386794    2.291255    0.226449  -23.831346  0.0   \n","2310  6467   -1.199029   -0.565144    2.225259   -5.672833   85.793067  1.0   \n","2311  6469   -0.964449    0.673534    1.754374   -1.489569   -8.871632  2.0   \n","2312  6471   -0.832078    -1.40681    2.368887   -1.517962   51.492272  1.0   \n","2313  6473    0.340885   -0.195353    0.372499   -1.375068  114.904649  0.0   \n","\n","0     2.0  3.0  4.0  ...  52834.0  52835.0  52836.0  52837.0  52838.0  \\\n","0     2.0  1.0  2.0  ...      2.0      1.0      1.0      2.0      1.0   \n","1     2.0  1.0  0.0  ...      0.0      1.0      1.0      2.0      2.0   \n","2     2.0  1.0  1.0  ...      0.0      1.0      1.0      2.0      0.0   \n","3     2.0  0.0  0.0  ...      1.0      1.0      1.0      2.0      1.0   \n","4     2.0  1.0  1.0  ...      0.0      1.0      2.0      2.0      1.0   \n","...   ...  ...  ...  ...      ...      ...      ...      ...      ...   \n","2309  1.0  2.0  1.0  ...      0.0      0.0      1.0      0.0      2.0   \n","2310  2.0  1.0  2.0  ...      1.0      1.0      2.0      0.0      0.0   \n","2311  2.0  2.0  1.0  ...      0.0      1.0      1.0      1.0      1.0   \n","2312  1.0  2.0  1.0  ...      1.0      1.0      2.0      1.0      1.0   \n","2313  2.0  2.0  1.0  ...      1.0      0.0      1.0      1.0      1.0   \n","\n","0     52839.0  52840.0  52841.0  52842.0  52843.0  \n","0         1.0      0.0      2.0      0.0      1.0  \n","1         2.0      1.0      2.0      1.0      2.0  \n","2         2.0      1.0      2.0      1.0      2.0  \n","3         2.0      1.0      2.0      1.0      2.0  \n","4         2.0      2.0      2.0      2.0      2.0  \n","...       ...      ...      ...      ...      ...  \n","2309      2.0      0.0      2.0      1.0      2.0  \n","2310      2.0      0.0      2.0      2.0      2.0  \n","2311      2.0      1.0      2.0      2.0      0.0  \n","2312      2.0      0.0      2.0      0.0      2.0  \n","2313      2.0      2.0      2.0      0.0      2.0  \n","\n","[2314 rows x 52849 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df.columns = df.iloc[0,:]\n","df = df.iloc[1:,:]\n","df.index = range(len(df))\n","df"]},{"cell_type":"code","execution_count":null,"id":"e40bdb87","metadata":{"tags":[],"id":"e40bdb87","outputId":"878749ab-da60-4280-8408-b3b74de2e1f6"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>t1</th>\n","      <th>t2</th>\n","      <th>t3</th>\n","      <th>t4</th>\n","      <th>t5</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.8687035</td>\n","      <td>0.28158495</td>\n","      <td>1.71659645</td>\n","      <td>-0.8861064</td>\n","      <td>47.6114438</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.13144417</td>\n","      <td>-1.7468354</td>\n","      <td>0.95272593</td>\n","      <td>0.15843581</td>\n","      <td>-41.940212</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.8536013</td>\n","      <td>-2.9968045</td>\n","      <td>-0.0871332</td>\n","      <td>-1.2177578</td>\n","      <td>100.653792</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.03416785</td>\n","      <td>-0.320321</td>\n","      <td>-0.071189</td>\n","      <td>-1.4672803</td>\n","      <td>13.8808255</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.57391141</td>\n","      <td>-2.0458374</td>\n","      <td>0.054743</td>\n","      <td>0.16402708</td>\n","      <td>-58.738674</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2309</th>\n","      <td>0.045294</td>\n","      <td>1.386794</td>\n","      <td>2.291255</td>\n","      <td>0.226449</td>\n","      <td>-23.831346</td>\n","    </tr>\n","    <tr>\n","      <th>2310</th>\n","      <td>-1.199029</td>\n","      <td>-0.565144</td>\n","      <td>2.225259</td>\n","      <td>-5.672833</td>\n","      <td>85.793067</td>\n","    </tr>\n","    <tr>\n","      <th>2311</th>\n","      <td>-0.964449</td>\n","      <td>0.673534</td>\n","      <td>1.754374</td>\n","      <td>-1.489569</td>\n","      <td>-8.871632</td>\n","    </tr>\n","    <tr>\n","      <th>2312</th>\n","      <td>-0.832078</td>\n","      <td>-1.40681</td>\n","      <td>2.368887</td>\n","      <td>-1.517962</td>\n","      <td>51.492272</td>\n","    </tr>\n","    <tr>\n","      <th>2313</th>\n","      <td>0.340885</td>\n","      <td>-0.195353</td>\n","      <td>0.372499</td>\n","      <td>-1.375068</td>\n","      <td>114.904649</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2314 rows × 5 columns</p>\n","</div>"],"text/plain":["0             t1          t2          t3          t4          t5\n","0     -0.8687035  0.28158495  1.71659645  -0.8861064  47.6114438\n","1     0.13144417  -1.7468354  0.95272593  0.15843581  -41.940212\n","2     -0.8536013  -2.9968045  -0.0871332  -1.2177578  100.653792\n","3     0.03416785   -0.320321   -0.071189  -1.4672803  13.8808255\n","4     0.57391141  -2.0458374    0.054743  0.16402708  -58.738674\n","...          ...         ...         ...         ...         ...\n","2309    0.045294    1.386794    2.291255    0.226449  -23.831346\n","2310   -1.199029   -0.565144    2.225259   -5.672833   85.793067\n","2311   -0.964449    0.673534    1.754374   -1.489569   -8.871632\n","2312   -0.832078    -1.40681    2.368887   -1.517962   51.492272\n","2313    0.340885   -0.195353    0.372499   -1.375068  114.904649\n","\n","[2314 rows x 5 columns]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["X = df.iloc[:,6:]\n","Y = df.iloc[:,1:6]\n","Y"]},{"cell_type":"code","execution_count":null,"id":"59a67a68-bef8-49ee-9d91-06d8eb28f0db","metadata":{"tags":[],"id":"59a67a68-bef8-49ee-9d91-06d8eb28f0db"},"outputs":[],"source":["Y = Y.astype('float')"]},{"cell_type":"code","execution_count":null,"id":"bff63ca3","metadata":{"tags":[],"id":"bff63ca3","outputId":"6c3e5590-6aec-47dc-df11-adbbd36b1bf4"},"outputs":[{"name":"stderr","output_type":"stream","text":["/root/miniconda3/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n"]}],"source":["from sklearn.preprocessing import OneHotEncoder\n","\n","\n","# 初始化OneHotEncoder\n","encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n","\n","# 选择要进行独热编码的列\n","cols_to_encode = df.columns[6:]\n","\n","# 拟合训练集\n","encoder.fit(X)\n","\n","# 对训练集和测试集进行转换\n","X_encoded = encoder.transform(X)\n"]},{"cell_type":"code","execution_count":null,"id":"fbe02c31-de55-48d2-b345-9958b357568e","metadata":{"tags":[],"id":"fbe02c31-de55-48d2-b345-9958b357568e","outputId":"deb7cabc-32ba-4eec-fdcc-0bf9ce4529c0"},"outputs":[{"data":{"text/plain":["(2314, 279995)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["X_encoded.shape"]},{"cell_type":"code","execution_count":null,"id":"36ae30d0-8f57-4a41-a271-a6ce80c08551","metadata":{"tags":[],"id":"36ae30d0-8f57-4a41-a271-a6ce80c08551"},"outputs":[],"source":["import numpy as np\n","\n","# 假设data是一个形状为(2314, 279995)的数组\n","# 裁剪数据以使列数为279841\n","data_trimmed = X_encoded[:, :279841]\n","\n","# 重塑数据为(2314, 529, 529)\n","data_reshaped = data_trimmed.reshape(2314,1, 529, 529)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"368149f5-a89c-4514-b44b-1b090dac53ae","metadata":{"id":"368149f5-a89c-4514-b44b-1b090dac53ae"},"outputs":[],"source":["\n","# # 假设data是一个形状为(N, 30987)的数组\n","# padding = np.zeros((X_encoded.shape[0], 31329 - X_encoded.shape[1]))\n","# data_padded = np.concatenate([X_encoded, padding], axis=1)\n","# # 然后，将数据重塑为(n, 177, 177, 1)\n","# data_reshaped = data_padded.reshape(-1, 1,177, 177)\n","# data_reshaped.shape"]},{"cell_type":"code","execution_count":null,"id":"03d20b9c","metadata":{"tags":[],"id":"03d20b9c"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, Y_train, Y_test = train_test_split(data_reshaped,Y , test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"id":"d5b15dab","metadata":{"tags":[],"id":"d5b15dab"},"outputs":[],"source":["def z_score(data):\n","    data = data.astype(float)\n","    Mean = data.mean()\n","    Var = ((data - Mean)**2).mean()\n","    Std = pow(Var,0.5)\n","    data = (data - Mean)/Std  # 标准化\n","    return Mean,Std,data\n","Mean,Std,Y_train = z_score(Y_train)\n","Y_test = (Y_test - Mean)/Std"]},{"cell_type":"code","execution_count":null,"id":"c7fc6a2b","metadata":{"tags":[],"id":"c7fc6a2b"},"outputs":[],"source":["Y_train = Y_train.values\n","Y_test = Y_test.values"]},{"cell_type":"code","execution_count":null,"id":"8e2f2c83","metadata":{"tags":[],"id":"8e2f2c83","outputId":"dae120a8-5802-4636-d626-50bebec44315"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1851, 1, 529, 529]) torch.Size([1851, 5]) torch.Size([463, 1, 529, 529]) torch.Size([463, 5])\n"]}],"source":["Y_train = np.squeeze(Y_train)\n","Y_test = np.squeeze(Y_test)\n","\n","X_train = torch.tensor(X_train,dtype = torch.float)\n","Y_train  = torch.tensor(Y_train,dtype = torch.float)\n","\n","X_test = torch.tensor(X_test,dtype = torch.float)\n","Y_test  = torch.tensor(Y_test,dtype = torch.float)\n","print(X_train.shape, Y_train.shape,X_test.shape, Y_test.shape)"]},{"cell_type":"code","execution_count":null,"id":"4a2b0600-2ee9-4e71-8b99-1e3e0ee08007","metadata":{"tags":[],"id":"4a2b0600-2ee9-4e71-8b99-1e3e0ee08007"},"outputs":[],"source":["train_loader = Data.DataLoader(\n","    dataset=Data.TensorDataset(X_train, Y_train),  # 封装进Data.TensorDataset()类的数据，可以为任意维度\n","    batch_size=35,  # 每块的大小\n","    shuffle=True,\n","    drop_last =True, #丢弃最后一组数据\n","    num_workers=0,  # 多进程（multiprocess）来读数据\n",")\n","test_loader = Data.DataLoader(\n","    dataset=Data.TensorDataset(X_test, Y_test),  # 封装进Data.TensorDataset()类的数据，可以为任意维度\n","    batch_size=35,  # 每块的大小\n","    shuffle=False,\n","    drop_last =True,\n","    num_workers=0,\n",")"]},{"cell_type":"code","execution_count":null,"id":"805c74c1-6422-4f9e-821e-405793dd45da","metadata":{"tags":[],"id":"805c74c1-6422-4f9e-821e-405793dd45da"},"outputs":[],"source":["class AdamL12(Optimizer):\n","    r\"\"\" Implements Adam with L0 regularization\n","\n","    A General Family of Proximal Methods for Stochastic Preconditioned Gradient Descent\n","\n","    This family contains the Adam-type curvature estimate and\n","    L0 (non-convex, non-smooth) regularizer\n","\n","    For this optimizer, the update rule is somewhat adaptive hard-thresholding\n","    \"\"\"\n","    def __init__(self, params, lr=1e-3, betas=(0.1, 0.999), eps=1e-8,\n","                 weight_decay=1e-2, amsgrad=False, penalty=0.0):\n","        if not 0.0 <= lr:\n","            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n","        if not 0.0 <= eps:\n","            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n","        if not 0.0 <= betas[0] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n","        if not 0.0 <= betas[1] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n","        defaults = dict(lr=lr, betas=betas, eps=eps,\n","                        weight_decay=weight_decay, amsgrad=amsgrad,\n","                        penalty=penalty)\n","        super(AdamL12, self).__init__(params, defaults)\n","\n","    def __setstate__(self, state):\n","        super(AdamL12, self).__setstate__(state)\n","        for group in self.param_groups:\n","            group.setdefault(\"amsgrad\", False)\n","\n","    def step(self, closure=None):\n","        \"\"\" Performs a single optimization step.\n","\n","        Arguments:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","\n","                # Perform weight-decay\n","                p.data.mul_(1 - group['lr'] * group['weight_decay'])\n","\n","                # Perform optimization step\n","                grad = p.grad.data\n","                if grad.is_sparse:\n","                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n","                amsgrad = group['amsgrad']\n","\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    # Exponential moving average of gradient values\n","                    state['exp_avg'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                    # Exponential moving average of squared gradient values\n","                    state['exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                    if amsgrad:\n","                        state['max_exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n","                if amsgrad:\n","                    max_exp_avg_sq = state['max_exp_avg_sq']\n","                beta1, beta2 = group['betas']\n","\n","                state['step'] += 1\n","                bias_correction1 = 1 - beta1 ** state['step']\n","                bias_correction2 = 1 - beta2 ** state['step']\n","\n","                # Decay the first and second moment running average coefficient\n","                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n","                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n","                if amsgrad:\n","                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n","                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n","                else:\n","                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n","\n","                step_size = group['lr'] / bias_correction1\n","\n","                p.data.addcdiv_(-step_size, exp_avg, denom)\n","\n","                if len(p.data.shape) == 2 or len(p.data.shape) == 4:\n","                    threshold = (54 ** (1/3) / 4) * ((2 * group['penalty'] * group['lr'] / denom) ** (2/3))\n","                    mask = p.data.abs() > threshold\n","                    mask = mask.float()\n","\n","                    zero_mask = p.data.abs() <= threshold\n","                    zero_mask = zero_mask.float() + 1e-4\n","\n","                    p.data.mul_(mask)\n","                    factor = (group['lr'] * group['penalty'] / denom) / 4\n","                    angle = factor * (((p.data.abs() + zero_mask) / 3) ** (-1.5))\n","                    angle = angle * mask\n","                    angle = torch.acos(angle)\n","\n","                    value = p.data * (2/3) * (1 + torch.cos(2/3 * (math.pi - angle)))\n","                    p.data = value * mask\n","\n","        return loss\n","\n","class AdamL23(Optimizer):\n","    r\"\"\" Implements Adam with L0 regularization\n","\n","    A General Family of Proximal Methods for Stochastic Preconditioned Gradient Descent\n","\n","    This family contains the Adam-type curvature estimate and\n","    L0 (non-convex, non-smooth) regularizer\n","\n","    For this optimizer, the update rule is somewhat adaptive hard-thresholding\n","    \"\"\"\n","    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n","                 weight_decay=1e-2, amsgrad=False, penalty=0.0):\n","        if not 0.0 <= lr:\n","            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n","        if not 0.0 <= eps:\n","            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n","        if not 0.0 <= betas[0] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n","        if not 0.0 <= betas[1] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n","        defaults = dict(lr=lr, betas=betas, eps=eps,\n","                        weight_decay=weight_decay, amsgrad=amsgrad,\n","                        penalty=penalty)\n","        super(AdamL23, self).__init__(params, defaults)\n","\n","    def __setstate__(self, state):\n","        super(AdamL23, self).__setstate__(state)\n","        for group in self.param_groups:\n","            group.setdefault(\"amsgrad\", False)\n","\n","    def step(self, closure=None):\n","        \"\"\" Performs a single optimization step.\n","\n","        Arguments:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","\n","                # Perform weight-decay\n","                p.data.mul_(1 - group['lr'] * group['weight_decay'])\n","\n","                # Perform optimization step\n","                grad = p.grad.data\n","                if grad.is_sparse:\n","                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n","                amsgrad = group['amsgrad']\n","\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    # Exponential moving average of gradient values\n","                    state['exp_avg'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                    # Exponential moving average of squared gradient values\n","                    state['exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                    if amsgrad:\n","                        state['max_exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n","                if amsgrad:\n","                    max_exp_avg_sq = state['max_exp_avg_sq']\n","                beta1, beta2 = group['betas']\n","\n","                state['step'] += 1\n","                bias_correction1 = 1 - beta1 ** state['step']\n","                bias_correction2 = 1 - beta2 ** state['step']\n","\n","                # Decay the first and second moment running average coefficient\n","                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n","                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n","                if amsgrad:\n","                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n","                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n","                else:\n","                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n","\n","                step_size = group['lr'] / bias_correction1\n","\n","                p.data.addcdiv_(-step_size, exp_avg, denom)\n","\n","                if len(p.data.shape) == 2 or len(p.data.shape) == 4:\n","                    eff_lam = 2 * group['lr'] * group['penalty'] / denom\n","                    threshold = (2/3) * (3 * eff_lam ** 3) ** (1/4)\n","                    mask = p.data.abs() > threshold\n","                    mask = mask.float()\n","\n","                    zero_mask = p.data.abs() <= threshold\n","                    zero_mask = zero_mask.float() * 100\n","\n","                    p.data.mul_(mask)\n","                    angle = acosh((27/16) * (p.data ** 2 + zero_mask) * (eff_lam ** (-1.5)))\n","                    angle = angle * mask\n","                    absA = (2/math.sqrt(3)) * (eff_lam ** (1/4)) * (torch.cosh(angle/3) ** (1/2))\n","\n","                    value = ((absA + torch.sqrt(2 * (p.data.abs() + zero_mask) / absA - absA ** 2)) / 2) ** 3\n","\n","                    p.data = p.data.sign() * value * mask\n","\n","        return loss"]},{"cell_type":"code","execution_count":null,"id":"db8ad935-0180-4f38-9c19-d28ad1f8d502","metadata":{"tags":[],"id":"db8ad935-0180-4f38-9c19-d28ad1f8d502","outputId":"6cc6075a-aa08-4b7b-a3c6-14d5a8472f3f"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_27884/1815375738.py:77: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)\n","  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n"]},{"name":"stdout","output_type":"stream","text":["已更新保存模型\n","Epochs 0 loss_train 9.122115786258991 loss_test 1.2171071905356188 每轮耗时： 12.8816819190979\n","已更新保存模型\n","Epochs 1 loss_train 1.005250632762909 loss_test 1.1548140782576342 每轮耗时： 12.876070261001587\n","Epochs 2 loss_train 0.9629660432155316 loss_test 1.3662020151431744 每轮耗时： 12.682474136352539\n","已更新保存模型\n","Epochs 3 loss_train 0.8447145750889411 loss_test 1.0866758364897509 每轮耗时： 12.797119617462158\n","Epochs 4 loss_train 0.7092279642820358 loss_test 1.869135251412025 每轮耗时： 12.846256971359253\n","Epochs 5 loss_train 0.5184256726732621 loss_test 1.1648250268055842 每轮耗时： 14.176190376281738\n","Epochs 6 loss_train 0.36601342547398347 loss_test 1.3581382082058833 每轮耗时： 12.75196886062622\n","Epochs 7 loss_train 0.275473166257143 loss_test 1.175881803035736 每轮耗时： 12.756240367889404\n","Epochs 8 loss_train 0.20524760264043623 loss_test 1.3472126447237456 每轮耗时： 12.795000553131104\n","Epochs 9 loss_train 0.15182370047729749 loss_test 1.2571119711949275 每轮耗时： 12.705238103866577\n","Epochs 10 loss_train 0.14204243713846573 loss_test 1.1755017133859487 每轮耗时： 12.713347434997559\n","Epochs 11 loss_train 0.11805770051880525 loss_test 1.2235099856670086 每轮耗时： 12.75163197517395\n","Epochs 12 loss_train 0.11099079853067031 loss_test 1.2339887710718007 每轮耗时： 12.778189182281494\n","Epochs 13 loss_train 0.10315748511885221 loss_test 1.1240923633942237 每轮耗时： 12.708536148071289\n","Epochs 14 loss_train 0.11971269089442033 loss_test 1.2279957028535695 每轮耗时： 12.734736442565918\n","Epochs 15 loss_train 0.12010358431591438 loss_test 1.7256290087333093 每轮耗时： 12.848230838775635\n","Epochs 16 loss_train 0.10856356041935775 loss_test 1.2378355127114515 每轮耗时： 12.689163208007812\n","已更新保存模型\n","Epochs 17 loss_train 0.10477436183450314 loss_test 1.0660980985714839 每轮耗时： 13.042572259902954\n","Epochs 18 loss_train 0.10135637775350076 loss_test 1.075404547728025 每轮耗时： 12.60854458808899\n","Epochs 19 loss_train 0.10610508725333673 loss_test 1.1477841230539174 每轮耗时： 12.888157367706299\n","Epochs 20 loss_train 0.08912789886100934 loss_test 1.4368767554943378 每轮耗时： 12.643301248550415\n","已更新保存模型\n","Epochs 21 loss_train 0.08972968805867892 loss_test 1.0413706027544463 每轮耗时： 13.63438868522644\n","Epochs 22 loss_train 0.08017268339888407 loss_test 1.1197981100815992 每轮耗时： 12.662467241287231\n","Epochs 23 loss_train 0.07879039107893522 loss_test 1.1867180970998912 每轮耗时： 12.61428713798523\n","Epochs 24 loss_train 0.09601975626383837 loss_test 1.044573073203747 每轮耗时： 12.871378421783447\n","Epochs 25 loss_train 0.09279903841133301 loss_test 1.180311565215771 每轮耗时： 12.59708023071289\n","Epochs 26 loss_train 0.10828940434238085 loss_test 1.3834270972471971 每轮耗时： 13.227866172790527\n","Epochs 27 loss_train 0.09245196163940889 loss_test 1.3025187437350934 每轮耗时： 12.662098169326782\n","已更新保存模型\n","Epochs 28 loss_train 0.08785392365490015 loss_test 1.0333183453633235 每轮耗时： 12.970430374145508\n","Epochs 29 loss_train 0.08297290352101509 loss_test 1.0980983605751624 每轮耗时： 12.652618408203125\n","Epochs 30 loss_train 0.06867669178889348 loss_test 1.0705521244269152 每轮耗时： 13.095709562301636\n","Epochs 31 loss_train 0.06952543685642573 loss_test 1.0375511783819933 每轮耗时： 12.701896905899048\n","Epochs 32 loss_train 0.07251256589706127 loss_test 1.0397354456094594 每轮耗时： 13.853403329849243\n","Epochs 33 loss_train 0.06502549472050025 loss_test 1.061580451635214 每轮耗时： 13.318847417831421\n","Epochs 34 loss_train 0.07524944977978101 loss_test 1.1187725754884572 每轮耗时： 13.58431887626648\n","Epochs 35 loss_train 0.07065978940003194 loss_test 1.170711948321416 每轮耗时： 12.761995315551758\n","Epochs 36 loss_train 0.07684248800461109 loss_test 1.1446857223143945 每轮耗时： 13.38356328010559\n","Epochs 37 loss_train 0.07787100533739878 loss_test 1.1228139125383818 每轮耗时： 12.701027631759644\n","Epochs 38 loss_train 0.07979618600354745 loss_test 1.1235985114024236 每轮耗时： 12.807400465011597\n","Epochs 39 loss_train 0.0684277738372867 loss_test 1.064746379852295 每轮耗时： 12.576644659042358\n","已更新保存模型\n","Epochs 40 loss_train 0.06938885825757797 loss_test 1.0009386264360869 每轮耗时： 12.815008640289307\n","Epochs 41 loss_train 0.070567390236717 loss_test 1.0894675850868225 每轮耗时： 13.568499565124512\n","Epochs 42 loss_train 0.06407487306457299 loss_test 1.0493022203445435 每轮耗时： 12.639457941055298\n","Epochs 43 loss_train 0.0653881926376086 loss_test 1.1406925549873939 每轮耗时： 12.93806767463684\n","Epochs 44 loss_train 0.07126489274490339 loss_test 1.0615834960570703 每轮耗时： 12.638944864273071\n","Epochs 45 loss_train 0.06929626172551742 loss_test 1.126896633551671 每轮耗时： 13.153734922409058\n","Epochs 46 loss_train 0.06764341339182395 loss_test 1.1196545454171987 每轮耗时： 12.681851387023926\n","Epochs 47 loss_train 0.05852381075517489 loss_test 1.0348395796922536 每轮耗时： 12.705545663833618\n","Epochs 48 loss_train 0.057748462145145125 loss_test 1.0630508707119868 每轮耗时： 12.745573282241821\n","Epochs 49 loss_train 0.05715069948480679 loss_test 1.019938051700592 每轮耗时： 12.675176858901978\n","Epochs 50 loss_train 0.06330749575191966 loss_test 1.0207413114034212 每轮耗时： 12.733643770217896\n","Epochs 51 loss_train 0.0632452004087659 loss_test 1.0972421994576087 每轮耗时： 13.785715579986572\n","Epochs 52 loss_train 0.06409742461087611 loss_test 1.0253691031382635 每轮耗时： 12.798701047897339\n","Epochs 53 loss_train 0.062033505322268374 loss_test 1.0826279062491198 每轮耗时： 12.721536636352539\n","Epochs 54 loss_train 0.06500991190282199 loss_test 1.0479260362111604 每轮耗时： 12.750478029251099\n","Epochs 55 loss_train 0.057024835393979 loss_test 1.0187752980452318 每轮耗时： 12.854154586791992\n","已更新保存模型\n","Epochs 56 loss_train 0.05779495212034537 loss_test 0.9853609708639292 每轮耗时： 12.987379789352417\n","Epochs 57 loss_train 0.055531470105051994 loss_test 1.0034941251461322 每轮耗时： 13.560792446136475\n","Epochs 58 loss_train 0.05568397009315399 loss_test 0.9922268849152786 每轮耗时： 12.8184654712677\n","Epochs 59 loss_train 0.06262587283093196 loss_test 1.0661978584069471 每轮耗时： 12.728122234344482\n","Epochs 60 loss_train 0.061994049984675184 loss_test 1.2464067431596608 每轮耗时： 12.89153528213501\n","Epochs 61 loss_train 0.06118501884003098 loss_test 1.0203630878375127 每轮耗时： 12.728941440582275\n","Epochs 62 loss_train 0.06486053564227544 loss_test 1.121272256741157 每轮耗时： 12.795143842697144\n","Epochs 63 loss_train 0.06124845096984735 loss_test 1.0050553725315974 每轮耗时： 12.784986019134521\n","Epochs 64 loss_train 0.061561526372455634 loss_test 1.0500801755831792 每轮耗时： 13.715470314025879\n","Epochs 65 loss_train 0.0611137138822904 loss_test 1.0589632942126348 每轮耗时： 12.738749980926514\n","已更新保存模型\n","Epochs 66 loss_train 0.05784147261426999 loss_test 0.9780647021073562 每轮耗时： 12.793125629425049\n","Epochs 67 loss_train 0.0514638451501154 loss_test 1.079876042329348 每轮耗时： 12.790835618972778\n","Epochs 68 loss_train 0.05513401354591434 loss_test 1.0932229986557593 每轮耗时： 14.187742471694946\n","Epochs 69 loss_train 0.05568068836314174 loss_test 0.9945174730741061 每轮耗时： 12.939506769180298\n","Epochs 70 loss_train 0.06804706901311874 loss_test 1.101873095218952 每轮耗时： 12.54251742362976\n","Epochs 71 loss_train 0.059273825564350076 loss_test 1.034250328174004 每轮耗时： 13.361393928527832\n","Epochs 72 loss_train 0.061520389603594176 loss_test 1.0075363791905916 每轮耗时： 12.608407974243164\n","Epochs 73 loss_train 0.058129337472984426 loss_test 1.1468479128984304 每轮耗时： 12.767602920532227\n","Epochs 74 loss_train 0.05583013852055256 loss_test 1.0329569578170776 每轮耗时： 12.524889945983887\n","Epochs 75 loss_train 0.051972972945525095 loss_test 0.9963080607927762 每轮耗时： 12.812873840332031\n","Epochs 76 loss_train 0.057527632667468145 loss_test 0.9788758112834051 每轮耗时： 12.706083536148071\n","Epochs 77 loss_train 0.05224343640013383 loss_test 0.9788283430613004 每轮耗时： 12.697506666183472\n","Epochs 78 loss_train 0.05957709585961241 loss_test 1.1110420868946955 每轮耗时： 12.839346408843994\n","Epochs 79 loss_train 0.06001368286804511 loss_test 1.0482746362686157 每轮耗时： 12.751872062683105\n","Epochs 80 loss_train 0.057661159035678096 loss_test 1.0364259894077594 每轮耗时： 12.656447887420654\n","Epochs 81 loss_train 0.059556337216725715 loss_test 1.0518691448064952 每轮耗时： 12.843438148498535\n","Epochs 82 loss_train 0.05753329030882854 loss_test 1.0030617576379042 每轮耗时： 12.664242029190063\n","Epochs 83 loss_train 0.05160138917227204 loss_test 1.0427125554818373 每轮耗时： 12.803343296051025\n","Epochs 84 loss_train 0.0576009565892701 loss_test 1.0511775887929475 每轮耗时： 12.675076484680176\n","Epochs 85 loss_train 0.05661860344788203 loss_test 1.0491122053219721 每轮耗时： 12.71992015838623\n","Epochs 86 loss_train 0.054612544329407126 loss_test 1.3227334022521973 每轮耗时： 12.66241979598999\n","Epochs 87 loss_train 0.050911377399013594 loss_test 0.985072488968189 每轮耗时： 12.830733060836792\n","Epochs 88 loss_train 0.04995768373975387 loss_test 1.0151726603507996 每轮耗时： 12.661994695663452\n","Epochs 89 loss_train 0.04923551031746543 loss_test 0.9868656626114478 每轮耗时： 13.037843704223633\n","已更新保存模型\n","Epochs 90 loss_train 0.0447878882002372 loss_test 0.9741606070445135 每轮耗时： 12.849483251571655\n","Epochs 91 loss_train 0.04888704598236542 loss_test 0.9937949272302481 每轮耗时： 12.606699228286743\n","Epochs 92 loss_train 0.048980768519238785 loss_test 0.9865877078129694 每轮耗时： 12.785048246383667\n","Epochs 93 loss_train 0.049182441766158894 loss_test 1.1786911258330712 每轮耗时： 12.511306285858154\n","Epochs 94 loss_train 0.05754987003568273 loss_test 1.0305279676730816 每轮耗时： 13.561970233917236\n","Epochs 95 loss_train 0.053532140902601756 loss_test 1.0558231472969055 每轮耗时： 12.761110544204712\n","Epochs 96 loss_train 0.053348136528466754 loss_test 1.0063030490508447 每轮耗时： 13.624720335006714\n","Epochs 97 loss_train 0.054280801103092156 loss_test 1.0053719052901635 每轮耗时： 12.59191608428955\n","Epochs 98 loss_train 0.053136830182316214 loss_test 1.0202254698826716 每轮耗时： 12.774686336517334\n","Epochs 99 loss_train 0.05191928047973376 loss_test 0.9922920923966628 每轮耗时： 13.112003326416016\n","总耗时: 1289.3816561698914\n"]}],"source":["Epochs                = 100      # 训练轮数\n","l                     = 0.01\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","def seed_torch():\n","    seed=1029\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed) # 为了禁止hash随机化，使得实验可复现\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","seed_torch()\n","model = ResNet18()\n","model = model.to(device)\n","loss_function = nn.MSELoss()  # loss\n","optimizer = AdamL12(model.parameters(), lr=l)  # 优化器\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=9, gamma=0.95)\n","LOSS = float('inf')\n","result = {}\n","result['train-loss']= []\n","result['test-loss']= []\n","start_time1 = time.time()\n","for epochs in range(Epochs):\n","    start_time = time.time()\n","    model.train()\n","    train_loss = 0\n","    test_loss  = 0\n","    for data_l in train_loader:\n","        seq, labels = data_l\n","        seq, labels = seq.to(device), labels.to(device)\n","        optimizer.zero_grad()                          #    清空过往梯度\n","        y_pred = model(seq)\n","        single_loss = loss_function(y_pred, labels)    #    获取loss：输入预测值和标签，计算损失函数\n","        single_loss.backward()                         #    反向传播，计算当前梯度\n","        optimizer.step()                               #    根据梯度更新网络参数\n","        train_loss += single_loss.item()\n","    train_loss = train_loss/len(train_loader)\n","    scheduler.step()\n","    model.eval()\n","    for data_l in test_loader:\n","        seq, labels = data_l\n","        seq, labels = seq.to(device), labels.to(device)\n","        y_pred = model(seq)\n","        single_loss = loss_function(y_pred, labels)    #    获取loss：输入预测值和标签，计算损失函数\n","        test_loss += single_loss.item()\n","    test_loss = test_loss/len(test_loader)\n","    if LOSS > test_loss:\n","        LOSS =test_loss\n","        torch.save(model, 'model_cnn_pig_data.pth')\n","        print('已更新保存模型')\n","    result['train-loss'].append(train_loss)\n","    result['test-loss'].append(test_loss)\n","    del seq, labels ,y_pred #删除数据与变量\n","    gc.collect() #清除数据与变量相关的缓存\n","    torch.cuda.empty_cache() #缓存分配器分配出去的内存给释放掉\n","    epoch_time = time.time() - start_time\n","    print('Epochs',epochs,'loss_train',train_loss,'loss_test',test_loss,'每轮耗时：',epoch_time)\n","all_time = time.time() - start_time1\n","print('总耗时:',all_time)"]},{"cell_type":"code","execution_count":null,"id":"b363e603-46e2-4005-b455-d041d109dbe3","metadata":{"tags":[],"id":"b363e603-46e2-4005-b455-d041d109dbe3"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.figure(dpi=150,figsize=(7,4))\n","plt.plot(result['train-loss'][:], label='train')\n","plt.plot(result['test-loss'][:], label='test')\n","plt.xlabel('iteration')\n","plt.ylabel('loss')\n","plt.title('Training and Testing Loss')\n","plt.legend()\n","plt.savefig('resnet18loss1.jpg',dpi=150)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"1c3642a0-1036-493a-9450-c8eba508fa7a","metadata":{"tags":[],"id":"1c3642a0-1036-493a-9450-c8eba508fa7a"},"outputs":[],"source":["evaluation_index = Evaluation_index(Y_test1,pre)"]},{"cell_type":"code","execution_count":null,"id":"edda82e3-ed71-4c80-b0c5-927f2717d8b7","metadata":{"id":"edda82e3-ed71-4c80-b0c5-927f2717d8b7"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"7cfa6612-37ac-49d8-8431-8049dc66bf64","metadata":{"id":"7cfa6612-37ac-49d8-8431-8049dc66bf64"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"ef0cfb90","metadata":{"id":"ef0cfb90"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}