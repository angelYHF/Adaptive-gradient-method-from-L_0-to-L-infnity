{"cells":[{"cell_type":"code","execution_count":null,"id":"6d30e714","metadata":{"tags":[],"id":"6d30e714"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import random,os\n","import torch.utils.data as Data\n","from sklearn.metrics import f1_score,recall_score,precision_score,roc_curve,auc,accuracy_score,confusion_matrix,r2_score, mean_squared_error\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import torch\n","# import torch.nn as nn\n","from torch import nn,optim\n","import torch.nn.functional as F\n","from torch.optim.optimizer import Optimizer, required\n","import gc\n","import time\n","import math\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"id":"d579e9d1","metadata":{"id":"d579e9d1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"f02da321","metadata":{"id":"f02da321"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"02360f62","metadata":{"tags":[],"id":"02360f62"},"outputs":[],"source":["'''ResNet in PyTorch.\n","\n","For Pre-activation ResNet, see 'preact_resnet.py'.\n","\n","Reference:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","'''\n","\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=2):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        # print('1',out.shape)\n","        out = self.bn2(self.conv2(out))\n","        # print('2',out.shape)\n","        out += self.shortcut(x)\n","        # print('3',out.shape)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=2):\n","        super(ResNet, self).__init__()\n","        self.input_channel = 1\n","        self.in_planes = 64\n","\n","\n","        self.conv1 = nn.Conv2d(self.input_channel, self.in_planes, kernel_size=3, stride=1, padding=0, bias=False)\n","        self.bn1 = nn.BatchNorm2d(self.in_planes)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=2)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*4, num_classes)\n","        self.dropout = nn.Dropout(0.2)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        # print(out.shape)\n","        out = self.layer1(out)\n","        # out = self.dropout(out)\n","        out = self.layer2(out)\n","        # out = self.dropout(out)\n","        out = self.layer3(out)\n","        # out = self.dropout(out)\n","        out = self.layer4(out)\n","        # out = self.dropout(out)\n","        out = F.avg_pool2d(out, 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","    def reg_loss(self):\n","        reg_loss = 0.0\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","                reg_loss += torch.sum(torch.abs(m.weight))\n","        return reg_loss\n","\n","    def l1reg_loss(self):\n","        reg_loss = 0.0\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","                reg_loss += torch.sum(torch.abs(m.weight))\n","        return reg_loss\n","\n","    def l12reg_loss(self):\n","        reg_loss = 0.0\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","                reg_loss += torch.sum((torch.abs(m.weight) + 1e-6).sqrt())\n","        return reg_loss\n","\n","    def l23reg_loss(self):\n","        reg_loss = 0.0\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","                reg_loss += torch.sum((torch.abs(m.weight) + 1e-6).pow(2/3))\n","        return reg_loss\n","\n","    def exact_sparsity(self):\n","        nnz = 0\n","        total_param = 0.0\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","                total_param += np.prod(m.weight.data.shape)\n","                nnz += torch.sum(m.weight.data != 0).detach().item()\n","        ratio = nnz / total_param\n","        return ratio\n","\n","    def sparsity_level(self):\n","        nnz_2 = 0.0\n","        nnz_3 = 0.0\n","        nnz_4 = 0.0\n","        total_param = 0.0\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","                total_param += np.prod(m.weight.data.shape)\n","                nnz_2 += torch.sum(m.weight.data.abs() >= 0.01).detach().item()\n","                nnz_3 += torch.sum(m.weight.data.abs() >= 0.001).detach().item()\n","                nnz_4 += torch.sum(m.weight.data.abs() >= 0.0001).detach().item()\n","        ratio_2 = nnz_2 / total_param\n","        ratio_3 = nnz_3 / total_param\n","        ratio_4 = nnz_4 / total_param\n","        return ratio_2, ratio_3\n","\n","\n","def ResNet18():\n","    return ResNet(BasicBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","def test():\n","    net = ResNet18()\n","    y = net(torch.randn(1,3,32,32))\n","    print(y.size())"]},{"cell_type":"code","execution_count":null,"id":"dbf65941","metadata":{"tags":[],"id":"dbf65941","outputId":"80118cad-52f1-4fdc-991f-bb830a0b2deb"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>10338</th>\n","      <th>10339</th>\n","      <th>10340</th>\n","      <th>10341</th>\n","      <th>10342</th>\n","      <th>10343</th>\n","      <th>10344</th>\n","      <th>10345</th>\n","      <th>10346</th>\n","      <th>10347</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.738094</td>\n","      <td>-0.023996</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.743638</td>\n","      <td>0.037044</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.431086</td>\n","      <td>-0.082434</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.144433</td>\n","      <td>0.017911</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.675366</td>\n","      <td>-0.040148</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1809</th>\n","      <td>-0.157640</td>\n","      <td>0.021284</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1810</th>\n","      <td>0.751668</td>\n","      <td>-0.073322</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1811</th>\n","      <td>0.186213</td>\n","      <td>0.056111</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1812</th>\n","      <td>-0.779898</td>\n","      <td>0.020232</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1813</th>\n","      <td>0.607548</td>\n","      <td>-0.037358</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1814 rows × 10348 columns</p>\n","</div>"],"text/plain":["         0         1      2      3      4      5      6      7      8      \\\n","0     0.738094 -0.023996      1      1      1      1      0      1      0   \n","1     0.743638  0.037044      1      1      2      1      1      1      1   \n","2     0.431086 -0.082434      2      0      2      2      0      2      0   \n","3     0.144433  0.017911      1      1      1      1      1      1      1   \n","4     0.675366 -0.040148      2      0      2      2      0      2      0   \n","...        ...       ...    ...    ...    ...    ...    ...    ...    ...   \n","1809 -0.157640  0.021284      1      1      1      1      0      1      0   \n","1810  0.751668 -0.073322      1      1      2      1      1      1      1   \n","1811  0.186213  0.056111      1      1      2      1      1      1      1   \n","1812 -0.779898  0.020232      1      1      1      1      1      1      1   \n","1813  0.607548 -0.037358      0      2      0      0      0      0      0   \n","\n","      9      ...  10338  10339  10340  10341  10342  10343  10344  10345  \\\n","0         1  ...      0      0      0      0      0      0      0      0   \n","1         2  ...      0      0      0      0      0      0      0      0   \n","2         2  ...      0      0      0      0      0      0      0      0   \n","3         2  ...      0      0      0      0      0      0      0      0   \n","4         2  ...      1      1      1      1      1      1      1      1   \n","...     ...  ...    ...    ...    ...    ...    ...    ...    ...    ...   \n","1809      1  ...      0      0      0      0      0      0      0      0   \n","1810      2  ...      0      0      0      0      0      0      0      0   \n","1811      2  ...      0      0      0      0      0      0      0      0   \n","1812      2  ...      0      0      0      1      0      0      0      2   \n","1813      0  ...      0      0      0      0      0      0      0      2   \n","\n","      10346  10347  \n","0         1      1  \n","1         2      0  \n","2         2      2  \n","3         2      0  \n","4         1      1  \n","...     ...    ...  \n","1809      2      2  \n","1810      2      2  \n","1811      2      2  \n","1812      2      0  \n","1813      2      0  \n","\n","[1814 rows x 10348 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["path1='/root/ResNet/'\n","path_name = os.path.join(path1,'micedata.csv')\n","df = pd.read_csv(path_name,header=None)\n","df"]},{"cell_type":"code","execution_count":null,"id":"e40bdb87","metadata":{"tags":[],"id":"e40bdb87"},"outputs":[],"source":["X = df.iloc[:,2:]\n","Y = df.iloc[:,:2]"]},{"cell_type":"code","execution_count":null,"id":"bff63ca3","metadata":{"tags":[],"id":"bff63ca3","outputId":"0b3f2feb-4a69-4dd8-9967-795cf9c388f2"},"outputs":[{"name":"stderr","output_type":"stream","text":["/root/miniconda3/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n"]}],"source":["from sklearn.preprocessing import OneHotEncoder\n","\n","\n","# 初始化OneHotEncoder\n","encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n","\n","# 选择要进行独热编码的列\n","cols_to_encode = df.columns[2:]\n","\n","# 拟合训练集\n","encoder.fit(X)\n","\n","# 对训练集和测试集进行转换\n","X_encoded = encoder.transform(X)\n",""]},{"cell_type":"code","execution_count":null,"id":"634c2cd8-1898-4ef5-a5f6-89c18d693184","metadata":{"tags":[],"id":"634c2cd8-1898-4ef5-a5f6-89c18d693184","outputId":"f19577c9-f581-4c78-9423-767803c592db"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>30977</th>\n","      <th>30978</th>\n","      <th>30979</th>\n","      <th>30980</th>\n","      <th>30981</th>\n","      <th>30982</th>\n","      <th>30983</th>\n","      <th>30984</th>\n","      <th>30985</th>\n","      <th>30986</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1809</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1810</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1811</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1812</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1813</th>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1814 rows × 30987 columns</p>\n","</div>"],"text/plain":["      0      1      2      3      4      5      6      7      8      9      \\\n","0       0.0    1.0    0.0    0.0    1.0    0.0    0.0    1.0    0.0    0.0   \n","1       0.0    1.0    0.0    0.0    1.0    0.0    0.0    0.0    1.0    0.0   \n","2       0.0    0.0    1.0    1.0    0.0    0.0    0.0    0.0    1.0    0.0   \n","3       0.0    1.0    0.0    0.0    1.0    0.0    0.0    1.0    0.0    0.0   \n","4       0.0    0.0    1.0    1.0    0.0    0.0    0.0    0.0    1.0    0.0   \n","...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n","1809    0.0    1.0    0.0    0.0    1.0    0.0    0.0    1.0    0.0    0.0   \n","1810    0.0    1.0    0.0    0.0    1.0    0.0    0.0    0.0    1.0    0.0   \n","1811    0.0    1.0    0.0    0.0    1.0    0.0    0.0    0.0    1.0    0.0   \n","1812    0.0    1.0    0.0    0.0    1.0    0.0    0.0    1.0    0.0    0.0   \n","1813    1.0    0.0    0.0    0.0    0.0    1.0    1.0    0.0    0.0    1.0   \n","\n","      ...  30977  30978  30979  30980  30981  30982  30983  30984  30985  \\\n","0     ...    0.0    1.0    0.0    0.0    0.0    1.0    0.0    0.0    1.0   \n","1     ...    0.0    1.0    0.0    0.0    0.0    0.0    1.0    1.0    0.0   \n","2     ...    0.0    1.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0   \n","3     ...    0.0    1.0    0.0    0.0    0.0    0.0    1.0    1.0    0.0   \n","4     ...    0.0    0.0    1.0    0.0    0.0    1.0    0.0    0.0    1.0   \n","...   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n","1809  ...    0.0    1.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0   \n","1810  ...    0.0    1.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0   \n","1811  ...    0.0    1.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0   \n","1812  ...    0.0    0.0    0.0    1.0    0.0    0.0    1.0    1.0    0.0   \n","1813  ...    0.0    0.0    0.0    1.0    0.0    0.0    1.0    1.0    0.0   \n","\n","      30986  \n","0       0.0  \n","1       0.0  \n","2       1.0  \n","3       0.0  \n","4       0.0  \n","...     ...  \n","1809    1.0  \n","1810    1.0  \n","1811    1.0  \n","1812    0.0  \n","1813    0.0  \n","\n","[1814 rows x 30987 columns]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["pd.DataFrame(X_encoded)"]},{"cell_type":"code","execution_count":null,"id":"03b76fb2-a78c-4efd-bb66-c331b7d77d99","metadata":{"tags":[],"id":"03b76fb2-a78c-4efd-bb66-c331b7d77d99","outputId":"ecf22cbc-62d6-40e4-8109-8edd58489082"},"outputs":[{"data":{"text/plain":["(1814, 1, 177, 177)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["\n","# 假设data是一个形状为(N, 30987)的数组\n","padding = np.zeros((X_encoded.shape[0], 31329 - X_encoded.shape[1]))\n","data_padded = np.concatenate([X_encoded, padding], axis=1)\n","# 然后，将数据重塑为(n, 177, 177, 1)\n","data_reshaped = data_padded.reshape(-1, 1,177, 177)\n","data_reshaped.shape"]},{"cell_type":"code","execution_count":null,"id":"03d20b9c","metadata":{"tags":[],"id":"03d20b9c"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, Y_train, Y_test = train_test_split(data_reshaped,Y , test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"id":"6a833285","metadata":{"id":"6a833285"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"d5b15dab","metadata":{"tags":[],"id":"d5b15dab"},"outputs":[],"source":["def z_score(data):\n","    data = data.astype(float)\n","    Mean = data.mean()\n","    Var = ((data - Mean)**2).mean()\n","    Std = pow(Var,0.5)\n","    data = (data - Mean)/Std  # 标准化\n","    return Mean,Std,data\n","Mean,Std,Y_train = z_score(Y_train)\n","Y_test = (Y_test - Mean)/Std"]},{"cell_type":"code","execution_count":null,"id":"c7fc6a2b","metadata":{"tags":[],"id":"c7fc6a2b"},"outputs":[],"source":["Y_train = Y_train.values\n","Y_test = Y_test.values"]},{"cell_type":"code","execution_count":null,"id":"8e2f2c83","metadata":{"tags":[],"id":"8e2f2c83","outputId":"c5a65d8b-94f6-4eac-f910-10b09bfbe963"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1451, 1, 177, 177]) torch.Size([1451, 2]) torch.Size([363, 1, 177, 177]) torch.Size([363, 2])\n"]}],"source":["Y_train = np.squeeze(Y_train)\n","Y_test = np.squeeze(Y_test)\n","\n","X_train = torch.tensor(X_train,dtype = torch.float)\n","Y_train  = torch.tensor(Y_train,dtype = torch.float)\n","\n","X_test = torch.tensor(X_test,dtype = torch.float)\n","Y_test  = torch.tensor(Y_test,dtype = torch.float)\n","print(X_train.shape, Y_train.shape,X_test.shape, Y_test.shape)"]},{"cell_type":"code","execution_count":null,"id":"4a2b0600-2ee9-4e71-8b99-1e3e0ee08007","metadata":{"tags":[],"id":"4a2b0600-2ee9-4e71-8b99-1e3e0ee08007"},"outputs":[],"source":["train_loader = Data.DataLoader(\n","    dataset=Data.TensorDataset(X_train, Y_train),  # 封装进Data.TensorDataset()类的数据，可以为任意维度\n","    batch_size=35,  # 每块的大小\n","    shuffle=True,\n","    drop_last =True, #丢弃最后一组数据\n","    num_workers=0,  # 多进程（multiprocess）来读数据\n",")\n","test_loader = Data.DataLoader(\n","    dataset=Data.TensorDataset(X_test, Y_test),  # 封装进Data.TensorDataset()类的数据，可以为任意维度\n","    batch_size=35,  # 每块的大小\n","    shuffle=False,\n","    drop_last =True,\n","    num_workers=0,\n",")"]},{"cell_type":"code","execution_count":null,"id":"805c74c1-6422-4f9e-821e-405793dd45da","metadata":{"tags":[],"id":"805c74c1-6422-4f9e-821e-405793dd45da"},"outputs":[],"source":["class AdamL12(Optimizer):\n","    r\"\"\" Implements Adam with L0 regularization\n","\n","    A General Family of Proximal Methods for Stochastic Preconditioned Gradient Descent\n","\n","    This family contains the Adam-type curvature estimate and\n","    L0 (non-convex, non-smooth) regularizer\n","\n","    For this optimizer, the update rule is somewhat adaptive hard-thresholding\n","    \"\"\"\n","    def __init__(self, params, lr=1e-3, betas=(0.1, 0.999), eps=1e-8,\n","                 weight_decay=1e-2, amsgrad=False, penalty=0.0):\n","        if not 0.0 <= lr:\n","            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n","        if not 0.0 <= eps:\n","            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n","        if not 0.0 <= betas[0] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n","        if not 0.0 <= betas[1] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n","        defaults = dict(lr=lr, betas=betas, eps=eps,\n","                        weight_decay=weight_decay, amsgrad=amsgrad,\n","                        penalty=penalty)\n","        super(AdamL12, self).__init__(params, defaults)\n","\n","    def __setstate__(self, state):\n","        super(AdamL12, self).__setstate__(state)\n","        for group in self.param_groups:\n","            group.setdefault(\"amsgrad\", False)\n","\n","    def step(self, closure=None):\n","        \"\"\" Performs a single optimization step.\n","\n","        Arguments:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","\n","                # Perform weight-decay\n","                p.data.mul_(1 - group['lr'] * group['weight_decay'])\n","\n","                # Perform optimization step\n","                grad = p.grad.data\n","                if grad.is_sparse:\n","                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n","                amsgrad = group['amsgrad']\n","\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    # Exponential moving average of gradient values\n","                    state['exp_avg'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                    # Exponential moving average of squared gradient values\n","                    state['exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                    if amsgrad:\n","                        state['max_exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n","                if amsgrad:\n","                    max_exp_avg_sq = state['max_exp_avg_sq']\n","                beta1, beta2 = group['betas']\n","\n","                state['step'] += 1\n","                bias_correction1 = 1 - beta1 ** state['step']\n","                bias_correction2 = 1 - beta2 ** state['step']\n","\n","                # Decay the first and second moment running average coefficient\n","                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n","                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n","                if amsgrad:\n","                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n","                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n","                else:\n","                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n","\n","                step_size = group['lr'] / bias_correction1\n","\n","                p.data.addcdiv_(-step_size, exp_avg, denom)\n","\n","                if len(p.data.shape) == 2 or len(p.data.shape) == 4:\n","                    threshold = (54 ** (1/3) / 4) * ((2 * group['penalty'] * group['lr'] / denom) ** (2/3))\n","                    mask = p.data.abs() > threshold\n","                    mask = mask.float()\n","\n","                    zero_mask = p.data.abs() <= threshold\n","                    zero_mask = zero_mask.float() + 1e-4\n","\n","                    p.data.mul_(mask)\n","                    factor = (group['lr'] * group['penalty'] / denom) / 4\n","                    angle = factor * (((p.data.abs() + zero_mask) / 3) ** (-1.5))\n","                    angle = angle * mask\n","                    angle = torch.acos(angle)\n","\n","                    value = p.data * (2/3) * (1 + torch.cos(2/3 * (math.pi - angle)))\n","                    p.data = value * mask\n","\n","        return loss\n","\n","class AdamL23(Optimizer):\n","    r\"\"\" Implements Adam with L0 regularization\n","\n","    A General Family of Proximal Methods for Stochastic Preconditioned Gradient Descent\n","\n","    This family contains the Adam-type curvature estimate and\n","    L0 (non-convex, non-smooth) regularizer\n","\n","    For this optimizer, the update rule is somewhat adaptive hard-thresholding\n","    \"\"\"\n","    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n","                 weight_decay=1e-2, amsgrad=False, penalty=0.0):\n","        if not 0.0 <= lr:\n","            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n","        if not 0.0 <= eps:\n","            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n","        if not 0.0 <= betas[0] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n","        if not 0.0 <= betas[1] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n","        defaults = dict(lr=lr, betas=betas, eps=eps,\n","                        weight_decay=weight_decay, amsgrad=amsgrad,\n","                        penalty=penalty)\n","        super(AdamL23, self).__init__(params, defaults)\n","\n","    def __setstate__(self, state):\n","        super(AdamL23, self).__setstate__(state)\n","        for group in self.param_groups:\n","            group.setdefault(\"amsgrad\", False)\n","\n","    def step(self, closure=None):\n","        \"\"\" Performs a single optimization step.\n","\n","        Arguments:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","\n","                # Perform weight-decay\n","                p.data.mul_(1 - group['lr'] * group['weight_decay'])\n","\n","                # Perform optimization step\n","                grad = p.grad.data\n","                if grad.is_sparse:\n","                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n","                amsgrad = group['amsgrad']\n","\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    # Exponential moving average of gradient values\n","                    state['exp_avg'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                    # Exponential moving average of squared gradient values\n","                    state['exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                    if amsgrad:\n","                        state['max_exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n","                if amsgrad:\n","                    max_exp_avg_sq = state['max_exp_avg_sq']\n","                beta1, beta2 = group['betas']\n","\n","                state['step'] += 1\n","                bias_correction1 = 1 - beta1 ** state['step']\n","                bias_correction2 = 1 - beta2 ** state['step']\n","\n","                # Decay the first and second moment running average coefficient\n","                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n","                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n","                if amsgrad:\n","                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n","                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n","                else:\n","                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n","\n","                step_size = group['lr'] / bias_correction1\n","\n","                p.data.addcdiv_(-step_size, exp_avg, denom)\n","\n","                if len(p.data.shape) == 2 or len(p.data.shape) == 4:\n","                    eff_lam = 2 * group['lr'] * group['penalty'] / denom\n","                    threshold = (2/3) * (3 * eff_lam ** 3) ** (1/4)\n","                    mask = p.data.abs() > threshold\n","                    mask = mask.float()\n","\n","                    zero_mask = p.data.abs() <= threshold\n","                    zero_mask = zero_mask.float() * 100\n","\n","                    p.data.mul_(mask)\n","                    angle = acosh((27/16) * (p.data ** 2 + zero_mask) * (eff_lam ** (-1.5)))\n","                    angle = angle * mask\n","                    absA = (2/math.sqrt(3)) * (eff_lam ** (1/4)) * (torch.cosh(angle/3) ** (1/2))\n","\n","                    value = ((absA + torch.sqrt(2 * (p.data.abs() + zero_mask) / absA - absA ** 2)) / 2) ** 3\n","\n","                    p.data = p.data.sign() * value * mask\n","\n","        return loss"]},{"cell_type":"code","execution_count":null,"id":"e2984400-11e9-45e0-83bf-2e776dc0812d","metadata":{"tags":[],"id":"e2984400-11e9-45e0-83bf-2e776dc0812d","outputId":"78c8846b-b12e-4fac-8278-9054e73d92a8"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_12234/1815375738.py:77: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)\n","  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n"]},{"name":"stdout","output_type":"stream","text":["已更新保存模型\n","Epochs 0 loss_train 11.807511844286104 loss_test 0.9608611464500427 每轮耗时： 2.696747064590454\n","Epochs 1 loss_train 1.0263680743008126 loss_test 1.022716325521469 每轮耗时： 2.139242649078369\n","Epochs 2 loss_train 0.7847797725258804 loss_test 1.1344991445541381 每轮耗时： 2.163578748703003\n","Epochs 3 loss_train 0.6241064638626285 loss_test 1.1654046773910522 每轮耗时： 2.1543164253234863\n","Epochs 4 loss_train 0.5401823513391542 loss_test 1.066801017522812 每轮耗时： 2.1954596042633057\n","Epochs 5 loss_train 0.4629551979099832 loss_test 1.2285907030105592 每轮耗时： 2.113521099090576\n","Epochs 6 loss_train 0.38179868168947173 loss_test 1.1009924769401551 每轮耗时： 2.1219799518585205\n","Epochs 7 loss_train 0.35302598520023065 loss_test 1.1940136194229125 每轮耗时： 2.120434522628784\n","Epochs 8 loss_train 0.2598665614680546 loss_test 1.0649650335311889 每轮耗时： 2.110954999923706\n","Epochs 9 loss_train 0.1862877278429706 loss_test 1.1671190559864044 每轮耗时： 2.1552650928497314\n","Epochs 10 loss_train 0.18443967401981354 loss_test 1.1265849947929383 每轮耗时： 2.1233057975769043\n","Epochs 11 loss_train 0.16065896029879406 loss_test 1.1300943076610566 每轮耗时： 2.1934282779693604\n","Epochs 12 loss_train 0.1605354153165003 loss_test 1.1671700716018676 每轮耗时： 2.1275830268859863\n","Epochs 13 loss_train 0.11174464625556295 loss_test 1.0891939520835876 每轮耗时： 2.134819507598877\n","Epochs 14 loss_train 0.10706990457526068 loss_test 1.1556869626045227 每轮耗时： 2.1534807682037354\n","Epochs 15 loss_train 0.10138511866694544 loss_test 1.0154760420322417 每轮耗时： 2.120931386947632\n","Epochs 16 loss_train 0.08614096527055996 loss_test 1.1064986288547516 每轮耗时： 2.121445655822754\n","Epochs 17 loss_train 0.06665989311366546 loss_test 1.028354573249817 每轮耗时： 2.1192867755889893\n","Epochs 18 loss_train 0.05004202029327067 loss_test 1.0314402401447296 每轮耗时： 2.111638069152832\n","Epochs 19 loss_train 0.04608808412421041 loss_test 1.1267309308052063 每轮耗时： 2.163785219192505\n","Epochs 20 loss_train 0.07472516996104543 loss_test 1.1547520101070403 每轮耗时： 2.1181459426879883\n","Epochs 21 loss_train 0.061586331848691146 loss_test 1.071826881170273 每轮耗时： 2.139983892440796\n","Epochs 22 loss_train 0.056491682941957215 loss_test 1.0770958006381988 每轮耗时： 2.136540412902832\n","Epochs 23 loss_train 0.05149657365570708 loss_test 1.0290366053581237 每轮耗时： 2.1534299850463867\n","Epochs 24 loss_train 0.04983956771107709 loss_test 0.9798686921596527 每轮耗时： 2.1220643520355225\n","Epochs 25 loss_train 0.055001127919772776 loss_test 1.0420640110969543 每轮耗时： 2.128697395324707\n","Epochs 26 loss_train 0.04637452778292865 loss_test 0.9928208947181701 每轮耗时： 2.1521692276000977\n","Epochs 27 loss_train 0.0466379512373994 loss_test 1.0258362233638763 每轮耗时： 2.1235880851745605\n","Epochs 28 loss_train 0.049503311015120365 loss_test 0.9983136057853699 每轮耗时： 2.100498914718628\n","Epochs 29 loss_train 0.06421863987314992 loss_test 1.0262244582176208 每轮耗时： 2.122899293899536\n","Epochs 30 loss_train 0.05496742544559444 loss_test 1.0122507214546204 每轮耗时： 2.171323537826538\n","Epochs 31 loss_train 0.0716787356338123 loss_test 1.0356775164604186 每轮耗时： 2.1126532554626465\n","Epochs 32 loss_train 0.0639667868523336 loss_test 1.0116288125514985 每轮耗时： 2.1305792331695557\n","Epochs 33 loss_train 0.04979135850217284 loss_test 1.0027403473854064 每轮耗时： 2.168642520904541\n","Epochs 34 loss_train 0.04534525946691269 loss_test 1.0059059381484985 每轮耗时： 2.145030975341797\n","Epochs 35 loss_train 0.04122100570579854 loss_test 0.9915054857730865 每轮耗时： 2.116353988647461\n","Epochs 36 loss_train 0.03508797919423115 loss_test 0.9997309505939483 每轮耗时： 2.1241989135742188\n","Epochs 37 loss_train 0.03423050354893615 loss_test 1.1114836812019349 每轮耗时： 2.110457181930542\n","Epochs 38 loss_train 0.03678413021673516 loss_test 1.017311578989029 每轮耗时： 2.1065986156463623\n","Epochs 39 loss_train 0.03231883607804775 loss_test 0.9719035744667053 每轮耗时： 2.1218128204345703\n","Epochs 40 loss_train 0.030192361235982033 loss_test 1.0087189495563507 每轮耗时： 2.1138970851898193\n","Epochs 41 loss_train 0.03288200670262662 loss_test 0.9847659707069397 每轮耗时： 2.1375699043273926\n","Epochs 42 loss_train 0.03343453871585974 loss_test 1.0200567245483398 每轮耗时： 2.1319003105163574\n","Epochs 43 loss_train 0.04225818885535729 loss_test 1.0176862537860871 每轮耗时： 2.1589553356170654\n","Epochs 44 loss_train 0.037015306831496515 loss_test 1.010427463054657 每轮耗时： 2.1365628242492676\n","Epochs 45 loss_train 0.034924176726035955 loss_test 0.995692926645279 每轮耗时： 2.118011236190796\n","Epochs 46 loss_train 0.030167759237129513 loss_test 1.0001427114009858 每轮耗时： 2.1239941120147705\n","Epochs 47 loss_train 0.03352940570926521 loss_test 0.9908204078674316 每轮耗时： 2.126054525375366\n","Epochs 48 loss_train 0.03594124626095702 loss_test 0.9958430409431458 每轮耗时： 2.1238532066345215\n","Epochs 49 loss_train 0.04062036720172661 loss_test 0.9762567579746246 每轮耗时： 2.1355676651000977\n","Epochs 50 loss_train 0.035853876118979805 loss_test 0.9628147602081298 每轮耗时： 2.1357734203338623\n","Epochs 51 loss_train 0.034558858949600195 loss_test 1.0020872056484222 每轮耗时： 2.1136975288391113\n","Epochs 52 loss_train 0.031010738846550628 loss_test 0.9917893648147583 每轮耗时： 2.129978656768799\n","Epochs 53 loss_train 0.03022668515218467 loss_test 0.9638311922550201 每轮耗时： 2.1146016120910645\n","Epochs 54 loss_train 0.03327724880470735 loss_test 0.9696709334850311 每轮耗时： 2.1788389682769775\n","Epochs 55 loss_train 0.043701995073295224 loss_test 0.9935335099697113 每轮耗时： 2.1353631019592285\n","Epochs 56 loss_train 0.04066945907727974 loss_test 1.0183279037475585 每轮耗时： 2.115276336669922\n","Epochs 57 loss_train 0.03465174356611764 loss_test 0.981933867931366 每轮耗时： 2.112316608428955\n","Epochs 58 loss_train 0.03129174891950154 loss_test 0.9997046291828156 每轮耗时： 2.1317081451416016\n","已更新保存模型\n","Epochs 59 loss_train 0.032830234935007446 loss_test 0.9509263217449189 每轮耗时： 2.19092059135437\n","已更新保存模型\n","Epochs 60 loss_train 0.028562443103732134 loss_test 0.9489569962024689 每轮耗时： 2.1982381343841553\n","Epochs 61 loss_train 0.029192803881880714 loss_test 0.9830400168895721 每轮耗时： 2.125875473022461\n","已更新保存模型\n","Epochs 62 loss_train 0.030302016337106868 loss_test 0.9469394683837891 每轮耗时： 2.2080721855163574\n","Epochs 63 loss_train 0.030248259307771194 loss_test 0.97506303191185 每轮耗时： 2.1137008666992188\n","Epochs 64 loss_train 0.03185991156937146 loss_test 0.9664613306522369 每轮耗时： 2.1227381229400635\n","Epochs 65 loss_train 0.031913586642320566 loss_test 1.0357994318008423 每轮耗时： 2.1303398609161377\n","Epochs 66 loss_train 0.0362302158482191 loss_test 0.9648820996284485 每轮耗时： 2.1175036430358887\n","Epochs 67 loss_train 0.039956395173581634 loss_test 0.9883996427059174 每轮耗时： 2.1444787979125977\n","Epochs 68 loss_train 0.040836667187693645 loss_test 0.9568452775478363 每轮耗时： 2.212789535522461\n","Epochs 69 loss_train 0.03685810485082429 loss_test 0.985881096124649 每轮耗时： 2.1338372230529785\n","Epochs 70 loss_train 0.030894343309649606 loss_test 1.0147016227245331 每轮耗时： 2.153643846511841\n","Epochs 71 loss_train 0.0256806154076646 loss_test 0.9509796500205994 每轮耗时： 2.1278414726257324\n","Epochs 72 loss_train 0.027900687350732523 loss_test 0.9839883089065552 每轮耗时： 2.1519691944122314\n","已更新保存模型\n","Epochs 73 loss_train 0.02351470854951114 loss_test 0.9447433710098266 每轮耗时： 2.1747474670410156\n","Epochs 74 loss_train 0.019877618496737828 loss_test 0.9678922355175018 每轮耗时： 2.1290738582611084\n","Epochs 75 loss_train 0.01923371465286104 loss_test 0.966704499721527 每轮耗时： 2.1140942573547363\n","Epochs 76 loss_train 0.020891768562539322 loss_test 0.9830956578254699 每轮耗时： 2.10478138923645\n","Epochs 77 loss_train 0.021909871184062665 loss_test 0.9646817147731781 每轮耗时： 2.1295247077941895\n","Epochs 78 loss_train 0.022569654600285902 loss_test 0.991462379693985 每轮耗时： 2.1310629844665527\n","Epochs 79 loss_train 0.027532478408297388 loss_test 0.9885475993156433 每轮耗时： 2.1367743015289307\n","Epochs 80 loss_train 0.027956784362109695 loss_test 0.9880129575729371 每轮耗时： 2.1422998905181885\n","Epochs 81 loss_train 0.027881348573761743 loss_test 0.9870961666107178 每轮耗时： 2.1672215461730957\n","Epochs 82 loss_train 0.026462766750738387 loss_test 1.0037806451320648 每轮耗时： 2.1235718727111816\n","Epochs 83 loss_train 0.02653253221566357 loss_test 0.9632356822490692 每轮耗时： 2.1451480388641357\n","Epochs 84 loss_train 0.024258862340413943 loss_test 0.9749761104583741 每轮耗时： 2.150635004043579\n","Epochs 85 loss_train 0.02179165820523006 loss_test 0.9560955882072448 每轮耗时： 2.1034440994262695\n","Epochs 86 loss_train 0.020270908115113655 loss_test 0.9875383973121643 每轮耗时： 2.120759963989258\n","Epochs 87 loss_train 0.020839198992201467 loss_test 0.9599305510520935 每轮耗时： 2.110588312149048\n","Epochs 88 loss_train 0.02163414938784227 loss_test 0.974739420413971 每轮耗时： 2.1075029373168945\n","Epochs 89 loss_train 0.02211487057005487 loss_test 0.9777154505252839 每轮耗时： 2.1289620399475098\n","Epochs 90 loss_train 0.024682367538533558 loss_test 0.9920089721679688 每轮耗时： 2.112490177154541\n","Epochs 91 loss_train 0.022328332202827057 loss_test 0.9779789268970489 每轮耗时： 2.1305947303771973\n","Epochs 92 loss_train 0.02016520311647072 loss_test 0.9840793967247009 每轮耗时： 2.1259565353393555\n","Epochs 93 loss_train 0.019658123120302108 loss_test 0.9598573327064515 每轮耗时： 2.1026413440704346\n","Epochs 94 loss_train 0.021439604075034945 loss_test 0.9499113380908966 每轮耗时： 2.1149580478668213\n","Epochs 95 loss_train 0.018209753558039665 loss_test 0.9795168817043305 每轮耗时： 2.1262850761413574\n","Epochs 96 loss_train 0.016717696244396816 loss_test 0.9650204241275787 每轮耗时： 2.1164562702178955\n","Epochs 97 loss_train 0.017125734264349064 loss_test 0.9571491956710816 每轮耗时： 2.1583430767059326\n","Epochs 98 loss_train 0.01942574721193168 loss_test 0.9771074891090393 每轮耗时： 2.123807907104492\n","Epochs 99 loss_train 0.01995691082372171 loss_test 0.9537988066673279 每轮耗时： 2.1307191848754883\n","总耗时: 214.03030633926392\n"]}],"source":["Epochs                = 100      # 训练轮数\n","l                     = 0.01\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","def seed_torch():\n","    seed=1029\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed) # 为了禁止hash随机化，使得实验可复现\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","seed_torch()\n","model = ResNet18()\n","model = model.to(device)\n","loss_function = nn.MSELoss()  # loss\n","optimizer = AdamL12(model.parameters(), lr=l)  # 优化器\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=9, gamma=0.95)\n","LOSS = float('inf')\n","result = {}\n","result['train-loss']= []\n","result['test-loss']= []\n","start_time1 = time.time()\n","for epochs in range(Epochs):\n","    start_time = time.time()\n","    model.train()\n","    train_loss = 0\n","    test_loss  = 0\n","    for data_l in train_loader:\n","        seq, labels = data_l\n","        seq, labels = seq.to(device), labels.to(device)\n","        optimizer.zero_grad()                          #    清空过往梯度\n","        y_pred = model(seq)\n","        single_loss = loss_function(y_pred, labels)    #    获取loss：输入预测值和标签，计算损失函数\n","        single_loss.backward()                         #    反向传播，计算当前梯度\n","        optimizer.step()                               #    根据梯度更新网络参数\n","        train_loss += single_loss.item()\n","    train_loss = train_loss/len(train_loader)\n","    scheduler.step()\n","    model.eval()\n","    for data_l in test_loader:\n","        seq, labels = data_l\n","        seq, labels = seq.to(device), labels.to(device)\n","        y_pred = model(seq)\n","        single_loss = loss_function(y_pred, labels)    #    获取loss：输入预测值和标签，计算损失函数\n","        test_loss += single_loss.item()\n","    test_loss = test_loss/len(test_loader)\n","    if LOSS > test_loss:\n","        LOSS =test_loss\n","        torch.save(model, 'model_cnn1.pth')\n","        print('已更新保存模型')\n","    result['train-loss'].append(train_loss)\n","    result['test-loss'].append(test_loss)\n","    del seq, labels ,y_pred #删除数据与变量\n","    gc.collect() #清除数据与变量相关的缓存\n","    torch.cuda.empty_cache() #缓存分配器分配出去的内存给释放掉\n","    epoch_time = time.time() - start_time\n","    print('Epochs',epochs,'loss_train',train_loss,'loss_test',test_loss,'每轮耗时：',epoch_time)\n","all_time = time.time() - start_time1\n","print('总耗时:',all_time)"]},{"cell_type":"code","execution_count":null,"id":"b363e603-46e2-4005-b455-d041d109dbe3","metadata":{"tags":[],"id":"b363e603-46e2-4005-b455-d041d109dbe3"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.figure(dpi=150,figsize=(7,4))\n","plt.plot(result['train-loss'][:], label='train')\n","plt.plot(result['test-loss'][:], label='test')\n","plt.xlabel('iteration')\n","plt.ylabel('loss')\n","plt.title('Training and Testing Loss')\n","plt.legend()\n","plt.savefig('resnet18loss.jpg',dpi=150)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"591ad040-6a5f-417c-9da2-5b5957f77ded","metadata":{"tags":[],"id":"591ad040-6a5f-417c-9da2-5b5957f77ded"},"outputs":[],"source":["\n","def load_model(model_path):\n","\n","    if torch.cuda.is_available():\n","        model = torch.load(model_path)\n","    else:\n","        model = torch.load(model_path, map_location=torch.device('cpu'))\n","\n","    model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n","    return model\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = load_model('model_cnn1.pth')\n","pre = model(X_test.to(device))\n","pre = pre.cpu().detach().numpy()\n","Y_test1 = Y_test.cpu().detach().numpy().copy()\n","for i in range(pre.shape[1]):\n","    pre[:,i] = pre[:,i]*Std[i] +Mean[i]\n","    Y_test1[:,i] = Y_test1[:,i]*Std[i] +Mean[i]\n","    plt.figure(dpi=150,figsize=(7,4))\n","    plt.plot(Y_test1[:,i], label='True Values', color='blue')\n","    plt.plot(pre[:,i], label='Predictions', linestyle='--', color='red')\n","    plt.title('True Values vs Predictions')\n","    plt.xlabel('Data Point')\n","    plt.ylabel('Value')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig('cnn_对比图_{}.jpg'.format(i),dpi=150)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"id":"f6e5cdf9-2e9d-450e-8904-4c3e83e61e09","metadata":{"tags":[],"id":"f6e5cdf9-2e9d-450e-8904-4c3e83e61e09"},"outputs":[],"source":["def Evaluation_index(Y_test1,pre):\n","    from sklearn.metrics import r2_score, mean_squared_error,explained_variance_score,mean_absolute_error\n","    r2 = r2_score(Y_test1,pre)\n","    ev = explained_variance_score(Y_test1,pre)\n","    mse = mean_squared_error(Y_test1,pre)\n","    rmse = np.sqrt(mse)\n","    mae = mean_absolute_error(Y_test1,pre)\n","\n","    pre = pre.reshape(-1)\n","    Y_test1 = Y_test1.reshape(-1)\n","    INDEX = []\n","    page = 0\n","    for i in Y_test1:\n","        if i ==0:\n","            INDEX.append(page)\n","        page +=1\n","    if INDEX !=[]:\n","        Y_test1 = np.delete(Y_test1,INDEX,0)\n","        pre     = np.delete(pre,INDEX,0)\n","    mape = (sum(abs((pre - Y_test1)/(Y_test1)))/len(Y_test1))\n","    evaluation_index = pd.DataFrame()\n","    evaluation_index['评估指标名称'] = ['r2','ev','mse','rmse','mae','mape']\n","    evaluation_index['评估指标值'] = [r2,ev,mse,rmse,mae,mape]\n","    print('r2:',r2)\n","    print('ev:',ev)\n","    print('mse:',mse)\n","    print('rmse:',rmse)\n","    print('mae:',mae)\n","    print('mape:',mape)\n","    return evaluation_index\n","for i in range(2):\n","    evaluation_index = Evaluation_index(Y_test1[:,i],pre[:,i])\n","    evaluation_index.to_csv(f'evaluation_index{i}.csv',index = False)\n","    print('-----------------')"]},{"cell_type":"code","execution_count":null,"id":"1c3642a0-1036-493a-9450-c8eba508fa7a","metadata":{"tags":[],"id":"1c3642a0-1036-493a-9450-c8eba508fa7a"},"outputs":[],"source":["evaluation_index = Evaluation_index(Y_test1,pre)"]},{"cell_type":"code","execution_count":null,"id":"edda82e3-ed71-4c80-b0c5-927f2717d8b7","metadata":{"id":"edda82e3-ed71-4c80-b0c5-927f2717d8b7"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"7cfa6612-37ac-49d8-8431-8049dc66bf64","metadata":{"id":"7cfa6612-37ac-49d8-8431-8049dc66bf64"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"ef0cfb90","metadata":{"id":"ef0cfb90"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}