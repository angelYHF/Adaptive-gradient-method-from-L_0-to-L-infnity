{"cells":[{"cell_type":"code","execution_count":null,"id":"6d30e714","metadata":{"tags":[],"id":"6d30e714"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import random,os\n","import torch.utils.data as Data\n","from sklearn.metrics import f1_score,recall_score,precision_score,roc_curve,auc,accuracy_score,confusion_matrix,r2_score, mean_squared_error\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import torch\n","# import torch.nn as nn\n","from torch import nn,optim\n","import torch.nn.functional as F\n","from torch.optim.optimizer import Optimizer, required\n","import gc\n","import time\n","import math\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"id":"d579e9d1","metadata":{"id":"d579e9d1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"f02da321","metadata":{"id":"f02da321"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"02360f62","metadata":{"tags":[],"id":"02360f62"},"outputs":[],"source":["'''ResNet in PyTorch.\n","\n","For Pre-activation ResNet, see 'preact_resnet.py'.\n","\n","Reference:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","'''\n","\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=2):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=5, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=5, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        # print('1',out.shape)\n","        out = self.bn2(self.conv2(out))\n","        # print('2',out.shape)\n","        # print(.shape)\n","        x1 = self.shortcut(x)\n","        if x1.shape[-1] != out.shape[-1]:\n","            x1 = x1[:,:,:out.shape[-1],:out.shape[-1]]\n","        out += x1\n","        # print('3',out.shape)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=5):\n","        super(ResNet, self).__init__()\n","        self.input_channel = 1\n","        self.in_planes = 64\n","\n","\n","        self.conv1 = nn.Conv2d(self.input_channel, self.in_planes, kernel_size=3, stride=1, padding=0, bias=False)\n","        self.bn1 = nn.BatchNorm2d(self.in_planes)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=2)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*2, num_classes)\n","        self.dropout = nn.Dropout(0.2)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        # print(out.shape)\n","        out = self.layer1(out)\n","        # out = self.dropout(out)\n","        out = self.layer2(out)\n","        # out = self.dropout(out)\n","        out = self.layer3(out)\n","        # out = self.dropout(out)\n","        # out = self.layer4(out)\n","        # out = self.dropout(out)\n","        # out = F.avg_pool2d(out, 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","    def reg_loss(self):\n","        reg_loss = 0.0\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","                reg_loss += torch.sum(torch.abs(m.weight))\n","        return reg_loss\n","\n","    def l1reg_loss(self):\n","        reg_loss = 0.0\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","                reg_loss += torch.sum(torch.abs(m.weight))\n","        return reg_loss\n","\n","    def l12reg_loss(self):\n","        reg_loss = 0.0\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","                reg_loss += torch.sum((torch.abs(m.weight) + 1e-6).sqrt())\n","        return reg_loss\n","\n","    def l23reg_loss(self):\n","        reg_loss = 0.0\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","                reg_loss += torch.sum((torch.abs(m.weight) + 1e-6).pow(2/3))\n","        return reg_loss\n","\n","    def exact_sparsity(self):\n","        nnz = 0\n","        total_param = 0.0\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","                total_param += np.prod(m.weight.data.shape)\n","                nnz += torch.sum(m.weight.data != 0).detach().item()\n","        ratio = nnz / total_param\n","        return ratio\n","\n","    def sparsity_level(self):\n","        nnz_2 = 0.0\n","        nnz_3 = 0.0\n","        nnz_4 = 0.0\n","        total_param = 0.0\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","                total_param += np.prod(m.weight.data.shape)\n","                nnz_2 += torch.sum(m.weight.data.abs() >= 0.01).detach().item()\n","                nnz_3 += torch.sum(m.weight.data.abs() >= 0.001).detach().item()\n","                nnz_4 += torch.sum(m.weight.data.abs() >= 0.0001).detach().item()\n","        ratio_2 = nnz_2 / total_param\n","        ratio_3 = nnz_3 / total_param\n","        ratio_4 = nnz_4 / total_param\n","        return ratio_2, ratio_3\n","\n","\n","def ResNet18():\n","    return ResNet(BasicBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","def test():\n","    net = ResNet18()\n","    y = net(torch.randn(1,3,32,32))\n","    print(y.size())"]},{"cell_type":"code","execution_count":null,"id":"218a2306-0631-4c66-a17f-cdac76369d08","metadata":{"scrolled":true,"tags":[],"id":"218a2306-0631-4c66-a17f-cdac76369d08"},"outputs":[],"source":["net = ResNet18()\n","net(torch.rand([35,1,529,529]))"]},{"cell_type":"code","execution_count":null,"id":"dbf65941","metadata":{"tags":[],"id":"dbf65941","outputId":"0ae2130b-fda0-487c-d62b-5760c4c8ea77"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_27884/1669737403.py:3: DtypeWarning: Columns (0,1,2,3,4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv(path_name,header=None)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>52839</th>\n","      <th>52840</th>\n","      <th>52841</th>\n","      <th>52842</th>\n","      <th>52843</th>\n","      <th>52844</th>\n","      <th>52845</th>\n","      <th>52846</th>\n","      <th>52847</th>\n","      <th>52848</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ID</td>\n","      <td>t1</td>\n","      <td>t2</td>\n","      <td>t3</td>\n","      <td>t4</td>\n","      <td>t5</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>...</td>\n","      <td>52834.0</td>\n","      <td>52835.0</td>\n","      <td>52836.0</td>\n","      <td>52837.0</td>\n","      <td>52838.0</td>\n","      <td>52839.0</td>\n","      <td>52840.0</td>\n","      <td>52841.0</td>\n","      <td>52842.0</td>\n","      <td>52843.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1136</td>\n","      <td>-0.8687035</td>\n","      <td>0.28158495</td>\n","      <td>1.71659645</td>\n","      <td>-0.8861064</td>\n","      <td>47.6114438</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>...</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1318</td>\n","      <td>0.13144417</td>\n","      <td>-1.7468354</td>\n","      <td>0.95272593</td>\n","      <td>0.15843581</td>\n","      <td>-41.940212</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1319</td>\n","      <td>-0.8536013</td>\n","      <td>-2.9968045</td>\n","      <td>-0.0871332</td>\n","      <td>-1.2177578</td>\n","      <td>100.653792</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1321</td>\n","      <td>0.03416785</td>\n","      <td>-0.320321</td>\n","      <td>-0.071189</td>\n","      <td>-1.4672803</td>\n","      <td>13.8808255</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2310</th>\n","      <td>6466</td>\n","      <td>0.045294</td>\n","      <td>1.386794</td>\n","      <td>2.291255</td>\n","      <td>0.226449</td>\n","      <td>-23.831346</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2311</th>\n","      <td>6467</td>\n","      <td>-1.199029</td>\n","      <td>-0.565144</td>\n","      <td>2.225259</td>\n","      <td>-5.672833</td>\n","      <td>85.793067</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2312</th>\n","      <td>6469</td>\n","      <td>-0.964449</td>\n","      <td>0.673534</td>\n","      <td>1.754374</td>\n","      <td>-1.489569</td>\n","      <td>-8.871632</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2313</th>\n","      <td>6471</td>\n","      <td>-0.832078</td>\n","      <td>-1.40681</td>\n","      <td>2.368887</td>\n","      <td>-1.517962</td>\n","      <td>51.492272</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2314</th>\n","      <td>6473</td>\n","      <td>0.340885</td>\n","      <td>-0.195353</td>\n","      <td>0.372499</td>\n","      <td>-1.375068</td>\n","      <td>114.904649</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2315 rows × 52849 columns</p>\n","</div>"],"text/plain":["     0           1           2           3           4           5      6      \\\n","0       ID          t1          t2          t3          t4          t5    1.0   \n","1     1136  -0.8687035  0.28158495  1.71659645  -0.8861064  47.6114438    1.0   \n","2     1318  0.13144417  -1.7468354  0.95272593  0.15843581  -41.940212    1.0   \n","3     1319  -0.8536013  -2.9968045  -0.0871332  -1.2177578  100.653792    1.0   \n","4     1321  0.03416785   -0.320321   -0.071189  -1.4672803  13.8808255    1.0   \n","...    ...         ...         ...         ...         ...         ...    ...   \n","2310  6466    0.045294    1.386794    2.291255    0.226449  -23.831346    0.0   \n","2311  6467   -1.199029   -0.565144    2.225259   -5.672833   85.793067    1.0   \n","2312  6469   -0.964449    0.673534    1.754374   -1.489569   -8.871632    2.0   \n","2313  6471   -0.832078    -1.40681    2.368887   -1.517962   51.492272    1.0   \n","2314  6473    0.340885   -0.195353    0.372499   -1.375068  114.904649    0.0   \n","\n","      7      8      9      ...    52839    52840    52841    52842    52843  \\\n","0       2.0    3.0    4.0  ...  52834.0  52835.0  52836.0  52837.0  52838.0   \n","1       2.0    1.0    2.0  ...      2.0      1.0      1.0      2.0      1.0   \n","2       2.0    1.0    0.0  ...      0.0      1.0      1.0      2.0      2.0   \n","3       2.0    1.0    1.0  ...      0.0      1.0      1.0      2.0      0.0   \n","4       2.0    0.0    0.0  ...      1.0      1.0      1.0      2.0      1.0   \n","...     ...    ...    ...  ...      ...      ...      ...      ...      ...   \n","2310    1.0    2.0    1.0  ...      0.0      0.0      1.0      0.0      2.0   \n","2311    2.0    1.0    2.0  ...      1.0      1.0      2.0      0.0      0.0   \n","2312    2.0    2.0    1.0  ...      0.0      1.0      1.0      1.0      1.0   \n","2313    1.0    2.0    1.0  ...      1.0      1.0      2.0      1.0      1.0   \n","2314    2.0    2.0    1.0  ...      1.0      0.0      1.0      1.0      1.0   \n","\n","        52844    52845    52846    52847    52848  \n","0     52839.0  52840.0  52841.0  52842.0  52843.0  \n","1         1.0      0.0      2.0      0.0      1.0  \n","2         2.0      1.0      2.0      1.0      2.0  \n","3         2.0      1.0      2.0      1.0      2.0  \n","4         2.0      1.0      2.0      1.0      2.0  \n","...       ...      ...      ...      ...      ...  \n","2310      2.0      0.0      2.0      1.0      2.0  \n","2311      2.0      0.0      2.0      2.0      2.0  \n","2312      2.0      1.0      2.0      2.0      0.0  \n","2313      2.0      0.0      2.0      0.0      2.0  \n","2314      2.0      2.0      2.0      0.0      2.0  \n","\n","[2315 rows x 52849 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["path1='/root/ResNet/'\n","path_name = os.path.join(path1,'pig_data.csv')\n","df = pd.read_csv(path_name,header=None)\n","df"]},{"cell_type":"code","execution_count":null,"id":"8a69809d-5ae5-46a8-aefa-903f085df1a7","metadata":{"tags":[],"id":"8a69809d-5ae5-46a8-aefa-903f085df1a7","outputId":"cb1cc460-f87b-466d-a351-2d825dc33915"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>t1</th>\n","      <th>t2</th>\n","      <th>t3</th>\n","      <th>t4</th>\n","      <th>t5</th>\n","      <th>1.0</th>\n","      <th>2.0</th>\n","      <th>3.0</th>\n","      <th>4.0</th>\n","      <th>...</th>\n","      <th>52834.0</th>\n","      <th>52835.0</th>\n","      <th>52836.0</th>\n","      <th>52837.0</th>\n","      <th>52838.0</th>\n","      <th>52839.0</th>\n","      <th>52840.0</th>\n","      <th>52841.0</th>\n","      <th>52842.0</th>\n","      <th>52843.0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1136</td>\n","      <td>-0.8687035</td>\n","      <td>0.28158495</td>\n","      <td>1.71659645</td>\n","      <td>-0.8861064</td>\n","      <td>47.6114438</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>...</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1318</td>\n","      <td>0.13144417</td>\n","      <td>-1.7468354</td>\n","      <td>0.95272593</td>\n","      <td>0.15843581</td>\n","      <td>-41.940212</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1319</td>\n","      <td>-0.8536013</td>\n","      <td>-2.9968045</td>\n","      <td>-0.0871332</td>\n","      <td>-1.2177578</td>\n","      <td>100.653792</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1321</td>\n","      <td>0.03416785</td>\n","      <td>-0.320321</td>\n","      <td>-0.071189</td>\n","      <td>-1.4672803</td>\n","      <td>13.8808255</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1322</td>\n","      <td>0.57391141</td>\n","      <td>-2.0458374</td>\n","      <td>0.054743</td>\n","      <td>0.16402708</td>\n","      <td>-58.738674</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2309</th>\n","      <td>6466</td>\n","      <td>0.045294</td>\n","      <td>1.386794</td>\n","      <td>2.291255</td>\n","      <td>0.226449</td>\n","      <td>-23.831346</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2310</th>\n","      <td>6467</td>\n","      <td>-1.199029</td>\n","      <td>-0.565144</td>\n","      <td>2.225259</td>\n","      <td>-5.672833</td>\n","      <td>85.793067</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2311</th>\n","      <td>6469</td>\n","      <td>-0.964449</td>\n","      <td>0.673534</td>\n","      <td>1.754374</td>\n","      <td>-1.489569</td>\n","      <td>-8.871632</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2312</th>\n","      <td>6471</td>\n","      <td>-0.832078</td>\n","      <td>-1.40681</td>\n","      <td>2.368887</td>\n","      <td>-1.517962</td>\n","      <td>51.492272</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2313</th>\n","      <td>6473</td>\n","      <td>0.340885</td>\n","      <td>-0.195353</td>\n","      <td>0.372499</td>\n","      <td>-1.375068</td>\n","      <td>114.904649</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2314 rows × 52849 columns</p>\n","</div>"],"text/plain":["0       ID          t1          t2          t3          t4          t5  1.0  \\\n","0     1136  -0.8687035  0.28158495  1.71659645  -0.8861064  47.6114438  1.0   \n","1     1318  0.13144417  -1.7468354  0.95272593  0.15843581  -41.940212  1.0   \n","2     1319  -0.8536013  -2.9968045  -0.0871332  -1.2177578  100.653792  1.0   \n","3     1321  0.03416785   -0.320321   -0.071189  -1.4672803  13.8808255  1.0   \n","4     1322  0.57391141  -2.0458374    0.054743  0.16402708  -58.738674  2.0   \n","...    ...         ...         ...         ...         ...         ...  ...   \n","2309  6466    0.045294    1.386794    2.291255    0.226449  -23.831346  0.0   \n","2310  6467   -1.199029   -0.565144    2.225259   -5.672833   85.793067  1.0   \n","2311  6469   -0.964449    0.673534    1.754374   -1.489569   -8.871632  2.0   \n","2312  6471   -0.832078    -1.40681    2.368887   -1.517962   51.492272  1.0   \n","2313  6473    0.340885   -0.195353    0.372499   -1.375068  114.904649  0.0   \n","\n","0     2.0  3.0  4.0  ...  52834.0  52835.0  52836.0  52837.0  52838.0  \\\n","0     2.0  1.0  2.0  ...      2.0      1.0      1.0      2.0      1.0   \n","1     2.0  1.0  0.0  ...      0.0      1.0      1.0      2.0      2.0   \n","2     2.0  1.0  1.0  ...      0.0      1.0      1.0      2.0      0.0   \n","3     2.0  0.0  0.0  ...      1.0      1.0      1.0      2.0      1.0   \n","4     2.0  1.0  1.0  ...      0.0      1.0      2.0      2.0      1.0   \n","...   ...  ...  ...  ...      ...      ...      ...      ...      ...   \n","2309  1.0  2.0  1.0  ...      0.0      0.0      1.0      0.0      2.0   \n","2310  2.0  1.0  2.0  ...      1.0      1.0      2.0      0.0      0.0   \n","2311  2.0  2.0  1.0  ...      0.0      1.0      1.0      1.0      1.0   \n","2312  1.0  2.0  1.0  ...      1.0      1.0      2.0      1.0      1.0   \n","2313  2.0  2.0  1.0  ...      1.0      0.0      1.0      1.0      1.0   \n","\n","0     52839.0  52840.0  52841.0  52842.0  52843.0  \n","0         1.0      0.0      2.0      0.0      1.0  \n","1         2.0      1.0      2.0      1.0      2.0  \n","2         2.0      1.0      2.0      1.0      2.0  \n","3         2.0      1.0      2.0      1.0      2.0  \n","4         2.0      2.0      2.0      2.0      2.0  \n","...       ...      ...      ...      ...      ...  \n","2309      2.0      0.0      2.0      1.0      2.0  \n","2310      2.0      0.0      2.0      2.0      2.0  \n","2311      2.0      1.0      2.0      2.0      0.0  \n","2312      2.0      0.0      2.0      0.0      2.0  \n","2313      2.0      2.0      2.0      0.0      2.0  \n","\n","[2314 rows x 52849 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df.columns = df.iloc[0,:]\n","df = df.iloc[1:,:]\n","df.index = range(len(df))\n","df"]},{"cell_type":"code","execution_count":null,"id":"e40bdb87","metadata":{"tags":[],"id":"e40bdb87"},"outputs":[],"source":["X = df.iloc[:,6:]\n","Y = df.iloc[:,1:6]\n","Y"]},{"cell_type":"code","execution_count":null,"id":"59a67a68-bef8-49ee-9d91-06d8eb28f0db","metadata":{"tags":[],"id":"59a67a68-bef8-49ee-9d91-06d8eb28f0db"},"outputs":[],"source":["Y = Y.astype('float')"]},{"cell_type":"code","execution_count":null,"id":"bff63ca3","metadata":{"tags":[],"id":"bff63ca3","outputId":"f6acf79a-80d8-4cdf-ca21-db59c2af5c75"},"outputs":[{"name":"stderr","output_type":"stream","text":["/root/miniconda3/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n"]}],"source":["from sklearn.preprocessing import OneHotEncoder\n","\n","\n","# 初始化OneHotEncoder\n","encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n","\n","# 选择要进行独热编码的列\n","cols_to_encode = df.columns[6:]\n","\n","# 拟合训练集\n","encoder.fit(X)\n","\n","# 对训练集和测试集进行转换\n","X_encoded = encoder.transform(X)\n"]},{"cell_type":"code","execution_count":null,"id":"fbe02c31-de55-48d2-b345-9958b357568e","metadata":{"tags":[],"id":"fbe02c31-de55-48d2-b345-9958b357568e","outputId":"e5ce4124-e87c-4c71-9346-4a48a03972a4"},"outputs":[{"data":{"text/plain":["(2314, 279995)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["X_encoded.shape"]},{"cell_type":"code","execution_count":null,"id":"36ae30d0-8f57-4a41-a271-a6ce80c08551","metadata":{"tags":[],"id":"36ae30d0-8f57-4a41-a271-a6ce80c08551"},"outputs":[],"source":["import numpy as np\n","\n","# 假设data是一个形状为(2314, 279995)的数组\n","# 裁剪数据以使列数为279841\n","data_trimmed = X_encoded[:, :279841]\n","\n","# 重塑数据为(2314, 529, 529)\n","data_reshaped = data_trimmed.reshape(2314,1, 529, 529)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"368149f5-a89c-4514-b44b-1b090dac53ae","metadata":{"id":"368149f5-a89c-4514-b44b-1b090dac53ae"},"outputs":[],"source":["\n","# # 假设data是一个形状为(N, 30987)的数组\n","# padding = np.zeros((X_encoded.shape[0], 31329 - X_encoded.shape[1]))\n","# data_padded = np.concatenate([X_encoded, padding], axis=1)\n","# # 然后，将数据重塑为(n, 177, 177, 1)\n","# data_reshaped = data_padded.reshape(-1, 1,177, 177)\n","# data_reshaped.shape"]},{"cell_type":"code","execution_count":null,"id":"03d20b9c","metadata":{"tags":[],"id":"03d20b9c"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, Y_train, Y_test = train_test_split(data_reshaped,Y , test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"id":"d5b15dab","metadata":{"tags":[],"id":"d5b15dab"},"outputs":[],"source":["def z_score(data):\n","    data = data.astype(float)\n","    Mean = data.mean()\n","    Var = ((data - Mean)**2).mean()\n","    Std = pow(Var,0.5)\n","    data = (data - Mean)/Std  # 标准化\n","    return Mean,Std,data\n","Mean,Std,Y_train = z_score(Y_train)\n","Y_test = (Y_test - Mean)/Std"]},{"cell_type":"code","execution_count":null,"id":"c7fc6a2b","metadata":{"tags":[],"id":"c7fc6a2b"},"outputs":[],"source":["Y_train = Y_train.values\n","Y_test = Y_test.values"]},{"cell_type":"code","execution_count":null,"id":"8e2f2c83","metadata":{"tags":[],"id":"8e2f2c83","outputId":"0504c4c0-b880-466c-99a7-42876ec10d1b"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1851, 1, 529, 529]) torch.Size([1851, 5]) torch.Size([463, 1, 529, 529]) torch.Size([463, 5])\n"]}],"source":["Y_train = np.squeeze(Y_train)\n","Y_test = np.squeeze(Y_test)\n","\n","X_train = torch.tensor(X_train,dtype = torch.float)\n","Y_train  = torch.tensor(Y_train,dtype = torch.float)\n","\n","X_test = torch.tensor(X_test,dtype = torch.float)\n","Y_test  = torch.tensor(Y_test,dtype = torch.float)\n","print(X_train.shape, Y_train.shape,X_test.shape, Y_test.shape)"]},{"cell_type":"code","execution_count":null,"id":"4a2b0600-2ee9-4e71-8b99-1e3e0ee08007","metadata":{"tags":[],"id":"4a2b0600-2ee9-4e71-8b99-1e3e0ee08007"},"outputs":[],"source":["train_loader = Data.DataLoader(\n","    dataset=Data.TensorDataset(X_train, Y_train),  # 封装进Data.TensorDataset()类的数据，可以为任意维度\n","    batch_size=35,  # 每块的大小\n","    shuffle=True,\n","    drop_last =True, #丢弃最后一组数据\n","    num_workers=0,  # 多进程（multiprocess）来读数据\n",")\n","test_loader = Data.DataLoader(\n","    dataset=Data.TensorDataset(X_test, Y_test),  # 封装进Data.TensorDataset()类的数据，可以为任意维度\n","    batch_size=35,  # 每块的大小\n","    shuffle=False,\n","    drop_last =True,\n","    num_workers=0,\n",")"]},{"cell_type":"code","execution_count":null,"id":"805c74c1-6422-4f9e-821e-405793dd45da","metadata":{"tags":[],"id":"805c74c1-6422-4f9e-821e-405793dd45da"},"outputs":[],"source":["class AdamL12(Optimizer):\n","    r\"\"\" Implements Adam with L0 regularization\n","\n","    A General Family of Proximal Methods for Stochastic Preconditioned Gradient Descent\n","\n","    This family contains the Adam-type curvature estimate and\n","    L0 (non-convex, non-smooth) regularizer\n","\n","    For this optimizer, the update rule is somewhat adaptive hard-thresholding\n","    \"\"\"\n","    def __init__(self, params, lr=1e-3, betas=(0.1, 0.999), eps=1e-8,\n","                 weight_decay=1e-2, amsgrad=False, penalty=0.0):\n","        if not 0.0 <= lr:\n","            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n","        if not 0.0 <= eps:\n","            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n","        if not 0.0 <= betas[0] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n","        if not 0.0 <= betas[1] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n","        defaults = dict(lr=lr, betas=betas, eps=eps,\n","                        weight_decay=weight_decay, amsgrad=amsgrad,\n","                        penalty=penalty)\n","        super(AdamL12, self).__init__(params, defaults)\n","\n","    def __setstate__(self, state):\n","        super(AdamL12, self).__setstate__(state)\n","        for group in self.param_groups:\n","            group.setdefault(\"amsgrad\", False)\n","\n","    def step(self, closure=None):\n","        \"\"\" Performs a single optimization step.\n","\n","        Arguments:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","\n","                # Perform weight-decay\n","                p.data.mul_(1 - group['lr'] * group['weight_decay'])\n","\n","                # Perform optimization step\n","                grad = p.grad.data\n","                if grad.is_sparse:\n","                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n","                amsgrad = group['amsgrad']\n","\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    # Exponential moving average of gradient values\n","                    state['exp_avg'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                    # Exponential moving average of squared gradient values\n","                    state['exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                    if amsgrad:\n","                        state['max_exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n","                if amsgrad:\n","                    max_exp_avg_sq = state['max_exp_avg_sq']\n","                beta1, beta2 = group['betas']\n","\n","                state['step'] += 1\n","                bias_correction1 = 1 - beta1 ** state['step']\n","                bias_correction2 = 1 - beta2 ** state['step']\n","\n","                # Decay the first and second moment running average coefficient\n","                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n","                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n","                if amsgrad:\n","                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n","                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n","                else:\n","                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n","\n","                step_size = group['lr'] / bias_correction1\n","\n","                p.data.addcdiv_(-step_size, exp_avg, denom)\n","\n","                if len(p.data.shape) == 2 or len(p.data.shape) == 4:\n","                    threshold = (54 ** (1/3) / 4) * ((2 * group['penalty'] * group['lr'] / denom) ** (2/3))\n","                    mask = p.data.abs() > threshold\n","                    mask = mask.float()\n","\n","                    zero_mask = p.data.abs() <= threshold\n","                    zero_mask = zero_mask.float() + 1e-4\n","\n","                    p.data.mul_(mask)\n","                    factor = (group['lr'] * group['penalty'] / denom) / 4\n","                    angle = factor * (((p.data.abs() + zero_mask) / 3) ** (-1.5))\n","                    angle = angle * mask\n","                    angle = torch.acos(angle)\n","\n","                    value = p.data * (2/3) * (1 + torch.cos(2/3 * (math.pi - angle)))\n","                    p.data = value * mask\n","\n","        return loss\n","\n","class AdamL23(Optimizer):\n","    r\"\"\" Implements Adam with L0 regularization\n","\n","    A General Family of Proximal Methods for Stochastic Preconditioned Gradient Descent\n","\n","    This family contains the Adam-type curvature estimate and\n","    L0 (non-convex, non-smooth) regularizer\n","\n","    For this optimizer, the update rule is somewhat adaptive hard-thresholding\n","    \"\"\"\n","    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n","                 weight_decay=1e-2, amsgrad=False, penalty=0.0):\n","        if not 0.0 <= lr:\n","            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n","        if not 0.0 <= eps:\n","            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n","        if not 0.0 <= betas[0] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n","        if not 0.0 <= betas[1] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n","        defaults = dict(lr=lr, betas=betas, eps=eps,\n","                        weight_decay=weight_decay, amsgrad=amsgrad,\n","                        penalty=penalty)\n","        super(AdamL23, self).__init__(params, defaults)\n","\n","    def __setstate__(self, state):\n","        super(AdamL23, self).__setstate__(state)\n","        for group in self.param_groups:\n","            group.setdefault(\"amsgrad\", False)\n","\n","    def step(self, closure=None):\n","        \"\"\" Performs a single optimization step.\n","\n","        Arguments:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","\n","                # Perform weight-decay\n","                p.data.mul_(1 - group['lr'] * group['weight_decay'])\n","\n","                # Perform optimization step\n","                grad = p.grad.data\n","                if grad.is_sparse:\n","                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n","                amsgrad = group['amsgrad']\n","\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    # Exponential moving average of gradient values\n","                    state['exp_avg'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                    # Exponential moving average of squared gradient values\n","                    state['exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                    if amsgrad:\n","                        state['max_exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n","                if amsgrad:\n","                    max_exp_avg_sq = state['max_exp_avg_sq']\n","                beta1, beta2 = group['betas']\n","\n","                state['step'] += 1\n","                bias_correction1 = 1 - beta1 ** state['step']\n","                bias_correction2 = 1 - beta2 ** state['step']\n","\n","                # Decay the first and second moment running average coefficient\n","                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n","                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n","                if amsgrad:\n","                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n","                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n","                else:\n","                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n","\n","                step_size = group['lr'] / bias_correction1\n","\n","                p.data.addcdiv_(-step_size, exp_avg, denom)\n","\n","                if len(p.data.shape) == 2 or len(p.data.shape) == 4:\n","                    eff_lam = 2 * group['lr'] * group['penalty'] / denom\n","                    threshold = (2/3) * (3 * eff_lam ** 3) ** (1/4)\n","                    mask = p.data.abs() > threshold\n","                    mask = mask.float()\n","\n","                    zero_mask = p.data.abs() <= threshold\n","                    zero_mask = zero_mask.float() * 100\n","\n","                    p.data.mul_(mask)\n","                    angle = acosh((27/16) * (p.data ** 2 + zero_mask) * (eff_lam ** (-1.5)))\n","                    angle = angle * mask\n","                    absA = (2/math.sqrt(3)) * (eff_lam ** (1/4)) * (torch.cosh(angle/3) ** (1/2))\n","\n","                    value = ((absA + torch.sqrt(2 * (p.data.abs() + zero_mask) / absA - absA ** 2)) / 2) ** 3\n","\n","                    p.data = p.data.sign() * value * mask\n","\n","        return loss"]},{"cell_type":"code","execution_count":null,"id":"db8ad935-0180-4f38-9c19-d28ad1f8d502","metadata":{"tags":[],"id":"db8ad935-0180-4f38-9c19-d28ad1f8d502"},"outputs":[],"source":["Epochs                = 100      # 训练轮数\n","l                     = 0.01\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","def seed_torch():\n","    seed=1029\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed) # 为了禁止hash随机化，使得实验可复现\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","seed_torch()\n","model = ResNet18()\n","model = model.to(device)\n","loss_function = nn.MSELoss()  # loss\n","optimizer = AdamL12(model.parameters(), lr=l)  # 优化器\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=9, gamma=0.95)\n","LOSS = float('inf')\n","result = {}\n","result['train-loss']= []\n","result['test-loss']= []\n","start_time1 = time.time()\n","for epochs in range(Epochs):\n","    start_time = time.time()\n","    model.train()\n","    train_loss = 0\n","    test_loss  = 0\n","    for data_l in train_loader:\n","        seq, labels = data_l\n","        seq, labels = seq.to(device), labels.to(device)\n","        optimizer.zero_grad()                          #    清空过往梯度\n","        y_pred = model(seq)\n","        single_loss = loss_function(y_pred, labels)    #    获取loss：输入预测值和标签，计算损失函数\n","        single_loss.backward()                         #    反向传播，计算当前梯度\n","        optimizer.step()                               #    根据梯度更新网络参数\n","        train_loss += single_loss.item()\n","    train_loss = train_loss/len(train_loader)\n","    scheduler.step()\n","    model.eval()\n","    for data_l in test_loader:\n","        seq, labels = data_l\n","        seq, labels = seq.to(device), labels.to(device)\n","        y_pred = model(seq)\n","        single_loss = loss_function(y_pred, labels)    #    获取loss：输入预测值和标签，计算损失函数\n","        test_loss += single_loss.item()\n","    test_loss = test_loss/len(test_loader)\n","    if LOSS > test_loss:\n","        LOSS =test_loss\n","        torch.save(model, 'model_cnn_pig_data.pth')\n","        print('已更新保存模型')\n","    result['train-loss'].append(train_loss)\n","    result['test-loss'].append(test_loss)\n","    del seq, labels ,y_pred #删除数据与变量\n","    gc.collect() #清除数据与变量相关的缓存\n","    torch.cuda.empty_cache() #缓存分配器分配出去的内存给释放掉\n","    epoch_time = time.time() - start_time\n","    print('Epochs',epochs,'loss_train',train_loss,'loss_test',test_loss,'每轮耗时：',epoch_time)\n","all_time = time.time() - start_time1\n","print('总耗时:',all_time)"]},{"cell_type":"code","execution_count":null,"id":"b363e603-46e2-4005-b455-d041d109dbe3","metadata":{"tags":[],"id":"b363e603-46e2-4005-b455-d041d109dbe3"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.figure(dpi=150,figsize=(7,4))\n","plt.plot(result['train-loss'][:], label='train')\n","plt.plot(result['test-loss'][:], label='test')\n","plt.xlabel('iteration')\n","plt.ylabel('loss')\n","plt.title('Training and Testing Loss')\n","plt.legend()\n","plt.savefig('resnet18loss1.jpg',dpi=150)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"5e5d1ed8-61aa-4914-a89a-332a1ef67084","metadata":{"tags":[],"id":"5e5d1ed8-61aa-4914-a89a-332a1ef67084"},"outputs":[],"source":["def load_model(model_path):\n","\n","    if torch.cuda.is_available():\n","        model = torch.load(model_path)\n","    else:\n","        model = torch.load(model_path, map_location=torch.device('cpu'))\n","\n","    model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n","    return model\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = load_model('model_cnn_pig_data.pth')\n","pre = []\n","for i in X_test:\n","    i = i.unsqueeze(0)\n","    pre.append(model(i.to(device)).cpu().detach().numpy())\n","pre = np.array(pre)\n","pre = np.squeeze(pre)\n","Y_test1 = Y_test.cpu().detach().numpy().copy()\n","for i in range(pre.shape[1]):\n","    pre[:,i] = pre[:,i]*Std[i] +Mean[i]\n","    Y_test1[:,i] = Y_test1[:,i]*Std[i] +Mean[i]\n","    plt.figure(dpi=150,figsize=(7,4))\n","    plt.plot(Y_test1[:,i], label='True Values', color='blue')\n","    plt.plot(pre[:,i], label='Predictions', linestyle='--', color='red')\n","    plt.title('True Values vs Predictions')\n","    plt.xlabel('Data Point')\n","    plt.ylabel('Value')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig('cnn_对比图_{}.jpg'.format(i),dpi=150)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"id":"edda82e3-ed71-4c80-b0c5-927f2717d8b7","metadata":{"id":"edda82e3-ed71-4c80-b0c5-927f2717d8b7"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"7cfa6612-37ac-49d8-8431-8049dc66bf64","metadata":{"id":"7cfa6612-37ac-49d8-8431-8049dc66bf64"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"ef0cfb90","metadata":{"id":"ef0cfb90"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}