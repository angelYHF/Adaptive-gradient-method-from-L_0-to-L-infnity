{"cells":[{"cell_type":"code","execution_count":null,"id":"6d30e714","metadata":{"tags":[],"id":"6d30e714"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import random,os\n","import torch.utils.data as Data\n","from sklearn.metrics import f1_score,recall_score,precision_score,roc_curve,auc,accuracy_score,confusion_matrix,r2_score, mean_squared_error\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import torch\n","# import torch.nn as nn\n","from torch import nn,optim\n","import torch.nn.functional as F\n","from torch.optim.optimizer import Optimizer, required\n","import gc\n","import time\n","import math\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"id":"d579e9d1","metadata":{"id":"d579e9d1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"f02da321","metadata":{"tags":[],"id":"f02da321","outputId":"8c3dedf8-7fe7-4774-d39a-ed5c3e82a785"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([32, 5])\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class BasicBlock1D(nn.Module):\n","    expansion = 1\n","    def __init__(self, in_planes, planes, stride=1, downsample=None):\n","        super(BasicBlock1D, self).__init__()\n","        self.conv1 = nn.Conv1d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm1d(planes)\n","        self.conv2 = nn.Conv1d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm1d(planes)\n","        self.downsample = downsample\n","        self.stride = stride\n","    def forward(self, x):\n","        identity = x\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","        out += identity\n","        out = F.relu(out)\n","        return out\n","class ResNet1D(nn.Module):\n","    def __init__(self, p,block, layers):\n","        super(ResNet1D, self).__init__()\n","        self.in_planes = 2\n","        self.conv1 = nn.Conv1d(1, 2, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.bn1 = nn.BatchNorm1d(2)\n","        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n","        self.layer1 = self._make_layer(block, 4, layers[0])\n","        self.layer2 = self._make_layer(block, 6, layers[1], stride=2)\n","        self.layer3 = self._make_layer(block, 8, layers[2], stride=2)\n","        self.layer4 = self._make_layer(block, 16, layers[3], stride=2)\n","        self.avgpool = nn.AdaptiveAvgPool1d(1)\n","        self.fc1 = nn.Linear(16 * block.expansion, 5)\n","        self.fc2 = nn.Linear(128, 5)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(p)\n","\n","    def _make_layer(self, block, planes, blocks, stride=1):\n","        downsample = None\n","        if stride != 1 or self.in_planes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv1d(self.in_planes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm1d(planes * block.expansion),\n","            )\n","        layers = []\n","        layers.append(block(self.in_planes, planes, stride, downsample))\n","        self.in_planes = planes * block.expansion\n","        for _ in range(1, blocks):\n","            layers.append(block(self.in_planes, planes))\n","        return nn.Sequential(*layers)\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = F.relu(x)\n","        x = self.maxpool(x)\n","        # print(x.shape)\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        # print(x.shape)\n","        x = self.layer4(x)\n","        # print(x.shape)\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        # print(x.shape)\n","        x = self.fc1(x)\n","\n","        # x = self.relu(x)\n","        # x = self.dropout(x)\n","        # x = self.fc2(x)\n","        return x\n","\n","\n","p = 0.01\n","# Example usage:\n","model = ResNet1D(p,BasicBlock1D, [2, 2, 2, 2])\n","input_data = torch.randn(32, 1, 30987)  # 32是batch size\n","output = model(input_data)\n","print(output.shape)\n"]},{"cell_type":"code","execution_count":null,"id":"59c5590e-7af1-45e9-a00c-e5e4fda01033","metadata":{"tags":[],"id":"59c5590e-7af1-45e9-a00c-e5e4fda01033","outputId":"745050e9-7d19-40e3-9764-c5234ada964d"},"outputs":[{"data":{"text/plain":["'/root/1/resnet_1d'"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["os.getcwd()"]},{"cell_type":"code","execution_count":null,"id":"dbf65941","metadata":{"tags":[],"id":"dbf65941","outputId":"13132c73-eae0-47fb-f504-8e73983df166"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_870/725109663.py:3: DtypeWarning: Columns (0,1,2,3,4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv(path_name,header=None)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>52839</th>\n","      <th>52840</th>\n","      <th>52841</th>\n","      <th>52842</th>\n","      <th>52843</th>\n","      <th>52844</th>\n","      <th>52845</th>\n","      <th>52846</th>\n","      <th>52847</th>\n","      <th>52848</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ID</td>\n","      <td>t1</td>\n","      <td>t2</td>\n","      <td>t3</td>\n","      <td>t4</td>\n","      <td>t5</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>...</td>\n","      <td>52834.0</td>\n","      <td>52835.0</td>\n","      <td>52836.0</td>\n","      <td>52837.0</td>\n","      <td>52838.0</td>\n","      <td>52839.0</td>\n","      <td>52840.0</td>\n","      <td>52841.0</td>\n","      <td>52842.0</td>\n","      <td>52843.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1136</td>\n","      <td>-0.8687035</td>\n","      <td>0.28158495</td>\n","      <td>1.71659645</td>\n","      <td>-0.8861064</td>\n","      <td>47.6114438</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>...</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1318</td>\n","      <td>0.13144417</td>\n","      <td>-1.7468354</td>\n","      <td>0.95272593</td>\n","      <td>0.15843581</td>\n","      <td>-41.940212</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1319</td>\n","      <td>-0.8536013</td>\n","      <td>-2.9968045</td>\n","      <td>-0.0871332</td>\n","      <td>-1.2177578</td>\n","      <td>100.653792</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1321</td>\n","      <td>0.03416785</td>\n","      <td>-0.320321</td>\n","      <td>-0.071189</td>\n","      <td>-1.4672803</td>\n","      <td>13.8808255</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2310</th>\n","      <td>6466</td>\n","      <td>0.045294</td>\n","      <td>1.386794</td>\n","      <td>2.291255</td>\n","      <td>0.226449</td>\n","      <td>-23.831346</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2311</th>\n","      <td>6467</td>\n","      <td>-1.199029</td>\n","      <td>-0.565144</td>\n","      <td>2.225259</td>\n","      <td>-5.672833</td>\n","      <td>85.793067</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2312</th>\n","      <td>6469</td>\n","      <td>-0.964449</td>\n","      <td>0.673534</td>\n","      <td>1.754374</td>\n","      <td>-1.489569</td>\n","      <td>-8.871632</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2313</th>\n","      <td>6471</td>\n","      <td>-0.832078</td>\n","      <td>-1.40681</td>\n","      <td>2.368887</td>\n","      <td>-1.517962</td>\n","      <td>51.492272</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2314</th>\n","      <td>6473</td>\n","      <td>0.340885</td>\n","      <td>-0.195353</td>\n","      <td>0.372499</td>\n","      <td>-1.375068</td>\n","      <td>114.904649</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2315 rows × 52849 columns</p>\n","</div>"],"text/plain":["     0           1           2           3           4           5      6      \\\n","0       ID          t1          t2          t3          t4          t5    1.0   \n","1     1136  -0.8687035  0.28158495  1.71659645  -0.8861064  47.6114438    1.0   \n","2     1318  0.13144417  -1.7468354  0.95272593  0.15843581  -41.940212    1.0   \n","3     1319  -0.8536013  -2.9968045  -0.0871332  -1.2177578  100.653792    1.0   \n","4     1321  0.03416785   -0.320321   -0.071189  -1.4672803  13.8808255    1.0   \n","...    ...         ...         ...         ...         ...         ...    ...   \n","2310  6466    0.045294    1.386794    2.291255    0.226449  -23.831346    0.0   \n","2311  6467   -1.199029   -0.565144    2.225259   -5.672833   85.793067    1.0   \n","2312  6469   -0.964449    0.673534    1.754374   -1.489569   -8.871632    2.0   \n","2313  6471   -0.832078    -1.40681    2.368887   -1.517962   51.492272    1.0   \n","2314  6473    0.340885   -0.195353    0.372499   -1.375068  114.904649    0.0   \n","\n","      7      8      9      ...    52839    52840    52841    52842    52843  \\\n","0       2.0    3.0    4.0  ...  52834.0  52835.0  52836.0  52837.0  52838.0   \n","1       2.0    1.0    2.0  ...      2.0      1.0      1.0      2.0      1.0   \n","2       2.0    1.0    0.0  ...      0.0      1.0      1.0      2.0      2.0   \n","3       2.0    1.0    1.0  ...      0.0      1.0      1.0      2.0      0.0   \n","4       2.0    0.0    0.0  ...      1.0      1.0      1.0      2.0      1.0   \n","...     ...    ...    ...  ...      ...      ...      ...      ...      ...   \n","2310    1.0    2.0    1.0  ...      0.0      0.0      1.0      0.0      2.0   \n","2311    2.0    1.0    2.0  ...      1.0      1.0      2.0      0.0      0.0   \n","2312    2.0    2.0    1.0  ...      0.0      1.0      1.0      1.0      1.0   \n","2313    1.0    2.0    1.0  ...      1.0      1.0      2.0      1.0      1.0   \n","2314    2.0    2.0    1.0  ...      1.0      0.0      1.0      1.0      1.0   \n","\n","        52844    52845    52846    52847    52848  \n","0     52839.0  52840.0  52841.0  52842.0  52843.0  \n","1         1.0      0.0      2.0      0.0      1.0  \n","2         2.0      1.0      2.0      1.0      2.0  \n","3         2.0      1.0      2.0      1.0      2.0  \n","4         2.0      1.0      2.0      1.0      2.0  \n","...       ...      ...      ...      ...      ...  \n","2310      2.0      0.0      2.0      1.0      2.0  \n","2311      2.0      0.0      2.0      2.0      2.0  \n","2312      2.0      1.0      2.0      2.0      0.0  \n","2313      2.0      0.0      2.0      0.0      2.0  \n","2314      2.0      2.0      2.0      0.0      2.0  \n","\n","[2315 rows x 52849 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["path1='/root/1/resnet_1d/'\n","path_name = os.path.join(path1,'pig_data.csv')\n","df = pd.read_csv(path_name,header=None)\n","df"]},{"cell_type":"code","execution_count":null,"id":"8a69809d-5ae5-46a8-aefa-903f085df1a7","metadata":{"tags":[],"id":"8a69809d-5ae5-46a8-aefa-903f085df1a7","outputId":"c8ae87ae-9dff-4375-c907-2fe8f8a64550"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>t1</th>\n","      <th>t2</th>\n","      <th>t3</th>\n","      <th>t4</th>\n","      <th>t5</th>\n","      <th>1.0</th>\n","      <th>2.0</th>\n","      <th>3.0</th>\n","      <th>4.0</th>\n","      <th>...</th>\n","      <th>52834.0</th>\n","      <th>52835.0</th>\n","      <th>52836.0</th>\n","      <th>52837.0</th>\n","      <th>52838.0</th>\n","      <th>52839.0</th>\n","      <th>52840.0</th>\n","      <th>52841.0</th>\n","      <th>52842.0</th>\n","      <th>52843.0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1136</td>\n","      <td>-0.8687035</td>\n","      <td>0.28158495</td>\n","      <td>1.71659645</td>\n","      <td>-0.8861064</td>\n","      <td>47.6114438</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>...</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1318</td>\n","      <td>0.13144417</td>\n","      <td>-1.7468354</td>\n","      <td>0.95272593</td>\n","      <td>0.15843581</td>\n","      <td>-41.940212</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1319</td>\n","      <td>-0.8536013</td>\n","      <td>-2.9968045</td>\n","      <td>-0.0871332</td>\n","      <td>-1.2177578</td>\n","      <td>100.653792</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1321</td>\n","      <td>0.03416785</td>\n","      <td>-0.320321</td>\n","      <td>-0.071189</td>\n","      <td>-1.4672803</td>\n","      <td>13.8808255</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1322</td>\n","      <td>0.57391141</td>\n","      <td>-2.0458374</td>\n","      <td>0.054743</td>\n","      <td>0.16402708</td>\n","      <td>-58.738674</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2309</th>\n","      <td>6466</td>\n","      <td>0.045294</td>\n","      <td>1.386794</td>\n","      <td>2.291255</td>\n","      <td>0.226449</td>\n","      <td>-23.831346</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2310</th>\n","      <td>6467</td>\n","      <td>-1.199029</td>\n","      <td>-0.565144</td>\n","      <td>2.225259</td>\n","      <td>-5.672833</td>\n","      <td>85.793067</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2311</th>\n","      <td>6469</td>\n","      <td>-0.964449</td>\n","      <td>0.673534</td>\n","      <td>1.754374</td>\n","      <td>-1.489569</td>\n","      <td>-8.871632</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2312</th>\n","      <td>6471</td>\n","      <td>-0.832078</td>\n","      <td>-1.40681</td>\n","      <td>2.368887</td>\n","      <td>-1.517962</td>\n","      <td>51.492272</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2313</th>\n","      <td>6473</td>\n","      <td>0.340885</td>\n","      <td>-0.195353</td>\n","      <td>0.372499</td>\n","      <td>-1.375068</td>\n","      <td>114.904649</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2314 rows × 52849 columns</p>\n","</div>"],"text/plain":["0       ID          t1          t2          t3          t4          t5  1.0  \\\n","0     1136  -0.8687035  0.28158495  1.71659645  -0.8861064  47.6114438  1.0   \n","1     1318  0.13144417  -1.7468354  0.95272593  0.15843581  -41.940212  1.0   \n","2     1319  -0.8536013  -2.9968045  -0.0871332  -1.2177578  100.653792  1.0   \n","3     1321  0.03416785   -0.320321   -0.071189  -1.4672803  13.8808255  1.0   \n","4     1322  0.57391141  -2.0458374    0.054743  0.16402708  -58.738674  2.0   \n","...    ...         ...         ...         ...         ...         ...  ...   \n","2309  6466    0.045294    1.386794    2.291255    0.226449  -23.831346  0.0   \n","2310  6467   -1.199029   -0.565144    2.225259   -5.672833   85.793067  1.0   \n","2311  6469   -0.964449    0.673534    1.754374   -1.489569   -8.871632  2.0   \n","2312  6471   -0.832078    -1.40681    2.368887   -1.517962   51.492272  1.0   \n","2313  6473    0.340885   -0.195353    0.372499   -1.375068  114.904649  0.0   \n","\n","0     2.0  3.0  4.0  ...  52834.0  52835.0  52836.0  52837.0  52838.0  \\\n","0     2.0  1.0  2.0  ...      2.0      1.0      1.0      2.0      1.0   \n","1     2.0  1.0  0.0  ...      0.0      1.0      1.0      2.0      2.0   \n","2     2.0  1.0  1.0  ...      0.0      1.0      1.0      2.0      0.0   \n","3     2.0  0.0  0.0  ...      1.0      1.0      1.0      2.0      1.0   \n","4     2.0  1.0  1.0  ...      0.0      1.0      2.0      2.0      1.0   \n","...   ...  ...  ...  ...      ...      ...      ...      ...      ...   \n","2309  1.0  2.0  1.0  ...      0.0      0.0      1.0      0.0      2.0   \n","2310  2.0  1.0  2.0  ...      1.0      1.0      2.0      0.0      0.0   \n","2311  2.0  2.0  1.0  ...      0.0      1.0      1.0      1.0      1.0   \n","2312  1.0  2.0  1.0  ...      1.0      1.0      2.0      1.0      1.0   \n","2313  2.0  2.0  1.0  ...      1.0      0.0      1.0      1.0      1.0   \n","\n","0     52839.0  52840.0  52841.0  52842.0  52843.0  \n","0         1.0      0.0      2.0      0.0      1.0  \n","1         2.0      1.0      2.0      1.0      2.0  \n","2         2.0      1.0      2.0      1.0      2.0  \n","3         2.0      1.0      2.0      1.0      2.0  \n","4         2.0      2.0      2.0      2.0      2.0  \n","...       ...      ...      ...      ...      ...  \n","2309      2.0      0.0      2.0      1.0      2.0  \n","2310      2.0      0.0      2.0      2.0      2.0  \n","2311      2.0      1.0      2.0      2.0      0.0  \n","2312      2.0      0.0      2.0      0.0      2.0  \n","2313      2.0      2.0      2.0      0.0      2.0  \n","\n","[2314 rows x 52849 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df.columns = df.iloc[0,:]\n","df = df.iloc[1:,:]\n","df.index = range(len(df))\n","df"]},{"cell_type":"code","execution_count":null,"id":"e40bdb87","metadata":{"tags":[],"id":"e40bdb87","outputId":"f5a70d1d-52e3-4f78-b408-6dffd688251b"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>t1</th>\n","      <th>t2</th>\n","      <th>t3</th>\n","      <th>t4</th>\n","      <th>t5</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.8687035</td>\n","      <td>0.28158495</td>\n","      <td>1.71659645</td>\n","      <td>-0.8861064</td>\n","      <td>47.6114438</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.13144417</td>\n","      <td>-1.7468354</td>\n","      <td>0.95272593</td>\n","      <td>0.15843581</td>\n","      <td>-41.940212</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.8536013</td>\n","      <td>-2.9968045</td>\n","      <td>-0.0871332</td>\n","      <td>-1.2177578</td>\n","      <td>100.653792</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.03416785</td>\n","      <td>-0.320321</td>\n","      <td>-0.071189</td>\n","      <td>-1.4672803</td>\n","      <td>13.8808255</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.57391141</td>\n","      <td>-2.0458374</td>\n","      <td>0.054743</td>\n","      <td>0.16402708</td>\n","      <td>-58.738674</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2309</th>\n","      <td>0.045294</td>\n","      <td>1.386794</td>\n","      <td>2.291255</td>\n","      <td>0.226449</td>\n","      <td>-23.831346</td>\n","    </tr>\n","    <tr>\n","      <th>2310</th>\n","      <td>-1.199029</td>\n","      <td>-0.565144</td>\n","      <td>2.225259</td>\n","      <td>-5.672833</td>\n","      <td>85.793067</td>\n","    </tr>\n","    <tr>\n","      <th>2311</th>\n","      <td>-0.964449</td>\n","      <td>0.673534</td>\n","      <td>1.754374</td>\n","      <td>-1.489569</td>\n","      <td>-8.871632</td>\n","    </tr>\n","    <tr>\n","      <th>2312</th>\n","      <td>-0.832078</td>\n","      <td>-1.40681</td>\n","      <td>2.368887</td>\n","      <td>-1.517962</td>\n","      <td>51.492272</td>\n","    </tr>\n","    <tr>\n","      <th>2313</th>\n","      <td>0.340885</td>\n","      <td>-0.195353</td>\n","      <td>0.372499</td>\n","      <td>-1.375068</td>\n","      <td>114.904649</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2314 rows × 5 columns</p>\n","</div>"],"text/plain":["0             t1          t2          t3          t4          t5\n","0     -0.8687035  0.28158495  1.71659645  -0.8861064  47.6114438\n","1     0.13144417  -1.7468354  0.95272593  0.15843581  -41.940212\n","2     -0.8536013  -2.9968045  -0.0871332  -1.2177578  100.653792\n","3     0.03416785   -0.320321   -0.071189  -1.4672803  13.8808255\n","4     0.57391141  -2.0458374    0.054743  0.16402708  -58.738674\n","...          ...         ...         ...         ...         ...\n","2309    0.045294    1.386794    2.291255    0.226449  -23.831346\n","2310   -1.199029   -0.565144    2.225259   -5.672833   85.793067\n","2311   -0.964449    0.673534    1.754374   -1.489569   -8.871632\n","2312   -0.832078    -1.40681    2.368887   -1.517962   51.492272\n","2313    0.340885   -0.195353    0.372499   -1.375068  114.904649\n","\n","[2314 rows x 5 columns]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["X = df.iloc[:,6:]\n","Y = df.iloc[:,1:6]\n","Y"]},{"cell_type":"code","execution_count":null,"id":"59a67a68-bef8-49ee-9d91-06d8eb28f0db","metadata":{"tags":[],"id":"59a67a68-bef8-49ee-9d91-06d8eb28f0db"},"outputs":[],"source":["Y = Y.astype('float')"]},{"cell_type":"code","execution_count":null,"id":"bff63ca3","metadata":{"tags":[],"id":"bff63ca3","outputId":"c8eec1e3-d299-4d3c-872f-84e5f3fe8ac2"},"outputs":[{"name":"stderr","output_type":"stream","text":["/root/miniconda3/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n"]}],"source":["from sklearn.preprocessing import OneHotEncoder\n","\n","\n","# 初始化OneHotEncoder\n","encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n","\n","# 选择要进行独热编码的列\n","cols_to_encode = df.columns[6:]\n","\n","# 拟合训练集\n","encoder.fit(X)\n","\n","# 对训练集和测试集进行转换\n","X_encoded = encoder.transform(X)\n"]},{"cell_type":"code","execution_count":null,"id":"fbe02c31-de55-48d2-b345-9958b357568e","metadata":{"tags":[],"id":"fbe02c31-de55-48d2-b345-9958b357568e","outputId":"0db53232-d55c-4924-c70c-b8c76cfaa120"},"outputs":[{"data":{"text/plain":["(2314, 279995)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["X_encoded.shape"]},{"cell_type":"code","execution_count":null,"id":"a5d08cd5-7567-4616-b10d-49e53d024b1c","metadata":{"tags":[],"id":"a5d08cd5-7567-4616-b10d-49e53d024b1c"},"outputs":[],"source":["X_encoded = X_encoded.reshape(len(X_encoded),1,-1)"]},{"cell_type":"code","execution_count":null,"id":"368149f5-a89c-4514-b44b-1b090dac53ae","metadata":{"id":"368149f5-a89c-4514-b44b-1b090dac53ae"},"outputs":[],"source":["\n","# # 假设data是一个形状为(N, 30987)的数组\n","# padding = np.zeros((X_encoded.shape[0], 31329 - X_encoded.shape[1]))\n","# data_padded = np.concatenate([X_encoded, padding], axis=1)\n","# # 然后，将数据重塑为(n, 177, 177, 1)\n","# data_reshaped = data_padded.reshape(-1, 1,177, 177)\n","# data_reshaped.shape"]},{"cell_type":"code","execution_count":null,"id":"03d20b9c","metadata":{"tags":[],"id":"03d20b9c"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, Y_train, Y_test = train_test_split(X_encoded,Y , test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"id":"d5b15dab","metadata":{"tags":[],"id":"d5b15dab"},"outputs":[],"source":["def z_score(data):\n","    data = data.astype(float)\n","    Mean = data.mean()\n","    Var = ((data - Mean)**2).mean()\n","    Std = pow(Var,0.5)\n","    data = (data - Mean)/Std  # 标准化\n","    return Mean,Std,data\n","Mean,Std,Y_train = z_score(Y_train)\n","Y_test = (Y_test - Mean)/Std"]},{"cell_type":"code","execution_count":null,"id":"c7fc6a2b","metadata":{"tags":[],"id":"c7fc6a2b"},"outputs":[],"source":["Y_train = Y_train.values\n","Y_test = Y_test.values"]},{"cell_type":"code","execution_count":null,"id":"8e2f2c83","metadata":{"tags":[],"id":"8e2f2c83","outputId":"b32a22e0-3750-47db-9ab6-4a072c0d8e37"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1851, 1, 279995]) torch.Size([1851, 5]) torch.Size([463, 1, 279995]) torch.Size([463, 5])\n"]}],"source":["Y_train = np.squeeze(Y_train)\n","Y_test = np.squeeze(Y_test)\n","\n","X_train = torch.tensor(X_train,dtype = torch.float)\n","Y_train  = torch.tensor(Y_train,dtype = torch.float)\n","\n","X_test = torch.tensor(X_test,dtype = torch.float)\n","Y_test  = torch.tensor(Y_test,dtype = torch.float)\n","print(X_train.shape, Y_train.shape,X_test.shape, Y_test.shape)"]},{"cell_type":"code","execution_count":null,"id":"4a2b0600-2ee9-4e71-8b99-1e3e0ee08007","metadata":{"tags":[],"id":"4a2b0600-2ee9-4e71-8b99-1e3e0ee08007"},"outputs":[],"source":["train_loader = Data.DataLoader(\n","    dataset=Data.TensorDataset(X_train, Y_train),  # 封装进Data.TensorDataset()类的数据，可以为任意维度\n","    batch_size=35,  # 每块的大小\n","    shuffle=True,\n","    drop_last =True, #丢弃最后一组数据\n","    num_workers=0,  # 多进程（multiprocess）来读数据\n",")\n","test_loader = Data.DataLoader(\n","    dataset=Data.TensorDataset(X_test, Y_test),  # 封装进Data.TensorDataset()类的数据，可以为任意维度\n","    batch_size=35,  # 每块的大小\n","    shuffle=False,\n","    drop_last =True,\n","    num_workers=0,\n",")"]},{"cell_type":"code","execution_count":null,"id":"474aa57b-a451-4b3e-88b0-abe6a24d0ca3","metadata":{"id":"474aa57b-a451-4b3e-88b0-abe6a24d0ca3"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"805c74c1-6422-4f9e-821e-405793dd45da","metadata":{"tags":[],"id":"805c74c1-6422-4f9e-821e-405793dd45da"},"outputs":[],"source":["class AdamL43(Optimizer):\n","    \"\"\" Implements Adam with L4/3 regularization based on the AdamL12 optimizer.\n","    \"\"\"\n","    def __init__(self, params, lr=1e-3, betas=(0.1, 0.999), eps=1e-8,\n","                 weight_decay=1e-2, amsgrad=False, penalty=0.0, kappa=1):\n","        if not 0.0 <= lr:\n","            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n","        if not 0.0 <= eps:\n","            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n","        if not 0.0 <= betas[0] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n","        if not 0.0 <= betas[1] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n","        defaults = dict(lr=lr, betas=betas, eps=eps,\n","                        weight_decay=weight_decay, amsgrad=amsgrad,\n","                        penalty=penalty, kappa=kappa)\n","        super(AdamL43, self).__init__(params, defaults)\n","\n","    def __setstate__(self, state):\n","        super(AdamL43, self).__setstate__(state)\n","        for group in self.param_groups:\n","            group.setdefault(\"amsgrad\", False)\n","\n","    def step(self, closure=None):\n","        \"\"\" Performs a single optimization step.\n","\n","        Arguments:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            lr = group['lr']\n","            weight_decay = group['weight_decay']\n","            amsgrad = group['amsgrad']\n","            penalty = group['penalty']\n","            kappa = group['kappa']\n","            beta1, beta2 = group['betas']\n","            eps = group['eps']\n","\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","\n","                # Perform weight decay\n","                p.data.mul_(1 - lr * weight_decay)\n","\n","                # Perform optimization step\n","                grad = p.grad.data\n","                if grad.is_sparse:\n","                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n","\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    state['exp_avg'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                    state['exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                    if amsgrad:\n","                        state['max_exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","\n","                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n","                if amsgrad:\n","                    max_exp_avg_sq = state['max_exp_avg_sq']\n","\n","                state['step'] += 1\n","                bias_correction1 = 1 - beta1 ** state['step']\n","                bias_correction2 = 1 - beta2 ** state['step']\n","\n","                # Decay the first and second moment running average coefficient\n","                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n","                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n","\n","                if amsgrad:\n","                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n","                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)\n","                else:\n","                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)\n","\n","                step_size = lr / bias_correction1\n","\n","                # Apply the proximal operator for L4/3 regularization\n","                prox_term = (8 * kappa * step_size * penalty) / (32 ** (1 / 3) * denom)\n","\n","                # Thresholding and proximal adjustment as per L4/3 formula\n","                theta = p.data.clone()\n","                chi = torch.sqrt(theta ** 2 + 256 * kappa ** 3 / 729)\n","\n","                proximal_adjustment = prox_term * ((chi - theta).pow(1 / 3) - (chi + theta).pow(1 / 3))\n","\n","                # Update parameter\n","                p.data.addcdiv_(-step_size, exp_avg, denom).add_(proximal_adjustment)\n","\n","        return loss\n","\n","\n","class AdamL12(Optimizer):\n","    r\"\"\" Implements Adam with L0 regularization\n","\n","    A General Family of Proximal Methods for Stochastic Preconditioned Gradient Descent\n","\n","    This family contains the Adam-type curvature estimate and\n","    L0 (non-convex, non-smooth) regularizer\n","\n","    For this optimizer, the update rule is somewhat adaptive hard-thresholding\n","    \"\"\"\n","    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n","                 weight_decay=1e-2, amsgrad=False, penalty=0.0):\n","        if not 0.0 <= lr:\n","            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n","        if not 0.0 <= eps:\n","            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n","        if not 0.0 <= betas[0] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n","        if not 0.0 <= betas[1] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n","        defaults = dict(lr=lr, betas=betas, eps=eps,\n","                        weight_decay=weight_decay, amsgrad=amsgrad,\n","                        penalty=penalty)\n","        super(AdamL12, self).__init__(params, defaults)\n","\n","    def __setstate__(self, state):\n","        super(AdamL12, self).__setstate__(state)\n","        for group in self.param_groups:\n","            group.setdefault(\"amsgrad\", False)\n","\n","    def step(self, closure=None):\n","        \"\"\" Performs a single optimization step.\n","\n","        Arguments:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","\n","                # Perform weight-decay\n","                p.data.mul_(1 - group['lr'] * group['weight_decay'])\n","\n","                # Perform optimization step\n","                grad = p.grad.data\n","                if grad.is_sparse:\n","                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n","                amsgrad = group['amsgrad']\n","\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    # Exponential moving average of gradient values\n","                    state['exp_avg'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                    # Exponential moving average of squared gradient values\n","                    state['exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                    if amsgrad:\n","                        state['max_exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n","                if amsgrad:\n","                    max_exp_avg_sq = state['max_exp_avg_sq']\n","                beta1, beta2 = group['betas']\n","\n","                state['step'] += 1\n","                bias_correction1 = 1 - beta1 ** state['step']\n","                bias_correction2 = 1 - beta2 ** state['step']\n","\n","                # Decay the first and second moment running average coefficient\n","                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n","                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n","                if amsgrad:\n","                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n","                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n","                else:\n","                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n","\n","                step_size = group['lr'] / bias_correction1\n","\n","                p.data.addcdiv_(-step_size, exp_avg, denom)\n","\n","                if len(p.data.shape) == 2 or len(p.data.shape) == 4:\n","                    threshold = (54 ** (1/3) / 4) * ((2 * group['penalty'] * group['lr'] / denom) ** (2/3))\n","                    mask = p.data.abs() > threshold\n","                    mask = mask.float()\n","\n","                    zero_mask = p.data.abs() <= threshold\n","                    zero_mask = zero_mask.float() + 1e-4\n","\n","                    p.data.mul_(mask)\n","                    factor = (group['lr'] * group['penalty'] / denom) / 4\n","                    angle = factor * (((p.data.abs() + zero_mask) / 3) ** (-1.5))\n","                    angle = angle * mask\n","                    angle = torch.acos(angle)\n","\n","                    value = p.data * (2/3) * (1 + torch.cos(2/3 * (math.pi - angle)))\n","                    p.data = value * mask\n","\n","        return loss\n","\n","class AdamL23(Optimizer):\n","    r\"\"\" Implements Adam with L0 regularization\n","\n","    A General Family of Proximal Methods for Stochastic Preconditioned Gradient Descent\n","\n","    This family contains the Adam-type curvature estimate and\n","    L0 (non-convex, non-smooth) regularizer\n","\n","    For this optimizer, the update rule is somewhat adaptive hard-thresholding\n","    \"\"\"\n","    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n","                 weight_decay=1e-2, amsgrad=False, penalty=0.0):\n","        if not 0.0 <= lr:\n","            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n","        if not 0.0 <= eps:\n","            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n","        if not 0.0 <= betas[0] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n","        if not 0.0 <= betas[1] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n","        defaults = dict(lr=lr, betas=betas, eps=eps,\n","                        weight_decay=weight_decay, amsgrad=amsgrad,\n","                        penalty=penalty)\n","        super(AdamL23, self).__init__(params, defaults)\n","\n","    def __setstate__(self, state):\n","        super(AdamL23, self).__setstate__(state)\n","        for group in self.param_groups:\n","            group.setdefault(\"amsgrad\", False)\n","\n","    def step(self, closure=None):\n","        \"\"\" Performs a single optimization step.\n","\n","        Arguments:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","\n","                # Perform weight-decay\n","                p.data.mul_(1 - group['lr'] * group['weight_decay'])\n","\n","                # Perform optimization step\n","                grad = p.grad.data\n","                if grad.is_sparse:\n","                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n","                amsgrad = group['amsgrad']\n","\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    # Exponential moving average of gradient values\n","                    state['exp_avg'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                    # Exponential moving average of squared gradient values\n","                    state['exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                    if amsgrad:\n","                        state['max_exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n","                if amsgrad:\n","                    max_exp_avg_sq = state['max_exp_avg_sq']\n","                beta1, beta2 = group['betas']\n","\n","                state['step'] += 1\n","                bias_correction1 = 1 - beta1 ** state['step']\n","                bias_correction2 = 1 - beta2 ** state['step']\n","\n","                # Decay the first and second moment running average coefficient\n","                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n","                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n","                if amsgrad:\n","                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n","                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n","                else:\n","                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n","\n","                step_size = group['lr'] / bias_correction1\n","\n","                p.data.addcdiv_(-step_size, exp_avg, denom)\n","\n","                if len(p.data.shape) == 2 or len(p.data.shape) == 4:\n","                    eff_lam = 2 * group['lr'] * group['penalty'] / denom\n","                    threshold = (2/3) * (3 * eff_lam ** 3) ** (1/4)\n","                    mask = p.data.abs() > threshold\n","                    mask = mask.float()\n","\n","                    zero_mask = p.data.abs() <= threshold\n","                    zero_mask = zero_mask.float() * 100\n","\n","                    p.data.mul_(mask)\n","                    angle = acosh((27/16) * (p.data ** 2 + zero_mask) * (eff_lam ** (-1.5)))\n","                    angle = angle * mask\n","                    absA = (2/math.sqrt(3)) * (eff_lam ** (1/4)) * (torch.cosh(angle/3) ** (1/2))\n","\n","                    value = ((absA + torch.sqrt(2 * (p.data.abs() + zero_mask) / absA - absA ** 2)) / 2) ** 3\n","\n","                    p.data = p.data.sign() * value * mask\n","\n","        return loss"]},{"cell_type":"code","execution_count":null,"id":"0b36f3e8-52e3-495b-86ec-09ddd88301a8","metadata":{"tags":[],"id":"0b36f3e8-52e3-495b-86ec-09ddd88301a8","outputId":"84fccdc2-4982-47b2-91cf-246941c866e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["  0%|          | 0/100 [00:00<?, ?trial/s, best loss=?]"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_870/1815375738.py:77: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)\n","  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n","\n"]},{"name":"stdout","output_type":"stream","text":[" 37%|███▋      | 37/100 [2:45:40<4:39:19, 266.03s/trial, best loss: 1.0143726330537062]"]}],"source":["import hyperopt\n","from hyperopt import hp, fmin, tpe, Trials, partial\n","from hyperopt.early_stop import no_progress_loss\n","def seed_torch():\n","    seed=1029\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed) # 为了禁止hash随机化，使得实验可复现\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","def hyperopt_objective(params):\n","    Epochs                = 100      # 训练轮数\n","    l           =params['lr']                     # 步长\n","    p           =params['p']\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","    seed_torch()\n","    model = ResNet1D(p,BasicBlock1D, [2, 2, 2, 2])\n","    model = model.to(device)\n","    # 提取第一层参数\n","    first_layer_params = []\n","    other_params = []\n","\n","    for name, param in model.named_parameters():\n","        if 'conv1' in name or 'bn1' in name:\n","            first_layer_params.append(param)\n","        else:\n","            other_params.append(param)\n","\n","    # 定义两个优化器\n","    optimizer_first_layer = AdamL12(first_layer_params, lr=l)\n","    optimizer_other_layers = optim.Adam(other_params, lr=l)\n","\n","\n","\n","\n","    loss_function = nn.MSELoss()  # loss\n","    # optimizer = AdamL12(model.parameters(), lr=l)  # 优化器\n","    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=9, gamma=0.95)\n","    LOSS = float('inf')\n","    result = {}\n","    result['train-loss']= []\n","    result['test-loss']= []\n","    start_time1 = time.time()\n","    for epochs in range(Epochs):\n","        start_time = time.time()\n","        model.train()\n","        train_loss = 0\n","        test_loss  = 0\n","        for data_l in train_loader:\n","            seq, labels = data_l\n","            seq, labels = seq.to(device), labels.to(device)\n","            optimizer_first_layer.zero_grad()                          #    清空过往梯度\n","            optimizer_other_layers.zero_grad()\n","            y_pred = model(seq)\n","            single_loss = loss_function(y_pred, labels)    #    获取loss：输入预测值和标签，计算损失函数\n","            single_loss.backward()                         #    反向传播，计算当前梯度\n","            # optimizer.step()                               #    根据梯度更新网络参数\n","            optimizer_first_layer.step()\n","            optimizer_other_layers.step()\n","            train_loss += single_loss.item()\n","        train_loss = train_loss/len(train_loader)\n","        # scheduler.step()\n","        model.eval()\n","        for data_l in test_loader:\n","            seq, labels = data_l\n","            seq, labels = seq.to(device), labels.to(device)\n","            y_pred = model(seq)\n","            single_loss = loss_function(y_pred, labels)    #    获取loss：输入预测值和标签，计算损失函数\n","            test_loss += single_loss.item()\n","        test_loss = test_loss/len(test_loader)\n","        if LOSS > test_loss:\n","            LOSS =test_loss\n","            # torch.save(model, 'model_cnn_pig_data.pth')\n","            # print('已更新保存模型')\n","        # result['train-loss'].append(train_loss)\n","        # result['test-loss'].append(test_loss)\n","        del seq, labels ,y_pred #删除数据与变量\n","        gc.collect() #清除数据与变量相关的缓存\n","        torch.cuda.empty_cache() #缓存分配器分配出去的内存给释放掉\n","        epoch_time = time.time() - start_time\n","    #     print('Epochs',epochs,'loss_train',train_loss,'loss_test',test_loss,'每轮耗时：',epoch_time)\n","    # all_time = time.time() - start_time1\n","    # print('总耗时:',all_time)\n","    return LOSS\n","\n","param_grid_simple = {'lr': hp.uniform(\"lr\",0.00005,0.1)\n","                     ,'p':hp.uniform('p',0.01,0.95)\n","                    }\n","def param_hyperopt(max_evals=100):\n","\n","    #保存迭代过程\n","    trials = Trials()\n","\n","    #设置提前停止\n","    early_stop_fn = no_progress_loss(100)\n","\n","    #定义代理模型\n","    #algo = partial(tpe.suggest, n_startup_jobs=20, n_EI_candidates=50)\n","    params_best = fmin(hyperopt_objective #目标函数\n","                       , space = param_grid_simple #参数空间\n","                       , algo = tpe.suggest #代理模型你要哪个呢？\n","                       #, algo = algo\n","                       , max_evals = max_evals #允许的迭代次数\n","                       , verbose=True\n","                       , trials = trials\n","                       , early_stop_fn = early_stop_fn\n","                      )\n","\n","    #打印最优参数，fmin会自动打印最佳分数\n","    print(\"\\n\",\"\\n\",\"best params: \", params_best,\n","          \"\\n\")\n","    return params_best, trials\n","params =  param_hyperopt(max_evals=100)[0]\n","params"]},{"cell_type":"code","execution_count":null,"id":"7a401035-c95f-4845-8194-1de3b4c1291a","metadata":{"tags":[],"id":"7a401035-c95f-4845-8194-1de3b4c1291a"},"outputs":[],"source":["l = params['lr']\n","p = params['p']"]},{"cell_type":"code","execution_count":null,"id":"db8ad935-0180-4f38-9c19-d28ad1f8d502","metadata":{"tags":[],"id":"db8ad935-0180-4f38-9c19-d28ad1f8d502","outputId":"1323d290-cc8a-4525-ba02-e4be61ba199f"},"outputs":[{"name":"stdout","output_type":"stream","text":["已更新保存模型\n","Epochs 0 loss_train 1.010607665547958 loss_test 1.0780249008765588 每轮耗时： 2.9164834022521973\n","Epochs 1 loss_train 0.9950360724559197 loss_test 1.1029571111385639 每轮耗时： 2.7396838665008545\n","Epochs 2 loss_train 0.9914432660891459 loss_test 1.1473208849246685 每轮耗时： 2.6563923358917236\n","已更新保存模型\n","Epochs 3 loss_train 0.9879113091872289 loss_test 1.0408187371033888 每轮耗时： 2.6594347953796387\n","Epochs 4 loss_train 0.9942276936310989 loss_test 1.0671539077391992 每轮耗时： 2.6840760707855225\n","Epochs 5 loss_train 0.9772659104603988 loss_test 1.098690940783574 每轮耗时： 2.6518712043762207\n","Epochs 6 loss_train 0.9774538381741598 loss_test 1.0605455728677602 每轮耗时： 2.741250514984131\n","Epochs 7 loss_train 0.9646104780527262 loss_test 1.0431903004646301 每轮耗时： 2.7423007488250732\n","Epochs 8 loss_train 0.9737558708741114 loss_test 1.3333098613298857 每轮耗时： 2.728891134262085\n","已更新保存模型\n","Epochs 9 loss_train 0.9739785366333448 loss_test 1.026265786244319 每轮耗时： 2.6693897247314453\n","已更新保存模型\n","Epochs 10 loss_train 0.962936771603731 loss_test 1.0145879983901978 每轮耗时： 2.657553195953369\n","Epochs 11 loss_train 0.9605997170393283 loss_test 1.157144872041849 每轮耗时： 2.6690497398376465\n","Epochs 12 loss_train 0.9549169551867706 loss_test 1.1966846860372102 每轮耗时： 2.6702537536621094\n","Epochs 13 loss_train 0.9542721555783198 loss_test 1.2334068096601045 每轮耗时： 2.670616626739502\n","Epochs 14 loss_train 0.9493163124873087 loss_test 1.4587322290127094 每轮耗时： 2.701890707015991\n","Epochs 15 loss_train 0.9464864822534415 loss_test 1.0977176840488727 每轮耗时： 2.7027599811553955\n","Epochs 16 loss_train 0.9447859869553492 loss_test 1.0156667415912335 每轮耗时： 2.712902545928955\n","Epochs 17 loss_train 0.939508964235966 loss_test 1.6201690618808453 每轮耗时： 2.7196707725524902\n","Epochs 18 loss_train 0.9376651816643201 loss_test 1.6269715290803175 每轮耗时： 2.677560329437256\n","Epochs 19 loss_train 0.927731902553485 loss_test 1.5239196098767793 每轮耗时： 2.663011312484741\n","Epochs 20 loss_train 0.9288021314602631 loss_test 1.130975338128897 每轮耗时： 2.6491308212280273\n","Epochs 21 loss_train 0.9289460869935843 loss_test 1.1920796174269457 每轮耗时： 2.658742666244507\n","Epochs 22 loss_train 0.9240279140380713 loss_test 2.2287795085173387 每轮耗时： 2.688875198364258\n","Epochs 23 loss_train 0.9223103305468192 loss_test 1.6221967935562134 每轮耗时： 2.6427228450775146\n","Epochs 24 loss_train 0.9221617063650718 loss_test 1.232689435665424 每轮耗时： 2.6783809661865234\n","Epochs 25 loss_train 0.9184219034818503 loss_test 2.7957384769733133 每轮耗时： 2.710468053817749\n","Epochs 26 loss_train 0.9086207816234002 loss_test 1.0157661117040193 每轮耗时： 2.7034482955932617\n","Epochs 27 loss_train 0.9094070734886023 loss_test 1.0836803775567274 每轮耗时： 2.7463526725769043\n","Epochs 28 loss_train 0.9051751861205468 loss_test 1.1185312546216524 每轮耗时： 2.6857519149780273\n","Epochs 29 loss_train 0.9072231203317642 loss_test 1.0378239246515126 每轮耗时： 2.6832752227783203\n","Epochs 30 loss_train 0.9019661247730255 loss_test 1.1056080277149494 每轮耗时： 2.687459945678711\n","Epochs 31 loss_train 0.899049699306488 loss_test 1.0587561772419856 每轮耗时： 2.695725440979004\n","Epochs 32 loss_train 0.8918694475522408 loss_test 1.0495832103949327 每轮耗时： 2.6850972175598145\n","Epochs 33 loss_train 0.8899642905363669 loss_test 1.8845737897432768 每轮耗时： 2.6708881855010986\n","Epochs 34 loss_train 0.894690596140348 loss_test 1.8014572033515344 每轮耗时： 2.670008659362793\n","Epochs 35 loss_train 0.8914970996288153 loss_test 1.0242352485656738 每轮耗时： 2.6433825492858887\n","Epochs 36 loss_train 0.8890032928723556 loss_test 3.0308662744668813 每轮耗时： 2.6856725215911865\n","Epochs 37 loss_train 0.8914047617178696 loss_test 1.251668549500979 每轮耗时： 2.702950954437256\n","Epochs 38 loss_train 0.8814682822961074 loss_test 3.758486399283776 每轮耗时： 2.6923649311065674\n","Epochs 39 loss_train 0.8821334231358308 loss_test 3.910145117686345 每轮耗时： 2.6883656978607178\n","Epochs 40 loss_train 0.8818503801639264 loss_test 1.0833751880205595 每轮耗时： 2.6874866485595703\n","Epochs 41 loss_train 0.8789230287075043 loss_test 2.469411088870122 每轮耗时： 2.6634268760681152\n","Epochs 42 loss_train 0.8772924198554113 loss_test 3.859711977151724 每轮耗时： 2.6446683406829834\n","Epochs 43 loss_train 0.8686553560770475 loss_test 1.2419397830963135 每轮耗时： 2.6718389987945557\n","Epochs 44 loss_train 0.8731101567928607 loss_test 1.6630502022229707 每轮耗时： 2.6452674865722656\n","Epochs 45 loss_train 0.8713686374517587 loss_test 2.132844851567195 每轮耗时： 2.724768877029419\n","Epochs 46 loss_train 0.8634449140383647 loss_test 1.4568873643875122 每轮耗时： 2.7532875537872314\n","Epochs 47 loss_train 0.8697754729252595 loss_test 2.729547243851882 每轮耗时： 2.7195537090301514\n","Epochs 48 loss_train 0.8650712817907333 loss_test 7.304432025322547 每轮耗时： 2.6615560054779053\n","Epochs 49 loss_train 0.860039268548672 loss_test 1.4616557267995982 每轮耗时： 2.659905433654785\n","Epochs 50 loss_train 0.8593921225804549 loss_test 1.3176865302599394 每轮耗时： 2.621617555618286\n","Epochs 51 loss_train 0.8573326411155554 loss_test 1.8782597046632032 每轮耗时： 2.646477699279785\n","Epochs 52 loss_train 0.8560952590062068 loss_test 2.333274116882911 每轮耗时： 2.6858599185943604\n","Epochs 53 loss_train 0.8558615079292884 loss_test 1.2620747181085439 每轮耗时： 2.644308090209961\n","Epochs 54 loss_train 0.8521701235037583 loss_test 1.1086616470263555 每轮耗时： 2.6480050086975098\n","Epochs 55 loss_train 0.8516407081714044 loss_test 1.0948356894346385 每轮耗时： 2.698798894882202\n","Epochs 56 loss_train 0.8539356818565955 loss_test 1.1852524463947003 每轮耗时： 2.7107818126678467\n","Epochs 57 loss_train 0.8413794109454522 loss_test 1.032559064718393 每轮耗时： 2.725736379623413\n","Epochs 58 loss_train 0.8496212351780671 loss_test 2.226133970113901 每轮耗时： 2.692094326019287\n","Epochs 59 loss_train 0.8518869704925097 loss_test 1.1516196269255419 每轮耗时： 2.660520553588867\n","Epochs 60 loss_train 0.8461586374502915 loss_test 2.5162839889526367 每轮耗时： 2.634711980819702\n","Epochs 61 loss_train 0.8444110258267477 loss_test 1.1132589486929088 每轮耗时： 2.637697696685791\n","Epochs 62 loss_train 0.8352161920987643 loss_test 1.051886375133808 每轮耗时： 2.657078504562378\n","Epochs 63 loss_train 0.8460541207056779 loss_test 1.1114798555007348 每轮耗时： 2.6771044731140137\n","Epochs 64 loss_train 0.8494576479379947 loss_test 3.8708050434405985 每轮耗时： 2.639556884765625\n","Epochs 65 loss_train 0.8407040754189858 loss_test 1.0527880879548879 每轮耗时： 2.659140110015869\n","Epochs 66 loss_train 0.8416455021271338 loss_test 3.915430472447322 每轮耗时： 2.6933088302612305\n","Epochs 67 loss_train 0.8355753020598338 loss_test 1.1744801035294166 每轮耗时： 2.7103118896484375\n","Epochs 68 loss_train 0.8408948251834283 loss_test 1.629633637575003 每轮耗时： 2.7118966579437256\n","Epochs 69 loss_train 0.8355139482479829 loss_test 1.1540321799424977 每轮耗时： 2.700543165206909\n","Epochs 70 loss_train 0.832126396206709 loss_test 1.0430239393160894 每轮耗时： 2.6400866508483887\n","Epochs 71 loss_train 0.8422971012500616 loss_test 1.5976697481595552 每轮耗时： 2.6502726078033447\n","Epochs 72 loss_train 0.8314515123000512 loss_test 2.7303662300109863 每轮耗时： 2.6427152156829834\n","Epochs 73 loss_train 0.8340182201220439 loss_test 1.0766421052125783 每轮耗时： 2.647585868835449\n","Epochs 74 loss_train 0.8311052138988788 loss_test 2.244494859988873 每轮耗时： 2.649160623550415\n","Epochs 75 loss_train 0.8358111496155078 loss_test 2.466929765848013 每轮耗时： 2.669848680496216\n","Epochs 76 loss_train 0.8375484851690439 loss_test 1.8902881695674016 每轮耗时： 2.6456925868988037\n","Epochs 77 loss_train 0.826088995887683 loss_test 1.4710857317997859 每轮耗时： 2.648367166519165\n","Epochs 78 loss_train 0.8258524875228221 loss_test 1.9870041058613703 每轮耗时： 2.6929142475128174\n","Epochs 79 loss_train 0.826055992108125 loss_test 1.0990795768224275 每轮耗时： 2.715427875518799\n","Epochs 80 loss_train 0.8286690299327557 loss_test 1.082445529791025 每轮耗时： 2.704585313796997\n","Epochs 81 loss_train 0.8235882876011041 loss_test 1.0473534006338854 每轮耗时： 2.684276819229126\n","Epochs 82 loss_train 0.8303218816335385 loss_test 1.239468203141139 每轮耗时： 2.645857095718384\n","Epochs 83 loss_train 0.827886809523289 loss_test 2.1793474600865292 每轮耗时： 2.6577799320220947\n","Epochs 84 loss_train 0.823149710893631 loss_test 2.497911526606633 每轮耗时： 2.656388521194458\n","Epochs 85 loss_train 0.8240391038931333 loss_test 1.245987213574923 每轮耗时： 2.664503574371338\n","Epochs 86 loss_train 0.8248221014554684 loss_test 1.097655883202186 每轮耗时： 2.6662991046905518\n","Epochs 87 loss_train 0.8218956669935813 loss_test 1.0550655676768377 每轮耗时： 2.696614980697632\n","Epochs 88 loss_train 0.8228210136294365 loss_test 4.87155558512761 每轮耗时： 2.6367645263671875\n","Epochs 89 loss_train 0.8242608091005912 loss_test 1.0524625411400428 每轮耗时： 2.646859884262085\n","Epochs 90 loss_train 0.8222825194780643 loss_test 2.7039318634913516 每轮耗时： 2.686937093734741\n","Epochs 91 loss_train 0.819651688520725 loss_test 1.5972953484608576 每轮耗时： 2.7033987045288086\n","Epochs 92 loss_train 0.8255732254340098 loss_test 2.38117685684791 每轮耗时： 2.695192575454712\n","Epochs 93 loss_train 0.8196468846156046 loss_test 2.23103260076963 每轮耗时： 2.7111730575561523\n","Epochs 94 loss_train 0.820538750061622 loss_test 1.0501984449533315 每轮耗时： 2.661837339401245\n","Epochs 95 loss_train 0.8197017850784155 loss_test 1.6285796532264123 每轮耗时： 2.647775411605835\n","Epochs 96 loss_train 0.8112330700342472 loss_test 3.209570261148306 每轮耗时： 2.6626718044281006\n","Epochs 97 loss_train 0.8166426420211792 loss_test 1.0937274556893568 每轮耗时： 2.6818835735321045\n","Epochs 98 loss_train 0.8169556701412568 loss_test 1.1694566836723914 每轮耗时： 2.651561975479126\n","Epochs 99 loss_train 0.8136167721106455 loss_test 1.1047089650080755 每轮耗时： 2.640826463699341\n","总耗时: 268.00915575027466\n"]}],"source":["Epochs                = 100      # 训练轮数\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","def seed_torch():\n","    seed=1029\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed) # 为了禁止hash随机化，使得实验可复现\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","seed_torch()\n","model = ResNet1D(p,BasicBlock1D, [2, 2, 2, 2])\n","model = model.to(device)\n","# 提取第一层参数\n","first_layer_params = []\n","other_params = []\n","\n","for name, param in model.named_parameters():\n","    if 'conv1' in name or 'bn1' in name:\n","        first_layer_params.append(param)\n","    else:\n","        other_params.append(param)\n","\n","# 定义两个优化器\n","optimizer_first_layer = AdamL12(first_layer_params, lr=l)\n","optimizer_other_layers = optim.Adam(other_params, lr=l)\n","\n","\n","\n","\n","loss_function = nn.MSELoss()  # loss\n","# optimizer = AdamL12(model.parameters(), lr=l)  # 优化器\n","# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=9, gamma=0.95)\n","LOSS = float('inf')\n","result = {}\n","result['train-loss']= []\n","result['test-loss']= []\n","start_time1 = time.time()\n","for epochs in range(Epochs):\n","    start_time = time.time()\n","    model.train()\n","    train_loss = 0\n","    test_loss  = 0\n","    for data_l in train_loader:\n","        seq, labels = data_l\n","        seq, labels = seq.to(device), labels.to(device)\n","        optimizer_first_layer.zero_grad()                          #    清空过往梯度\n","        optimizer_other_layers.zero_grad()\n","        y_pred = model(seq)\n","        single_loss = loss_function(y_pred, labels)    #    获取loss：输入预测值和标签，计算损失函数\n","        single_loss.backward()                         #    反向传播，计算当前梯度\n","        # optimizer.step()                               #    根据梯度更新网络参数\n","        optimizer_first_layer.step()\n","        optimizer_other_layers.step()\n","        train_loss += single_loss.item()\n","    train_loss = train_loss/len(train_loader)\n","    # scheduler.step()\n","    model.eval()\n","    for data_l in test_loader:\n","        seq, labels = data_l\n","        seq, labels = seq.to(device), labels.to(device)\n","        y_pred = model(seq)\n","        single_loss = loss_function(y_pred, labels)    #    获取loss：输入预测值和标签，计算损失函数\n","        test_loss += single_loss.item()\n","    test_loss = test_loss/len(test_loader)\n","    if LOSS > test_loss:\n","        LOSS =test_loss\n","        torch.save(model, 'model_cnn_pig_data.pth')\n","        print('已更新保存模型')\n","    result['train-loss'].append(train_loss)\n","    result['test-loss'].append(test_loss)\n","    del seq, labels ,y_pred #删除数据与变量\n","    gc.collect() #清除数据与变量相关的缓存\n","    torch.cuda.empty_cache() #缓存分配器分配出去的内存给释放掉\n","    epoch_time = time.time() - start_time\n","    print('Epochs',epochs,'loss_train',train_loss,'loss_test',test_loss,'每轮耗时：',epoch_time)\n","all_time = time.time() - start_time1\n","print('总耗时:',all_time)"]},{"cell_type":"code","execution_count":null,"id":"b363e603-46e2-4005-b455-d041d109dbe3","metadata":{"tags":[],"id":"b363e603-46e2-4005-b455-d041d109dbe3"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.figure(dpi=150,figsize=(7,4))\n","plt.plot(result['train-loss'][:], label='train')\n","plt.plot(result['test-loss'][:], label='test')\n","plt.xlabel('iteration')\n","plt.ylabel('loss')\n","plt.title('Training and Testing Loss')\n","plt.legend()\n","plt.savefig('resnet18loss1.jpg',dpi=150)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"5e5d1ed8-61aa-4914-a89a-332a1ef67084","metadata":{"tags":[],"id":"5e5d1ed8-61aa-4914-a89a-332a1ef67084"},"outputs":[],"source":["def load_model(model_path):\n","\n","    if torch.cuda.is_available():\n","        model = torch.load(model_path)\n","    else:\n","        model = torch.load(model_path, map_location=torch.device('cpu'))\n","\n","    model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n","    return model\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = load_model('model_cnn_pig_data.pth')\n","pre = []\n","for i in X_test:\n","    i = i.unsqueeze(0)\n","    pre.append(model(i.to(device)).cpu().detach().numpy())\n","pre = np.array(pre)\n","pre = np.squeeze(pre)\n","Y_test1 = Y_test.cpu().detach().numpy().copy()\n","for i in range(pre.shape[1]):\n","    pre[:,i] = pre[:,i]*Std[i] +Mean[i]\n","    Y_test1[:,i] = Y_test1[:,i]*Std[i] +Mean[i]\n","    plt.figure(dpi=150,figsize=(7,4))\n","    plt.plot(Y_test1[:,i], label='True Values', color='blue')\n","    plt.plot(pre[:,i], label='Predictions', linestyle='--', color='red')\n","    plt.title('True Values vs Predictions')\n","    plt.xlabel('Data Point')\n","    plt.ylabel('Value')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig('cnn_对比图_{}.jpg'.format(i),dpi=150)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"id":"f6e5cdf9-2e9d-450e-8904-4c3e83e61e09","metadata":{"tags":[],"id":"f6e5cdf9-2e9d-450e-8904-4c3e83e61e09"},"outputs":[],"source":["def Evaluation_index(Y_test1,pre):\n","    from sklearn.metrics import r2_score, mean_squared_error,explained_variance_score,mean_absolute_error\n","    r2 = r2_score(Y_test1,pre)\n","    ev = explained_variance_score(Y_test1,pre)\n","    mse = mean_squared_error(Y_test1,pre)\n","    rmse = np.sqrt(mse)\n","    mae = mean_absolute_error(Y_test1,pre)\n","\n","    pre = pre.reshape(-1)\n","    Y_test1 = Y_test1.reshape(-1)\n","    INDEX = []\n","    page = 0\n","    for i in Y_test1:\n","        if i ==0:\n","            INDEX.append(page)\n","        page +=1\n","    if INDEX !=[]:\n","        Y_test1 = np.delete(Y_test1,INDEX,0)\n","        pre     = np.delete(pre,INDEX,0)\n","    mape = (sum(abs((pre - Y_test1)/(Y_test1)))/len(Y_test1))\n","    evaluation_index = pd.DataFrame()\n","    evaluation_index['评估指标名称'] = ['r2','ev','mse','rmse','mae','mape']\n","    evaluation_index['评估指标值'] = [r2,ev,mse,rmse,mae,mape]\n","    print('r2:',r2)\n","    print('ev:',ev)\n","    print('mse:',mse)\n","    print('rmse:',rmse)\n","    print('mae:',mae)\n","    print('mape:',mape)\n","    return evaluation_index\n","for i in range(pre.shape[1]):\n","    evaluation_index = Evaluation_index(Y_test1[:,i],pre[:,i])\n","    evaluation_index.to_csv(f'evaluation_index_pig_{i}.csv',index = False)\n","    print('-----------------')"]},{"cell_type":"code","execution_count":null,"id":"ef0cfb90","metadata":{"tags":[],"id":"ef0cfb90"},"outputs":[],"source":["evaluation_index = Evaluation_index(Y_test1,pre)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}