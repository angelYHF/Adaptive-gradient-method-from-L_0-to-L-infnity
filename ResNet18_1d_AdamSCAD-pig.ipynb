{"cells":[{"cell_type":"code","execution_count":null,"id":"6d30e714","metadata":{"tags":[],"id":"6d30e714"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import random,os\n","import torch.utils.data as Data\n","from sklearn.metrics import f1_score,recall_score,precision_score,roc_curve,auc,accuracy_score,confusion_matrix,r2_score, mean_squared_error\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import torch\n","# import torch.nn as nn\n","from torch import nn,optim\n","import torch.nn.functional as F\n","from torch.optim.optimizer import Optimizer, required\n","import gc\n","import time\n","import math\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"id":"d579e9d1","metadata":{"id":"d579e9d1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"f02da321","metadata":{"tags":[],"id":"f02da321","outputId":"8c3dedf8-7fe7-4774-d39a-ed5c3e82a785"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([32, 5])\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class BasicBlock1D(nn.Module):\n","    expansion = 1\n","    def __init__(self, in_planes, planes, stride=1, downsample=None):\n","        super(BasicBlock1D, self).__init__()\n","        self.conv1 = nn.Conv1d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm1d(planes)\n","        self.conv2 = nn.Conv1d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm1d(planes)\n","        self.downsample = downsample\n","        self.stride = stride\n","    def forward(self, x):\n","        identity = x\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","        out += identity\n","        out = F.relu(out)\n","        return out\n","class ResNet1D(nn.Module):\n","    def __init__(self, p,block, layers):\n","        super(ResNet1D, self).__init__()\n","        self.in_planes = 2\n","        self.conv1 = nn.Conv1d(1, 2, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.bn1 = nn.BatchNorm1d(2)\n","        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n","        self.layer1 = self._make_layer(block, 4, layers[0])\n","        self.layer2 = self._make_layer(block, 6, layers[1], stride=2)\n","        self.layer3 = self._make_layer(block, 8, layers[2], stride=2)\n","        self.layer4 = self._make_layer(block, 16, layers[3], stride=2)\n","        self.avgpool = nn.AdaptiveAvgPool1d(1)\n","        self.fc1 = nn.Linear(16 * block.expansion, 5)\n","        self.fc2 = nn.Linear(128, 5)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(p)\n","\n","    def _make_layer(self, block, planes, blocks, stride=1):\n","        downsample = None\n","        if stride != 1 or self.in_planes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv1d(self.in_planes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm1d(planes * block.expansion),\n","            )\n","        layers = []\n","        layers.append(block(self.in_planes, planes, stride, downsample))\n","        self.in_planes = planes * block.expansion\n","        for _ in range(1, blocks):\n","            layers.append(block(self.in_planes, planes))\n","        return nn.Sequential(*layers)\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = F.relu(x)\n","        x = self.maxpool(x)\n","        # print(x.shape)\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        # print(x.shape)\n","        x = self.layer4(x)\n","        # print(x.shape)\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        # print(x.shape)\n","        x = self.fc1(x)\n","\n","        # x = self.relu(x)\n","        # x = self.dropout(x)\n","        # x = self.fc2(x)\n","        return x\n","\n","\n","p = 0.01\n","# Example usage:\n","model = ResNet1D(p,BasicBlock1D, [2, 2, 2, 2])\n","input_data = torch.randn(32, 1, 30987)  # 32是batch size\n","output = model(input_data)\n","print(output.shape)\n"]},{"cell_type":"code","execution_count":null,"id":"59c5590e-7af1-45e9-a00c-e5e4fda01033","metadata":{"tags":[],"id":"59c5590e-7af1-45e9-a00c-e5e4fda01033","outputId":"745050e9-7d19-40e3-9764-c5234ada964d"},"outputs":[{"data":{"text/plain":["'/root/1/resnet_1d'"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["os.getcwd()"]},{"cell_type":"code","execution_count":null,"id":"dbf65941","metadata":{"tags":[],"id":"dbf65941","outputId":"13132c73-eae0-47fb-f504-8e73983df166"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_870/725109663.py:3: DtypeWarning: Columns (0,1,2,3,4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv(path_name,header=None)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>52839</th>\n","      <th>52840</th>\n","      <th>52841</th>\n","      <th>52842</th>\n","      <th>52843</th>\n","      <th>52844</th>\n","      <th>52845</th>\n","      <th>52846</th>\n","      <th>52847</th>\n","      <th>52848</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ID</td>\n","      <td>t1</td>\n","      <td>t2</td>\n","      <td>t3</td>\n","      <td>t4</td>\n","      <td>t5</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>...</td>\n","      <td>52834.0</td>\n","      <td>52835.0</td>\n","      <td>52836.0</td>\n","      <td>52837.0</td>\n","      <td>52838.0</td>\n","      <td>52839.0</td>\n","      <td>52840.0</td>\n","      <td>52841.0</td>\n","      <td>52842.0</td>\n","      <td>52843.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1136</td>\n","      <td>-0.8687035</td>\n","      <td>0.28158495</td>\n","      <td>1.71659645</td>\n","      <td>-0.8861064</td>\n","      <td>47.6114438</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>...</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1318</td>\n","      <td>0.13144417</td>\n","      <td>-1.7468354</td>\n","      <td>0.95272593</td>\n","      <td>0.15843581</td>\n","      <td>-41.940212</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1319</td>\n","      <td>-0.8536013</td>\n","      <td>-2.9968045</td>\n","      <td>-0.0871332</td>\n","      <td>-1.2177578</td>\n","      <td>100.653792</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1321</td>\n","      <td>0.03416785</td>\n","      <td>-0.320321</td>\n","      <td>-0.071189</td>\n","      <td>-1.4672803</td>\n","      <td>13.8808255</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2310</th>\n","      <td>6466</td>\n","      <td>0.045294</td>\n","      <td>1.386794</td>\n","      <td>2.291255</td>\n","      <td>0.226449</td>\n","      <td>-23.831346</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2311</th>\n","      <td>6467</td>\n","      <td>-1.199029</td>\n","      <td>-0.565144</td>\n","      <td>2.225259</td>\n","      <td>-5.672833</td>\n","      <td>85.793067</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2312</th>\n","      <td>6469</td>\n","      <td>-0.964449</td>\n","      <td>0.673534</td>\n","      <td>1.754374</td>\n","      <td>-1.489569</td>\n","      <td>-8.871632</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2313</th>\n","      <td>6471</td>\n","      <td>-0.832078</td>\n","      <td>-1.40681</td>\n","      <td>2.368887</td>\n","      <td>-1.517962</td>\n","      <td>51.492272</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2314</th>\n","      <td>6473</td>\n","      <td>0.340885</td>\n","      <td>-0.195353</td>\n","      <td>0.372499</td>\n","      <td>-1.375068</td>\n","      <td>114.904649</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2315 rows × 52849 columns</p>\n","</div>"],"text/plain":["     0           1           2           3           4           5      6      \\\n","0       ID          t1          t2          t3          t4          t5    1.0   \n","1     1136  -0.8687035  0.28158495  1.71659645  -0.8861064  47.6114438    1.0   \n","2     1318  0.13144417  -1.7468354  0.95272593  0.15843581  -41.940212    1.0   \n","3     1319  -0.8536013  -2.9968045  -0.0871332  -1.2177578  100.653792    1.0   \n","4     1321  0.03416785   -0.320321   -0.071189  -1.4672803  13.8808255    1.0   \n","...    ...         ...         ...         ...         ...         ...    ...   \n","2310  6466    0.045294    1.386794    2.291255    0.226449  -23.831346    0.0   \n","2311  6467   -1.199029   -0.565144    2.225259   -5.672833   85.793067    1.0   \n","2312  6469   -0.964449    0.673534    1.754374   -1.489569   -8.871632    2.0   \n","2313  6471   -0.832078    -1.40681    2.368887   -1.517962   51.492272    1.0   \n","2314  6473    0.340885   -0.195353    0.372499   -1.375068  114.904649    0.0   \n","\n","      7      8      9      ...    52839    52840    52841    52842    52843  \\\n","0       2.0    3.0    4.0  ...  52834.0  52835.0  52836.0  52837.0  52838.0   \n","1       2.0    1.0    2.0  ...      2.0      1.0      1.0      2.0      1.0   \n","2       2.0    1.0    0.0  ...      0.0      1.0      1.0      2.0      2.0   \n","3       2.0    1.0    1.0  ...      0.0      1.0      1.0      2.0      0.0   \n","4       2.0    0.0    0.0  ...      1.0      1.0      1.0      2.0      1.0   \n","...     ...    ...    ...  ...      ...      ...      ...      ...      ...   \n","2310    1.0    2.0    1.0  ...      0.0      0.0      1.0      0.0      2.0   \n","2311    2.0    1.0    2.0  ...      1.0      1.0      2.0      0.0      0.0   \n","2312    2.0    2.0    1.0  ...      0.0      1.0      1.0      1.0      1.0   \n","2313    1.0    2.0    1.0  ...      1.0      1.0      2.0      1.0      1.0   \n","2314    2.0    2.0    1.0  ...      1.0      0.0      1.0      1.0      1.0   \n","\n","        52844    52845    52846    52847    52848  \n","0     52839.0  52840.0  52841.0  52842.0  52843.0  \n","1         1.0      0.0      2.0      0.0      1.0  \n","2         2.0      1.0      2.0      1.0      2.0  \n","3         2.0      1.0      2.0      1.0      2.0  \n","4         2.0      1.0      2.0      1.0      2.0  \n","...       ...      ...      ...      ...      ...  \n","2310      2.0      0.0      2.0      1.0      2.0  \n","2311      2.0      0.0      2.0      2.0      2.0  \n","2312      2.0      1.0      2.0      2.0      0.0  \n","2313      2.0      0.0      2.0      0.0      2.0  \n","2314      2.0      2.0      2.0      0.0      2.0  \n","\n","[2315 rows x 52849 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["path1='/root/1/resnet_1d/'\n","path_name = os.path.join(path1,'pig_data.csv')\n","df = pd.read_csv(path_name,header=None)\n","df"]},{"cell_type":"code","execution_count":null,"id":"8a69809d-5ae5-46a8-aefa-903f085df1a7","metadata":{"tags":[],"id":"8a69809d-5ae5-46a8-aefa-903f085df1a7","outputId":"c8ae87ae-9dff-4375-c907-2fe8f8a64550"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>t1</th>\n","      <th>t2</th>\n","      <th>t3</th>\n","      <th>t4</th>\n","      <th>t5</th>\n","      <th>1.0</th>\n","      <th>2.0</th>\n","      <th>3.0</th>\n","      <th>4.0</th>\n","      <th>...</th>\n","      <th>52834.0</th>\n","      <th>52835.0</th>\n","      <th>52836.0</th>\n","      <th>52837.0</th>\n","      <th>52838.0</th>\n","      <th>52839.0</th>\n","      <th>52840.0</th>\n","      <th>52841.0</th>\n","      <th>52842.0</th>\n","      <th>52843.0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1136</td>\n","      <td>-0.8687035</td>\n","      <td>0.28158495</td>\n","      <td>1.71659645</td>\n","      <td>-0.8861064</td>\n","      <td>47.6114438</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>...</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1318</td>\n","      <td>0.13144417</td>\n","      <td>-1.7468354</td>\n","      <td>0.95272593</td>\n","      <td>0.15843581</td>\n","      <td>-41.940212</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1319</td>\n","      <td>-0.8536013</td>\n","      <td>-2.9968045</td>\n","      <td>-0.0871332</td>\n","      <td>-1.2177578</td>\n","      <td>100.653792</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1321</td>\n","      <td>0.03416785</td>\n","      <td>-0.320321</td>\n","      <td>-0.071189</td>\n","      <td>-1.4672803</td>\n","      <td>13.8808255</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1322</td>\n","      <td>0.57391141</td>\n","      <td>-2.0458374</td>\n","      <td>0.054743</td>\n","      <td>0.16402708</td>\n","      <td>-58.738674</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2309</th>\n","      <td>6466</td>\n","      <td>0.045294</td>\n","      <td>1.386794</td>\n","      <td>2.291255</td>\n","      <td>0.226449</td>\n","      <td>-23.831346</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2310</th>\n","      <td>6467</td>\n","      <td>-1.199029</td>\n","      <td>-0.565144</td>\n","      <td>2.225259</td>\n","      <td>-5.672833</td>\n","      <td>85.793067</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2311</th>\n","      <td>6469</td>\n","      <td>-0.964449</td>\n","      <td>0.673534</td>\n","      <td>1.754374</td>\n","      <td>-1.489569</td>\n","      <td>-8.871632</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2312</th>\n","      <td>6471</td>\n","      <td>-0.832078</td>\n","      <td>-1.40681</td>\n","      <td>2.368887</td>\n","      <td>-1.517962</td>\n","      <td>51.492272</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2313</th>\n","      <td>6473</td>\n","      <td>0.340885</td>\n","      <td>-0.195353</td>\n","      <td>0.372499</td>\n","      <td>-1.375068</td>\n","      <td>114.904649</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2314 rows × 52849 columns</p>\n","</div>"],"text/plain":["0       ID          t1          t2          t3          t4          t5  1.0  \\\n","0     1136  -0.8687035  0.28158495  1.71659645  -0.8861064  47.6114438  1.0   \n","1     1318  0.13144417  -1.7468354  0.95272593  0.15843581  -41.940212  1.0   \n","2     1319  -0.8536013  -2.9968045  -0.0871332  -1.2177578  100.653792  1.0   \n","3     1321  0.03416785   -0.320321   -0.071189  -1.4672803  13.8808255  1.0   \n","4     1322  0.57391141  -2.0458374    0.054743  0.16402708  -58.738674  2.0   \n","...    ...         ...         ...         ...         ...         ...  ...   \n","2309  6466    0.045294    1.386794    2.291255    0.226449  -23.831346  0.0   \n","2310  6467   -1.199029   -0.565144    2.225259   -5.672833   85.793067  1.0   \n","2311  6469   -0.964449    0.673534    1.754374   -1.489569   -8.871632  2.0   \n","2312  6471   -0.832078    -1.40681    2.368887   -1.517962   51.492272  1.0   \n","2313  6473    0.340885   -0.195353    0.372499   -1.375068  114.904649  0.0   \n","\n","0     2.0  3.0  4.0  ...  52834.0  52835.0  52836.0  52837.0  52838.0  \\\n","0     2.0  1.0  2.0  ...      2.0      1.0      1.0      2.0      1.0   \n","1     2.0  1.0  0.0  ...      0.0      1.0      1.0      2.0      2.0   \n","2     2.0  1.0  1.0  ...      0.0      1.0      1.0      2.0      0.0   \n","3     2.0  0.0  0.0  ...      1.0      1.0      1.0      2.0      1.0   \n","4     2.0  1.0  1.0  ...      0.0      1.0      2.0      2.0      1.0   \n","...   ...  ...  ...  ...      ...      ...      ...      ...      ...   \n","2309  1.0  2.0  1.0  ...      0.0      0.0      1.0      0.0      2.0   \n","2310  2.0  1.0  2.0  ...      1.0      1.0      2.0      0.0      0.0   \n","2311  2.0  2.0  1.0  ...      0.0      1.0      1.0      1.0      1.0   \n","2312  1.0  2.0  1.0  ...      1.0      1.0      2.0      1.0      1.0   \n","2313  2.0  2.0  1.0  ...      1.0      0.0      1.0      1.0      1.0   \n","\n","0     52839.0  52840.0  52841.0  52842.0  52843.0  \n","0         1.0      0.0      2.0      0.0      1.0  \n","1         2.0      1.0      2.0      1.0      2.0  \n","2         2.0      1.0      2.0      1.0      2.0  \n","3         2.0      1.0      2.0      1.0      2.0  \n","4         2.0      2.0      2.0      2.0      2.0  \n","...       ...      ...      ...      ...      ...  \n","2309      2.0      0.0      2.0      1.0      2.0  \n","2310      2.0      0.0      2.0      2.0      2.0  \n","2311      2.0      1.0      2.0      2.0      0.0  \n","2312      2.0      0.0      2.0      0.0      2.0  \n","2313      2.0      2.0      2.0      0.0      2.0  \n","\n","[2314 rows x 52849 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df.columns = df.iloc[0,:]\n","df = df.iloc[1:,:]\n","df.index = range(len(df))\n","df"]},{"cell_type":"code","execution_count":null,"id":"e40bdb87","metadata":{"tags":[],"id":"e40bdb87","outputId":"f5a70d1d-52e3-4f78-b408-6dffd688251b"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>t1</th>\n","      <th>t2</th>\n","      <th>t3</th>\n","      <th>t4</th>\n","      <th>t5</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.8687035</td>\n","      <td>0.28158495</td>\n","      <td>1.71659645</td>\n","      <td>-0.8861064</td>\n","      <td>47.6114438</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.13144417</td>\n","      <td>-1.7468354</td>\n","      <td>0.95272593</td>\n","      <td>0.15843581</td>\n","      <td>-41.940212</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.8536013</td>\n","      <td>-2.9968045</td>\n","      <td>-0.0871332</td>\n","      <td>-1.2177578</td>\n","      <td>100.653792</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.03416785</td>\n","      <td>-0.320321</td>\n","      <td>-0.071189</td>\n","      <td>-1.4672803</td>\n","      <td>13.8808255</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.57391141</td>\n","      <td>-2.0458374</td>\n","      <td>0.054743</td>\n","      <td>0.16402708</td>\n","      <td>-58.738674</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2309</th>\n","      <td>0.045294</td>\n","      <td>1.386794</td>\n","      <td>2.291255</td>\n","      <td>0.226449</td>\n","      <td>-23.831346</td>\n","    </tr>\n","    <tr>\n","      <th>2310</th>\n","      <td>-1.199029</td>\n","      <td>-0.565144</td>\n","      <td>2.225259</td>\n","      <td>-5.672833</td>\n","      <td>85.793067</td>\n","    </tr>\n","    <tr>\n","      <th>2311</th>\n","      <td>-0.964449</td>\n","      <td>0.673534</td>\n","      <td>1.754374</td>\n","      <td>-1.489569</td>\n","      <td>-8.871632</td>\n","    </tr>\n","    <tr>\n","      <th>2312</th>\n","      <td>-0.832078</td>\n","      <td>-1.40681</td>\n","      <td>2.368887</td>\n","      <td>-1.517962</td>\n","      <td>51.492272</td>\n","    </tr>\n","    <tr>\n","      <th>2313</th>\n","      <td>0.340885</td>\n","      <td>-0.195353</td>\n","      <td>0.372499</td>\n","      <td>-1.375068</td>\n","      <td>114.904649</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2314 rows × 5 columns</p>\n","</div>"],"text/plain":["0             t1          t2          t3          t4          t5\n","0     -0.8687035  0.28158495  1.71659645  -0.8861064  47.6114438\n","1     0.13144417  -1.7468354  0.95272593  0.15843581  -41.940212\n","2     -0.8536013  -2.9968045  -0.0871332  -1.2177578  100.653792\n","3     0.03416785   -0.320321   -0.071189  -1.4672803  13.8808255\n","4     0.57391141  -2.0458374    0.054743  0.16402708  -58.738674\n","...          ...         ...         ...         ...         ...\n","2309    0.045294    1.386794    2.291255    0.226449  -23.831346\n","2310   -1.199029   -0.565144    2.225259   -5.672833   85.793067\n","2311   -0.964449    0.673534    1.754374   -1.489569   -8.871632\n","2312   -0.832078    -1.40681    2.368887   -1.517962   51.492272\n","2313    0.340885   -0.195353    0.372499   -1.375068  114.904649\n","\n","[2314 rows x 5 columns]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["X = df.iloc[:,6:]\n","Y = df.iloc[:,1:6]\n","Y"]},{"cell_type":"code","execution_count":null,"id":"59a67a68-bef8-49ee-9d91-06d8eb28f0db","metadata":{"tags":[],"id":"59a67a68-bef8-49ee-9d91-06d8eb28f0db"},"outputs":[],"source":["Y = Y.astype('float')"]},{"cell_type":"code","execution_count":null,"id":"bff63ca3","metadata":{"tags":[],"id":"bff63ca3","outputId":"c8eec1e3-d299-4d3c-872f-84e5f3fe8ac2"},"outputs":[{"name":"stderr","output_type":"stream","text":["/root/miniconda3/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n"]}],"source":["from sklearn.preprocessing import OneHotEncoder\n","\n","\n","# 初始化OneHotEncoder\n","encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n","\n","# 选择要进行独热编码的列\n","cols_to_encode = df.columns[6:]\n","\n","# 拟合训练集\n","encoder.fit(X)\n","\n","# 对训练集和测试集进行转换\n","X_encoded = encoder.transform(X)\n"]},{"cell_type":"code","execution_count":null,"id":"fbe02c31-de55-48d2-b345-9958b357568e","metadata":{"tags":[],"id":"fbe02c31-de55-48d2-b345-9958b357568e","outputId":"0db53232-d55c-4924-c70c-b8c76cfaa120"},"outputs":[{"data":{"text/plain":["(2314, 279995)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["X_encoded.shape"]},{"cell_type":"code","execution_count":null,"id":"a5d08cd5-7567-4616-b10d-49e53d024b1c","metadata":{"tags":[],"id":"a5d08cd5-7567-4616-b10d-49e53d024b1c"},"outputs":[],"source":["X_encoded = X_encoded.reshape(len(X_encoded),1,-1)"]},{"cell_type":"code","execution_count":null,"id":"368149f5-a89c-4514-b44b-1b090dac53ae","metadata":{"id":"368149f5-a89c-4514-b44b-1b090dac53ae"},"outputs":[],"source":["\n","# # 假设data是一个形状为(N, 30987)的数组\n","# padding = np.zeros((X_encoded.shape[0], 31329 - X_encoded.shape[1]))\n","# data_padded = np.concatenate([X_encoded, padding], axis=1)\n","# # 然后，将数据重塑为(n, 177, 177, 1)\n","# data_reshaped = data_padded.reshape(-1, 1,177, 177)\n","# data_reshaped.shape"]},{"cell_type":"code","execution_count":null,"id":"03d20b9c","metadata":{"tags":[],"id":"03d20b9c"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, Y_train, Y_test = train_test_split(X_encoded,Y , test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"id":"d5b15dab","metadata":{"tags":[],"id":"d5b15dab"},"outputs":[],"source":["def z_score(data):\n","    data = data.astype(float)\n","    Mean = data.mean()\n","    Var = ((data - Mean)**2).mean()\n","    Std = pow(Var,0.5)\n","    data = (data - Mean)/Std  # 标准化\n","    return Mean,Std,data\n","Mean,Std,Y_train = z_score(Y_train)\n","Y_test = (Y_test - Mean)/Std"]},{"cell_type":"code","execution_count":null,"id":"c7fc6a2b","metadata":{"tags":[],"id":"c7fc6a2b"},"outputs":[],"source":["Y_train = Y_train.values\n","Y_test = Y_test.values"]},{"cell_type":"code","execution_count":null,"id":"8e2f2c83","metadata":{"tags":[],"id":"8e2f2c83","outputId":"b32a22e0-3750-47db-9ab6-4a072c0d8e37"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1851, 1, 279995]) torch.Size([1851, 5]) torch.Size([463, 1, 279995]) torch.Size([463, 5])\n"]}],"source":["Y_train = np.squeeze(Y_train)\n","Y_test = np.squeeze(Y_test)\n","\n","X_train = torch.tensor(X_train,dtype = torch.float)\n","Y_train  = torch.tensor(Y_train,dtype = torch.float)\n","\n","X_test = torch.tensor(X_test,dtype = torch.float)\n","Y_test  = torch.tensor(Y_test,dtype = torch.float)\n","print(X_train.shape, Y_train.shape,X_test.shape, Y_test.shape)"]},{"cell_type":"code","execution_count":null,"id":"4a2b0600-2ee9-4e71-8b99-1e3e0ee08007","metadata":{"tags":[],"id":"4a2b0600-2ee9-4e71-8b99-1e3e0ee08007"},"outputs":[],"source":["train_loader = Data.DataLoader(\n","    dataset=Data.TensorDataset(X_train, Y_train),  # 封装进Data.TensorDataset()类的数据，可以为任意维度\n","    batch_size=35,  # 每块的大小\n","    shuffle=True,\n","    drop_last =True, #丢弃最后一组数据\n","    num_workers=0,  # 多进程（multiprocess）来读数据\n",")\n","test_loader = Data.DataLoader(\n","    dataset=Data.TensorDataset(X_test, Y_test),  # 封装进Data.TensorDataset()类的数据，可以为任意维度\n","    batch_size=35,  # 每块的大小\n","    shuffle=False,\n","    drop_last =True,\n","    num_workers=0,\n",")"]},{"cell_type":"code","execution_count":null,"id":"474aa57b-a451-4b3e-88b0-abe6a24d0ca3","metadata":{"id":"474aa57b-a451-4b3e-88b0-abe6a24d0ca3"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"805c74c1-6422-4f9e-821e-405793dd45da","metadata":{"tags":[],"id":"805c74c1-6422-4f9e-821e-405793dd45da"},"outputs":[],"source":["class AdamSCAD(Optimizer):\n","    r\"\"\"Implements Adam optimizer with SCAD (Smoothly Clipped Absolute Deviation) regularization.\n","\n","    Args:\n","        params (iterable): Iterable of parameters to optimize or dicts defining parameter groups.\n","        lr (float, optional): Learning rate. Default: 1e-3\n","        betas (Tuple[float, float], optional): Coefficients used for computing running averages of gradient\n","            and its square. Default: (0.9, 0.999)\n","        eps (float, optional): Term added to the denominator to improve numerical stability. Default: 1e-8\n","        weight_decay (float, optional): Weight decay (L2 penalty). Default: 1e-2\n","        amsgrad (boolean, optional): Whether to use the AMSGrad variant of this algorithm. Default: False\n","        penalty (float, optional): Penalty coefficient for SCAD regularization. Default: 0.0\n","        a (float, optional): SCAD parameter that controls shape of the penalty (a > 2). Default: 3.7\n","    \"\"\"\n","    def __init__(self, params, lr=1e-3, betas=(0.1, 0.999), eps=1e-8,\n","                 weight_decay=1e-2, amsgrad=False, penalty=0.0, a=3.7):\n","        if not 0.0 <= lr:\n","            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n","        if not 0.0 <= eps:\n","            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n","        if not 0.0 <= betas[0] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n","        if not 0.0 <= betas[1] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n","        if a <= 2:\n","            raise ValueError(\"SCAD parameter 'a' must be greater than 2.\")\n","\n","        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n","                        amsgrad=amsgrad, penalty=penalty, a=a)\n","        super(AdamSCAD, self).__init__(params, defaults)\n","\n","    def __setstate__(self, state):\n","        super(AdamSCAD, self).__setstate__(state)\n","        for group in self.param_groups:\n","            group.setdefault(\"amsgrad\", False)\n","\n","    def scad_proximal_operator(self, theta, lambda_val, a):\n","        \"\"\"Applies the SCAD proximal operator on each parameter element.\"\"\"\n","        abs_theta = theta.abs()\n","        sign_theta = theta.sign()\n","\n","        # Region 1: |theta| <= 2 * lambda\n","        mask1 = (abs_theta <= 2 * lambda_val)\n","        theta1 = sign_theta * torch.maximum(abs_theta - lambda_val, torch.zeros_like(theta))\n","\n","        # Region 2: 2 * lambda < |theta| <= a * lambda\n","        mask2 = (abs_theta > 2 * lambda_val) & (abs_theta <= a * lambda_val)\n","        theta2 = ((a - 1) * theta - sign_theta * a * lambda_val) / (a - 2)\n","\n","        # Region 3: |theta| > a * lambda (no regularization)\n","        mask3 = (abs_theta > a * lambda_val)\n","        theta3 = theta\n","\n","        # Combine regions\n","        prox_theta = mask1 * theta1 + mask2 * theta2 + mask3 * theta3\n","        return prox_theta\n","\n","    def step(self, closure=None):\n","        \"\"\"Performs a single optimization step.\"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","\n","                # Perform weight decay\n","                p.data.mul_(1 - group['lr'] * group['weight_decay'])\n","\n","                grad = p.grad.data\n","                if grad.is_sparse:\n","                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n","                amsgrad = group['amsgrad']\n","\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    state['exp_avg'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                    state['exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                    if amsgrad:\n","                        state['max_exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","\n","                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n","                if amsgrad:\n","                    max_exp_avg_sq = state['max_exp_avg_sq']\n","                beta1, beta2 = group['betas']\n","\n","                state['step'] += 1\n","                bias_correction1 = 1 - beta1 ** state['step']\n","                bias_correction2 = 1 - beta2 ** state['step']\n","\n","                # Decay the first and second moment running average coefficient\n","                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n","                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n","                if amsgrad:\n","                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n","                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n","                else:\n","                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n","\n","                step_size = group['lr'] / bias_correction1\n","\n","                # Apply Adam update\n","                p.data.addcdiv_(-step_size, exp_avg, denom)\n","\n","                # Apply SCAD proximal operator if penalty > 0\n","                if group['penalty'] > 0:\n","                    lambda_val = group['penalty'] / denom\n","                    a = group['a']\n","                    p.data = self.scad_proximal_operator(p.data, lambda_val, a)\n","\n","        return loss\n","\n","class AdamL12(Optimizer):\n","    r\"\"\" Implements Adam with L0 regularization\n","\n","    A General Family of Proximal Methods for Stochastic Preconditioned Gradient Descent\n","\n","    This family contains the Adam-type curvature estimate and\n","    L0 (non-convex, non-smooth) regularizer\n","\n","    For this optimizer, the update rule is somewhat adaptive hard-thresholding\n","    \"\"\"\n","    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n","                 weight_decay=1e-2, amsgrad=False, penalty=0.0):\n","        if not 0.0 <= lr:\n","            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n","        if not 0.0 <= eps:\n","            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n","        if not 0.0 <= betas[0] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n","        if not 0.0 <= betas[1] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n","        defaults = dict(lr=lr, betas=betas, eps=eps,\n","                        weight_decay=weight_decay, amsgrad=amsgrad,\n","                        penalty=penalty)\n","        super(AdamL12, self).__init__(params, defaults)\n","\n","    def __setstate__(self, state):\n","        super(AdamL12, self).__setstate__(state)\n","        for group in self.param_groups:\n","            group.setdefault(\"amsgrad\", False)\n","\n","    def step(self, closure=None):\n","        \"\"\" Performs a single optimization step.\n","\n","        Arguments:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","\n","                # Perform weight-decay\n","                p.data.mul_(1 - group['lr'] * group['weight_decay'])\n","\n","                # Perform optimization step\n","                grad = p.grad.data\n","                if grad.is_sparse:\n","                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n","                amsgrad = group['amsgrad']\n","\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    # Exponential moving average of gradient values\n","                    state['exp_avg'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                    # Exponential moving average of squared gradient values\n","                    state['exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                    if amsgrad:\n","                        state['max_exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n","                if amsgrad:\n","                    max_exp_avg_sq = state['max_exp_avg_sq']\n","                beta1, beta2 = group['betas']\n","\n","                state['step'] += 1\n","                bias_correction1 = 1 - beta1 ** state['step']\n","                bias_correction2 = 1 - beta2 ** state['step']\n","\n","                # Decay the first and second moment running average coefficient\n","                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n","                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n","                if amsgrad:\n","                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n","                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n","                else:\n","                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n","\n","                step_size = group['lr'] / bias_correction1\n","\n","                p.data.addcdiv_(-step_size, exp_avg, denom)\n","\n","                if len(p.data.shape) == 2 or len(p.data.shape) == 4:\n","                    threshold = (54 ** (1/3) / 4) * ((2 * group['penalty'] * group['lr'] / denom) ** (2/3))\n","                    mask = p.data.abs() > threshold\n","                    mask = mask.float()\n","\n","                    zero_mask = p.data.abs() <= threshold\n","                    zero_mask = zero_mask.float() + 1e-4\n","\n","                    p.data.mul_(mask)\n","                    factor = (group['lr'] * group['penalty'] / denom) / 4\n","                    angle = factor * (((p.data.abs() + zero_mask) / 3) ** (-1.5))\n","                    angle = angle * mask\n","                    angle = torch.acos(angle)\n","\n","                    value = p.data * (2/3) * (1 + torch.cos(2/3 * (math.pi - angle)))\n","                    p.data = value * mask\n","\n","        return loss\n","\n","class AdamL23(Optimizer):\n","    r\"\"\" Implements Adam with L0 regularization\n","\n","    A General Family of Proximal Methods for Stochastic Preconditioned Gradient Descent\n","\n","    This family contains the Adam-type curvature estimate and\n","    L0 (non-convex, non-smooth) regularizer\n","\n","    For this optimizer, the update rule is somewhat adaptive hard-thresholding\n","    \"\"\"\n","    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n","                 weight_decay=1e-2, amsgrad=False, penalty=0.0):\n","        if not 0.0 <= lr:\n","            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n","        if not 0.0 <= eps:\n","            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n","        if not 0.0 <= betas[0] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n","        if not 0.0 <= betas[1] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n","        defaults = dict(lr=lr, betas=betas, eps=eps,\n","                        weight_decay=weight_decay, amsgrad=amsgrad,\n","                        penalty=penalty)\n","        super(AdamL23, self).__init__(params, defaults)\n","\n","    def __setstate__(self, state):\n","        super(AdamL23, self).__setstate__(state)\n","        for group in self.param_groups:\n","            group.setdefault(\"amsgrad\", False)\n","\n","    def step(self, closure=None):\n","        \"\"\" Performs a single optimization step.\n","\n","        Arguments:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","\n","                # Perform weight-decay\n","                p.data.mul_(1 - group['lr'] * group['weight_decay'])\n","\n","                # Perform optimization step\n","                grad = p.grad.data\n","                if grad.is_sparse:\n","                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n","                amsgrad = group['amsgrad']\n","\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    # Exponential moving average of gradient values\n","                    state['exp_avg'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                    # Exponential moving average of squared gradient values\n","                    state['exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                    if amsgrad:\n","                        state['max_exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n","                if amsgrad:\n","                    max_exp_avg_sq = state['max_exp_avg_sq']\n","                beta1, beta2 = group['betas']\n","\n","                state['step'] += 1\n","                bias_correction1 = 1 - beta1 ** state['step']\n","                bias_correction2 = 1 - beta2 ** state['step']\n","\n","                # Decay the first and second moment running average coefficient\n","                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n","                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n","                if amsgrad:\n","                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n","                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n","                else:\n","                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n","\n","                step_size = group['lr'] / bias_correction1\n","\n","                p.data.addcdiv_(-step_size, exp_avg, denom)\n","\n","                if len(p.data.shape) == 2 or len(p.data.shape) == 4:\n","                    eff_lam = 2 * group['lr'] * group['penalty'] / denom\n","                    threshold = (2/3) * (3 * eff_lam ** 3) ** (1/4)\n","                    mask = p.data.abs() > threshold\n","                    mask = mask.float()\n","\n","                    zero_mask = p.data.abs() <= threshold\n","                    zero_mask = zero_mask.float() * 100\n","\n","                    p.data.mul_(mask)\n","                    angle = acosh((27/16) * (p.data ** 2 + zero_mask) * (eff_lam ** (-1.5)))\n","                    angle = angle * mask\n","                    absA = (2/math.sqrt(3)) * (eff_lam ** (1/4)) * (torch.cosh(angle/3) ** (1/2))\n","\n","                    value = ((absA + torch.sqrt(2 * (p.data.abs() + zero_mask) / absA - absA ** 2)) / 2) ** 3\n","\n","                    p.data = p.data.sign() * value * mask\n","\n","        return loss"]},{"cell_type":"code","execution_count":null,"id":"0b36f3e8-52e3-495b-86ec-09ddd88301a8","metadata":{"tags":[],"id":"0b36f3e8-52e3-495b-86ec-09ddd88301a8","outputId":"84fccdc2-4982-47b2-91cf-246941c866e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["  0%|          | 0/100 [00:00<?, ?trial/s, best loss=?]"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_870/1815375738.py:77: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)\n","  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n","\n"]},{"name":"stdout","output_type":"stream","text":[" 37%|███▋      | 37/100 [2:45:40<4:39:19, 266.03s/trial, best loss: 1.0143726330537062]"]}],"source":["import hyperopt\n","from hyperopt import hp, fmin, tpe, Trials, partial\n","from hyperopt.early_stop import no_progress_loss\n","def seed_torch():\n","    seed=1029\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed) # 为了禁止hash随机化，使得实验可复现\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","def hyperopt_objective(params):\n","    Epochs                = 100      # 训练轮数\n","    l           =params['lr']                     # 步长\n","    p           =params['p']\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","    seed_torch()\n","    model = ResNet1D(p,BasicBlock1D, [2, 2, 2, 2])\n","    model = model.to(device)\n","    # 提取第一层参数\n","    first_layer_params = []\n","    other_params = []\n","\n","    for name, param in model.named_parameters():\n","        if 'conv1' in name or 'bn1' in name:\n","            first_layer_params.append(param)\n","        else:\n","            other_params.append(param)\n","\n","    # 定义两个优化器\n","    optimizer_first_layer = AdamL12(first_layer_params, lr=l)\n","    optimizer_other_layers = optim.Adam(other_params, lr=l)\n","\n","\n","\n","\n","    loss_function = nn.MSELoss()  # loss\n","    # optimizer = AdamL12(model.parameters(), lr=l)  # 优化器\n","    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=9, gamma=0.95)\n","    LOSS = float('inf')\n","    result = {}\n","    result['train-loss']= []\n","    result['test-loss']= []\n","    start_time1 = time.time()\n","    for epochs in range(Epochs):\n","        start_time = time.time()\n","        model.train()\n","        train_loss = 0\n","        test_loss  = 0\n","        for data_l in train_loader:\n","            seq, labels = data_l\n","            seq, labels = seq.to(device), labels.to(device)\n","            optimizer_first_layer.zero_grad()                          #    清空过往梯度\n","            optimizer_other_layers.zero_grad()\n","            y_pred = model(seq)\n","            single_loss = loss_function(y_pred, labels)    #    获取loss：输入预测值和标签，计算损失函数\n","            single_loss.backward()                         #    反向传播，计算当前梯度\n","            # optimizer.step()                               #    根据梯度更新网络参数\n","            optimizer_first_layer.step()\n","            optimizer_other_layers.step()\n","            train_loss += single_loss.item()\n","        train_loss = train_loss/len(train_loader)\n","        # scheduler.step()\n","        model.eval()\n","        for data_l in test_loader:\n","            seq, labels = data_l\n","            seq, labels = seq.to(device), labels.to(device)\n","            y_pred = model(seq)\n","            single_loss = loss_function(y_pred, labels)    #    获取loss：输入预测值和标签，计算损失函数\n","            test_loss += single_loss.item()\n","        test_loss = test_loss/len(test_loader)\n","        if LOSS > test_loss:\n","            LOSS =test_loss\n","            # torch.save(model, 'model_cnn_pig_data.pth')\n","            # print('已更新保存模型')\n","        # result['train-loss'].append(train_loss)\n","        # result['test-loss'].append(test_loss)\n","        del seq, labels ,y_pred #删除数据与变量\n","        gc.collect() #清除数据与变量相关的缓存\n","        torch.cuda.empty_cache() #缓存分配器分配出去的内存给释放掉\n","        epoch_time = time.time() - start_time\n","    #     print('Epochs',epochs,'loss_train',train_loss,'loss_test',test_loss,'每轮耗时：',epoch_time)\n","    # all_time = time.time() - start_time1\n","    # print('总耗时:',all_time)\n","    return LOSS\n","\n","param_grid_simple = {'lr': hp.uniform(\"lr\",0.00005,0.1)\n","                     ,'p':hp.uniform('p',0.01,0.95)\n","                    }\n","def param_hyperopt(max_evals=100):\n","\n","    #保存迭代过程\n","    trials = Trials()\n","\n","    #设置提前停止\n","    early_stop_fn = no_progress_loss(100)\n","\n","    #定义代理模型\n","    #algo = partial(tpe.suggest, n_startup_jobs=20, n_EI_candidates=50)\n","    params_best = fmin(hyperopt_objective #目标函数\n","                       , space = param_grid_simple #参数空间\n","                       , algo = tpe.suggest #代理模型你要哪个呢？\n","                       #, algo = algo\n","                       , max_evals = max_evals #允许的迭代次数\n","                       , verbose=True\n","                       , trials = trials\n","                       , early_stop_fn = early_stop_fn\n","                      )\n","\n","    #打印最优参数，fmin会自动打印最佳分数\n","    print(\"\\n\",\"\\n\",\"best params: \", params_best,\n","          \"\\n\")\n","    return params_best, trials\n","params =  param_hyperopt(max_evals=100)[0]\n","params"]},{"cell_type":"code","execution_count":null,"id":"7a401035-c95f-4845-8194-1de3b4c1291a","metadata":{"tags":[],"id":"7a401035-c95f-4845-8194-1de3b4c1291a"},"outputs":[],"source":["l = params['lr']\n","p = params['p']"]},{"cell_type":"code","execution_count":null,"id":"db8ad935-0180-4f38-9c19-d28ad1f8d502","metadata":{"tags":[],"id":"db8ad935-0180-4f38-9c19-d28ad1f8d502"},"outputs":[],"source":["Epochs                = 100      # 训练轮数\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","def seed_torch():\n","    seed=1029\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed) # 为了禁止hash随机化，使得实验可复现\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","seed_torch()\n","model = ResNet1D(p,BasicBlock1D, [2, 2, 2, 2])\n","model = model.to(device)\n","# 提取第一层参数\n","first_layer_params = []\n","other_params = []\n","\n","for name, param in model.named_parameters():\n","    if 'conv1' in name or 'bn1' in name:\n","        first_layer_params.append(param)\n","    else:\n","        other_params.append(param)\n","\n","# 定义两个优化器\n","optimizer_first_layer = AdamL12(first_layer_params, lr=l)\n","optimizer_other_layers = optim.Adam(other_params, lr=l)\n","\n","\n","\n","\n","loss_function = nn.MSELoss()  # loss\n","# optimizer = AdamL12(model.parameters(), lr=l)  # 优化器\n","# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=9, gamma=0.95)\n","LOSS = float('inf')\n","result = {}\n","result['train-loss']= []\n","result['test-loss']= []\n","start_time1 = time.time()\n","for epochs in range(Epochs):\n","    start_time = time.time()\n","    model.train()\n","    train_loss = 0\n","    test_loss  = 0\n","    for data_l in train_loader:\n","        seq, labels = data_l\n","        seq, labels = seq.to(device), labels.to(device)\n","        optimizer_first_layer.zero_grad()                          #    清空过往梯度\n","        optimizer_other_layers.zero_grad()\n","        y_pred = model(seq)\n","        single_loss = loss_function(y_pred, labels)    #    获取loss：输入预测值和标签，计算损失函数\n","        single_loss.backward()                         #    反向传播，计算当前梯度\n","        # optimizer.step()                               #    根据梯度更新网络参数\n","        optimizer_first_layer.step()\n","        optimizer_other_layers.step()\n","        train_loss += single_loss.item()\n","    train_loss = train_loss/len(train_loader)\n","    # scheduler.step()\n","    model.eval()\n","    for data_l in test_loader:\n","        seq, labels = data_l\n","        seq, labels = seq.to(device), labels.to(device)\n","        y_pred = model(seq)\n","        single_loss = loss_function(y_pred, labels)    #    获取loss：输入预测值和标签，计算损失函数\n","        test_loss += single_loss.item()\n","    test_loss = test_loss/len(test_loader)\n","    if LOSS > test_loss:\n","        LOSS =test_loss\n","        torch.save(model, 'model_cnn_pig_data.pth')\n","        print('已更新保存模型')\n","    result['train-loss'].append(train_loss)\n","    result['test-loss'].append(test_loss)\n","    del seq, labels ,y_pred #删除数据与变量\n","    gc.collect() #清除数据与变量相关的缓存\n","    torch.cuda.empty_cache() #缓存分配器分配出去的内存给释放掉\n","    epoch_time = time.time() - start_time\n","    print('Epochs',epochs,'loss_train',train_loss,'loss_test',test_loss,'每轮耗时：',epoch_time)\n","all_time = time.time() - start_time1\n","print('总耗时:',all_time)"]},{"cell_type":"code","execution_count":null,"id":"b363e603-46e2-4005-b455-d041d109dbe3","metadata":{"tags":[],"id":"b363e603-46e2-4005-b455-d041d109dbe3"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.figure(dpi=150,figsize=(7,4))\n","plt.plot(result['train-loss'][:], label='train')\n","plt.plot(result['test-loss'][:], label='test')\n","plt.xlabel('iteration')\n","plt.ylabel('loss')\n","plt.title('Training and Testing Loss')\n","plt.legend()\n","plt.savefig('resnet18loss1.jpg',dpi=150)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"5e5d1ed8-61aa-4914-a89a-332a1ef67084","metadata":{"tags":[],"id":"5e5d1ed8-61aa-4914-a89a-332a1ef67084"},"outputs":[],"source":["def load_model(model_path):\n","\n","    if torch.cuda.is_available():\n","        model = torch.load(model_path)\n","    else:\n","        model = torch.load(model_path, map_location=torch.device('cpu'))\n","\n","    model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n","    return model\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = load_model('model_cnn_pig_data.pth')\n","pre = []\n","for i in X_test:\n","    i = i.unsqueeze(0)\n","    pre.append(model(i.to(device)).cpu().detach().numpy())\n","pre = np.array(pre)\n","pre = np.squeeze(pre)\n","Y_test1 = Y_test.cpu().detach().numpy().copy()\n","for i in range(pre.shape[1]):\n","    pre[:,i] = pre[:,i]*Std[i] +Mean[i]\n","    Y_test1[:,i] = Y_test1[:,i]*Std[i] +Mean[i]\n","    plt.figure(dpi=150,figsize=(7,4))\n","    plt.plot(Y_test1[:,i], label='True Values', color='blue')\n","    plt.plot(pre[:,i], label='Predictions', linestyle='--', color='red')\n","    plt.title('True Values vs Predictions')\n","    plt.xlabel('Data Point')\n","    plt.ylabel('Value')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig('cnn_对比图_{}.jpg'.format(i),dpi=150)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"id":"f6e5cdf9-2e9d-450e-8904-4c3e83e61e09","metadata":{"tags":[],"id":"f6e5cdf9-2e9d-450e-8904-4c3e83e61e09"},"outputs":[],"source":["def Evaluation_index(Y_test1,pre):\n","    from sklearn.metrics import r2_score, mean_squared_error,explained_variance_score,mean_absolute_error\n","    r2 = r2_score(Y_test1,pre)\n","    ev = explained_variance_score(Y_test1,pre)\n","    mse = mean_squared_error(Y_test1,pre)\n","    rmse = np.sqrt(mse)\n","    mae = mean_absolute_error(Y_test1,pre)\n","\n","    pre = pre.reshape(-1)\n","    Y_test1 = Y_test1.reshape(-1)\n","    INDEX = []\n","    page = 0\n","    for i in Y_test1:\n","        if i ==0:\n","            INDEX.append(page)\n","        page +=1\n","    if INDEX !=[]:\n","        Y_test1 = np.delete(Y_test1,INDEX,0)\n","        pre     = np.delete(pre,INDEX,0)\n","    mape = (sum(abs((pre - Y_test1)/(Y_test1)))/len(Y_test1))\n","    evaluation_index = pd.DataFrame()\n","    evaluation_index['评估指标名称'] = ['r2','ev','mse','rmse','mae','mape']\n","    evaluation_index['评估指标值'] = [r2,ev,mse,rmse,mae,mape]\n","    print('r2:',r2)\n","    print('ev:',ev)\n","    print('mse:',mse)\n","    print('rmse:',rmse)\n","    print('mae:',mae)\n","    print('mape:',mape)\n","    return evaluation_index\n","for i in range(pre.shape[1]):\n","    evaluation_index = Evaluation_index(Y_test1[:,i],pre[:,i])\n","    evaluation_index.to_csv(f'evaluation_index_pig_{i}.csv',index = False)\n","    print('-----------------')"]},{"cell_type":"code","execution_count":null,"id":"ef0cfb90","metadata":{"tags":[],"id":"ef0cfb90"},"outputs":[],"source":["evaluation_index = Evaluation_index(Y_test1,pre)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}