{"cells":[{"cell_type":"code","execution_count":null,"id":"6d30e714","metadata":{"tags":[],"id":"6d30e714"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import random,os\n","import torch.utils.data as Data\n","from sklearn.metrics import f1_score,recall_score,precision_score,roc_curve,auc,accuracy_score,confusion_matrix,r2_score, mean_squared_error\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import torch\n","# import torch.nn as nn\n","from torch import nn,optim\n","import torch.nn.functional as F\n","from torch.optim.optimizer import Optimizer, required\n","import gc\n","import time\n","import math\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"id":"d579e9d1","metadata":{"id":"d579e9d1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"f02da321","metadata":{"id":"f02da321"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"02360f62","metadata":{"tags":[],"id":"02360f62"},"outputs":[],"source":["'''ResNet in PyTorch.\n","\n","For Pre-activation ResNet, see 'preact_resnet.py'.\n","\n","Reference:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","'''\n","\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=2):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv1d(in_planes, planes, kernel_size=1, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm1d(planes)\n","        self.conv2 = nn.Conv1d(planes, planes, kernel_size=1, stride=1, padding=1, bias=False)\n","\n","\n","        self.bn2 = nn.BatchNorm1d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv1d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm1d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        # print('1',out.shape)\n","        out = self.bn2(self.conv2(out))\n","        # print('2',out.shape)\n","        out += self.shortcut(x)\n","        # print('3',out.shape)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv1d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm1d(planes)\n","        self.conv2 = nn.Conv1d(planes, planes, kernel_size=1, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm1d(planes)\n","        self.conv3 = nn.Conv1d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm1d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Con12d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=3):\n","        super(ResNet, self).__init__()\n","        self.input_channel = 1\n","        self.in_planes = 64\n","\n","\n","        self.conv1 = nn.Conv2d(self.input_channel, self.in_planes, kernel_size=1, stride=1, padding=0, bias=False)\n","        self.bn1 = nn.BatchNorm2d(self.in_planes)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*1, num_classes)\n","        self.dropout = nn.Dropout(0.2)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        # print(out.shape)\n","        out = self.layer1(out)\n","        # out = self.dropout(out)\n","        out = self.layer2(out)\n","        # out = self.dropout(out)\n","        out = self.layer3(out)\n","        # out = self.dropout(out)\n","        out = self.layer4(out)\n","        # out = self.dropout(out)\n","        out = F.avg_pool2d(out, 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","    def reg_loss(self):\n","        reg_loss = 0.0\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","                reg_loss += torch.sum(torch.abs(m.weight))\n","        return reg_loss\n","\n","    def l1reg_loss(self):\n","        reg_loss = 0.0\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","                reg_loss += torch.sum(torch.abs(m.weight))\n","        return reg_loss\n","\n","    def l12reg_loss(self):\n","        reg_loss = 0.0\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","                reg_loss += torch.sum((torch.abs(m.weight) + 1e-6).sqrt())\n","        return reg_loss\n","\n","    def l23reg_loss(self):\n","        reg_loss = 0.0\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","                reg_loss += torch.sum((torch.abs(m.weight) + 1e-6).pow(2/3))\n","        return reg_loss\n","\n","    def exact_sparsity(self):\n","        nnz = 0\n","        total_param = 0.0\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","                total_param += np.prod(m.weight.data.shape)\n","                nnz += torch.sum(m.weight.data != 0).detach().item()\n","        ratio = nnz / total_param\n","        return ratio\n","\n","    def sparsity_level(self):\n","        nnz_2 = 0.0\n","        nnz_3 = 0.0\n","        nnz_4 = 0.0\n","        total_param = 0.0\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","                total_param += np.prod(m.weight.data.shape)\n","                nnz_2 += torch.sum(m.weight.data.abs() >= 0.01).detach().item()\n","                nnz_3 += torch.sum(m.weight.data.abs() >= 0.001).detach().item()\n","                nnz_4 += torch.sum(m.weight.data.abs() >= 0.0001).detach().item()\n","        ratio_2 = nnz_2 / total_param\n","        ratio_3 = nnz_3 / total_param\n","        ratio_4 = nnz_4 / total_param\n","        return ratio_2, ratio_3\n","\n","\n","def ResNet18():\n","    return ResNet(BasicBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","def test():\n","    net = ResNet18()\n","    y = net(torch.randn(1,3,32,32))\n","    print(y.size())"]},{"cell_type":"code","execution_count":null,"id":"952a2788-c31b-4de0-9efa-b053b141c3eb","metadata":{"scrolled":true,"tags":[],"id":"952a2788-c31b-4de0-9efa-b053b141c3eb","outputId":"d9c871fa-52e7-4d18-8d1c-74bc3ee38add"},"outputs":[{"name":"stdout","output_type":"stream","text":["1 torch.Size([35, 64, 49, 49])\n","2 torch.Size([35, 64, 49, 49])\n","3 torch.Size([35, 64, 49, 49])\n","1 torch.Size([35, 64, 49, 49])\n","2 torch.Size([35, 64, 49, 49])\n","3 torch.Size([35, 64, 49, 49])\n","1 torch.Size([35, 128, 25, 25])\n","2 torch.Size([35, 128, 25, 25])\n","3 torch.Size([35, 128, 25, 25])\n","1 torch.Size([35, 128, 25, 25])\n","2 torch.Size([35, 128, 25, 25])\n","3 torch.Size([35, 128, 25, 25])\n","1 torch.Size([35, 256, 13, 13])\n","2 torch.Size([35, 256, 13, 13])\n","3 torch.Size([35, 256, 13, 13])\n","1 torch.Size([35, 256, 13, 13])\n","2 torch.Size([35, 256, 13, 13])\n","3 torch.Size([35, 256, 13, 13])\n","1 torch.Size([35, 512, 7, 7])\n","2 torch.Size([35, 512, 7, 7])\n","3 torch.Size([35, 512, 7, 7])\n","1 torch.Size([35, 512, 7, 7])\n","2 torch.Size([35, 512, 7, 7])\n","3 torch.Size([35, 512, 7, 7])\n"]},{"data":{"text/plain":["tensor([[-0.3852, -0.4985],\n","        [-0.5877, -0.7751],\n","        [-0.6015, -0.4879],\n","        [-0.3046, -0.3221],\n","        [-0.4773, -0.5930],\n","        [-0.6225, -0.4416],\n","        [-0.5199, -0.5315],\n","        [-0.5544, -0.3692],\n","        [-0.5210, -0.4015],\n","        [-0.4693, -0.6513],\n","        [-0.5185, -0.4121],\n","        [-0.4849, -0.4991],\n","        [-0.7528, -0.2886],\n","        [-0.6742, -0.5451],\n","        [-0.5200, -0.5804],\n","        [-0.6189, -0.8407],\n","        [-0.3440, -0.4205],\n","        [-0.6133, -0.6977],\n","        [-0.5205, -0.4997],\n","        [-0.6177, -0.5377],\n","        [-0.5300, -0.4375],\n","        [-0.4438, -0.4627],\n","        [-0.5555, -0.4656],\n","        [-0.7416, -0.3221],\n","        [-0.6015, -0.5963],\n","        [-0.3772, -0.4001],\n","        [-0.4069, -0.6790],\n","        [-0.5520, -0.1784],\n","        [-0.6369, -0.5412],\n","        [-0.5030, -0.7022],\n","        [-0.5645, -0.3393],\n","        [-0.7253, -0.5869],\n","        [-0.6004, -0.5232],\n","        [-0.5587, -0.6141],\n","        [-0.3931, -0.4716]], grad_fn=<AddmmBackward0>)"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["net = ResNet18()\n","net(torch.rand([35,1,51,51]))"]},{"cell_type":"code","execution_count":null,"id":"dbf65941","metadata":{"tags":[],"id":"dbf65941","outputId":"6da147ad-8abc-4ea7-ed20-443dcc82322f"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>1273</th>\n","      <th>1274</th>\n","      <th>1275</th>\n","      <th>1276</th>\n","      <th>1277</th>\n","      <th>1278</th>\n","      <th>1279</th>\n","      <th>1280</th>\n","      <th>1281</th>\n","      <th>1282</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.671629</td>\n","      <td>-1.727470</td>\n","      <td>-1.890285</td>\n","      <td>0.050916</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.252703</td>\n","      <td>0.409522</td>\n","      <td>0.309386</td>\n","      <td>-1.738759</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.341815</td>\n","      <td>-0.648626</td>\n","      <td>-0.799559</td>\n","      <td>-1.053569</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.785439</td>\n","      <td>0.093949</td>\n","      <td>0.570468</td>\n","      <td>0.551757</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.998318</td>\n","      <td>-0.282481</td>\n","      <td>1.618682</td>\n","      <td>-0.114285</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>594</th>\n","      <td>-1.248504</td>\n","      <td>1.419042</td>\n","      <td>1.574549</td>\n","      <td>1.659656</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>595</th>\n","      <td>-1.052126</td>\n","      <td>0.082785</td>\n","      <td>2.138355</td>\n","      <td>0.554385</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>596</th>\n","      <td>-1.209974</td>\n","      <td>0.451963</td>\n","      <td>2.022570</td>\n","      <td>1.848717</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>597</th>\n","      <td>-0.902397</td>\n","      <td>0.527907</td>\n","      <td>1.719077</td>\n","      <td>2.692814</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>598</th>\n","      <td>0.222276</td>\n","      <td>-0.542811</td>\n","      <td>-0.304348</td>\n","      <td>0.381317</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>599 rows × 1283 columns</p>\n","</div>"],"text/plain":["         0         1         2         3     4     5     6     7     8     \\\n","0    1.671629 -1.727470 -1.890285  0.050916     0     1     1     1     1   \n","1   -0.252703  0.409522  0.309386 -1.738759     1     1     1     1     1   \n","2    0.341815 -0.648626 -0.799559 -1.053569     1     1     1     1     1   \n","3    0.785439  0.093949  0.570468  0.551757     0     1     1     1     1   \n","4    0.998318 -0.282481  1.618682 -0.114285     0     1     1     1     1   \n","..        ...       ...       ...       ...   ...   ...   ...   ...   ...   \n","594 -1.248504  1.419042  1.574549  1.659656     1     1     1     1     1   \n","595 -1.052126  0.082785  2.138355  0.554385     0     1     1     1     1   \n","596 -1.209974  0.451963  2.022570  1.848717     0     1     1     1     1   \n","597 -0.902397  0.527907  1.719077  2.692814     1     0     1     1     1   \n","598  0.222276 -0.542811 -0.304348  0.381317     1     1     1     1     1   \n","\n","     9     ...  1273  1274  1275  1276  1277  1278  1279  1280  1281  1282  \n","0       0  ...     1     0     1     1     1     0     0     0     0     1  \n","1       0  ...     1     1     0     0     0     0     1     1     1     1  \n","2       0  ...     1     1     0     0     0     0     1     1     1     1  \n","3       0  ...     1     1     1     1     0     0     1     0     1     0  \n","4       0  ...     1     1     1     1     1     0     0     0     1     1  \n","..    ...  ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n","594     0  ...     1     0     0     1     0     0     1     0     1     1  \n","595     0  ...     1     0     1     1     0     0     0     1     0     1  \n","596     1  ...     1     0     1     1     0     0     1     1     1     1  \n","597     0  ...     1     0     1     0     0     0     0     1     1     0  \n","598     1  ...     1     0     1     0     1     0     0     1     1     1  \n","\n","[599 rows x 1283 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["path1='/root/ResNet/'\n","path_name = os.path.join(path1,'wheat_data.csv')\n","df = pd.read_csv(path_name,header=None)\n","df"]},{"cell_type":"code","execution_count":null,"id":"e40bdb87","metadata":{"tags":[],"id":"e40bdb87","outputId":"98d0ebde-461c-4003-8569-c3d2a80c3fba"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-1.727470</td>\n","      <td>-1.890285</td>\n","      <td>0.050916</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.409522</td>\n","      <td>0.309386</td>\n","      <td>-1.738759</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.648626</td>\n","      <td>-0.799559</td>\n","      <td>-1.053569</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.093949</td>\n","      <td>0.570468</td>\n","      <td>0.551757</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.282481</td>\n","      <td>1.618682</td>\n","      <td>-0.114285</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>594</th>\n","      <td>1.419042</td>\n","      <td>1.574549</td>\n","      <td>1.659656</td>\n","    </tr>\n","    <tr>\n","      <th>595</th>\n","      <td>0.082785</td>\n","      <td>2.138355</td>\n","      <td>0.554385</td>\n","    </tr>\n","    <tr>\n","      <th>596</th>\n","      <td>0.451963</td>\n","      <td>2.022570</td>\n","      <td>1.848717</td>\n","    </tr>\n","    <tr>\n","      <th>597</th>\n","      <td>0.527907</td>\n","      <td>1.719077</td>\n","      <td>2.692814</td>\n","    </tr>\n","    <tr>\n","      <th>598</th>\n","      <td>-0.542811</td>\n","      <td>-0.304348</td>\n","      <td>0.381317</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>599 rows × 3 columns</p>\n","</div>"],"text/plain":["            1         2         3\n","0   -1.727470 -1.890285  0.050916\n","1    0.409522  0.309386 -1.738759\n","2   -0.648626 -0.799559 -1.053569\n","3    0.093949  0.570468  0.551757\n","4   -0.282481  1.618682 -0.114285\n","..        ...       ...       ...\n","594  1.419042  1.574549  1.659656\n","595  0.082785  2.138355  0.554385\n","596  0.451963  2.022570  1.848717\n","597  0.527907  1.719077  2.692814\n","598 -0.542811 -0.304348  0.381317\n","\n","[599 rows x 3 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["X = df.iloc[:,4:]\n","Y = df.iloc[:,1:4]\n","Y"]},{"cell_type":"code","execution_count":null,"id":"bff63ca3","metadata":{"tags":[],"id":"bff63ca3","outputId":"67f53c0f-7b8b-4cec-a6d8-24e7ccb0286f"},"outputs":[{"name":"stderr","output_type":"stream","text":["/root/miniconda3/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n"]}],"source":["from sklearn.preprocessing import OneHotEncoder\n","\n","\n","# 初始化OneHotEncoder\n","encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n","\n","# 选择要进行独热编码的列\n","cols_to_encode = df.columns[4:]\n","\n","# 拟合训练集\n","encoder.fit(X)\n","\n","# 对训练集和测试集进行转换\n","X_encoded = encoder.transform(X)\n"]},{"cell_type":"code","execution_count":null,"id":"634c2cd8-1898-4ef5-a5f6-89c18d693184","metadata":{"tags":[],"id":"634c2cd8-1898-4ef5-a5f6-89c18d693184","outputId":"be1c8838-2640-4a61-dca9-acb04459c45b"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>2548</th>\n","      <th>2549</th>\n","      <th>2550</th>\n","      <th>2551</th>\n","      <th>2552</th>\n","      <th>2553</th>\n","      <th>2554</th>\n","      <th>2555</th>\n","      <th>2556</th>\n","      <th>2557</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>594</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>595</th>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>596</th>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>597</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>598</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>599 rows × 2558 columns</p>\n","</div>"],"text/plain":["     0     1     2     3     4     5     6     7     8     9     ...  2548  \\\n","0     1.0   0.0   0.0   1.0   0.0   1.0   0.0   1.0   0.0   1.0  ...   1.0   \n","1     0.0   1.0   0.0   1.0   0.0   1.0   0.0   1.0   0.0   1.0  ...   1.0   \n","2     0.0   1.0   0.0   1.0   0.0   1.0   0.0   1.0   0.0   1.0  ...   1.0   \n","3     1.0   0.0   0.0   1.0   0.0   1.0   0.0   1.0   0.0   1.0  ...   1.0   \n","4     1.0   0.0   0.0   1.0   0.0   1.0   0.0   1.0   0.0   1.0  ...   1.0   \n","..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n","594   0.0   1.0   0.0   1.0   0.0   1.0   0.0   1.0   0.0   1.0  ...   1.0   \n","595   1.0   0.0   0.0   1.0   0.0   1.0   0.0   1.0   0.0   1.0  ...   1.0   \n","596   1.0   0.0   0.0   1.0   0.0   1.0   0.0   1.0   0.0   1.0  ...   1.0   \n","597   0.0   1.0   1.0   0.0   0.0   1.0   0.0   1.0   0.0   1.0  ...   1.0   \n","598   0.0   1.0   0.0   1.0   0.0   1.0   0.0   1.0   0.0   1.0  ...   1.0   \n","\n","     2549  2550  2551  2552  2553  2554  2555  2556  2557  \n","0     0.0   1.0   0.0   1.0   0.0   1.0   0.0   0.0   1.0  \n","1     0.0   0.0   1.0   0.0   1.0   0.0   1.0   0.0   1.0  \n","2     0.0   0.0   1.0   0.0   1.0   0.0   1.0   0.0   1.0  \n","3     0.0   0.0   1.0   1.0   0.0   0.0   1.0   1.0   0.0  \n","4     0.0   1.0   0.0   1.0   0.0   0.0   1.0   0.0   1.0  \n","..    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n","594   0.0   0.0   1.0   1.0   0.0   0.0   1.0   0.0   1.0  \n","595   0.0   1.0   0.0   0.0   1.0   1.0   0.0   0.0   1.0  \n","596   0.0   0.0   1.0   0.0   1.0   0.0   1.0   0.0   1.0  \n","597   0.0   1.0   0.0   0.0   1.0   0.0   1.0   1.0   0.0  \n","598   0.0   1.0   0.0   0.0   1.0   0.0   1.0   0.0   1.0  \n","\n","[599 rows x 2558 columns]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["pd.DataFrame(X_encoded)"]},{"cell_type":"code","execution_count":null,"id":"03b76fb2-a78c-4efd-bb66-c331b7d77d99","metadata":{"tags":[],"id":"03b76fb2-a78c-4efd-bb66-c331b7d77d99"},"outputs":[],"source":["import numpy as np\n","\n","# 假设data是一个形状为(2314, 2558)的数组\n","# 填充数据以使列数为2601\n","padding = np.zeros((X_encoded.shape[0], 2601 - X_encoded.shape[1]))\n","data_padded = np.concatenate([X_encoded, padding], axis=1)\n","\n","# 重塑数据为(2314, 51, 51)\n","data_reshaped = data_padded.reshape(X_encoded.shape[0],1, 51, 51)\n"]},{"cell_type":"code","execution_count":null,"id":"03d20b9c","metadata":{"tags":[],"id":"03d20b9c"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, Y_train, Y_test = train_test_split(data_reshaped,Y , test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"id":"6a833285","metadata":{"id":"6a833285"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"d5b15dab","metadata":{"tags":[],"id":"d5b15dab"},"outputs":[],"source":["def z_score(data):\n","    data = data.astype(float)\n","    Mean = data.mean()\n","    Var = ((data - Mean)**2).mean()\n","    Std = pow(Var,0.5)\n","    data = (data - Mean)/Std  # 标准化\n","    return Mean,Std,data\n","Mean,Std,Y_train = z_score(Y_train)\n","Y_test = (Y_test - Mean)/Std"]},{"cell_type":"code","execution_count":null,"id":"c7fc6a2b","metadata":{"tags":[],"id":"c7fc6a2b"},"outputs":[],"source":["Y_train = Y_train.values\n","Y_test = Y_test.values"]},{"cell_type":"code","execution_count":null,"id":"8e2f2c83","metadata":{"tags":[],"id":"8e2f2c83","outputId":"c9604ea4-cee2-4c36-ace3-34545db79f26"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([479, 1, 51, 51]) torch.Size([479, 3]) torch.Size([120, 1, 51, 51]) torch.Size([120, 3])\n"]}],"source":["Y_train = np.squeeze(Y_train)\n","Y_test = np.squeeze(Y_test)\n","\n","X_train = torch.tensor(X_train,dtype = torch.float)\n","Y_train  = torch.tensor(Y_train,dtype = torch.float)\n","\n","X_test = torch.tensor(X_test,dtype = torch.float)\n","Y_test  = torch.tensor(Y_test,dtype = torch.float)\n","print(X_train.shape, Y_train.shape,X_test.shape, Y_test.shape)"]},{"cell_type":"code","execution_count":null,"id":"4a2b0600-2ee9-4e71-8b99-1e3e0ee08007","metadata":{"tags":[],"id":"4a2b0600-2ee9-4e71-8b99-1e3e0ee08007"},"outputs":[],"source":["train_loader = Data.DataLoader(\n","    dataset=Data.TensorDataset(X_train, Y_train),  # 封装进Data.TensorDataset()类的数据，可以为任意维度\n","    batch_size=35,  # 每块的大小\n","    shuffle=True,\n","    drop_last =True, #丢弃最后一组数据\n","    num_workers=0,  # 多进程（multiprocess）来读数据\n",")\n","test_loader = Data.DataLoader(\n","    dataset=Data.TensorDataset(X_test, Y_test),  # 封装进Data.TensorDataset()类的数据，可以为任意维度\n","    batch_size=35,  # 每块的大小\n","    shuffle=False,\n","    drop_last =True,\n","    num_workers=0,\n",")"]},{"cell_type":"code","execution_count":null,"id":"805c74c1-6422-4f9e-821e-405793dd45da","metadata":{"tags":[],"id":"805c74c1-6422-4f9e-821e-405793dd45da"},"outputs":[],"source":["class AdamL12(Optimizer):\n","    r\"\"\" Implements Adam with L0 regularization\n","\n","    A General Family of Proximal Methods for Stochastic Preconditioned Gradient Descent\n","\n","    This family contains the Adam-type curvature estimate and\n","    L0 (non-convex, non-smooth) regularizer\n","\n","    For this optimizer, the update rule is somewhat adaptive hard-thresholding\n","    \"\"\"\n","    def __init__(self, params, lr=1e-3, betas=(0.1, 0.999), eps=1e-8,\n","                 weight_decay=1e-2, amsgrad=False, penalty=0.0):\n","        if not 0.0 <= lr:\n","            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n","        if not 0.0 <= eps:\n","            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n","        if not 0.0 <= betas[0] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n","        if not 0.0 <= betas[1] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n","        defaults = dict(lr=lr, betas=betas, eps=eps,\n","                        weight_decay=weight_decay, amsgrad=amsgrad,\n","                        penalty=penalty)\n","        super(AdamL12, self).__init__(params, defaults)\n","\n","    def __setstate__(self, state):\n","        super(AdamL12, self).__setstate__(state)\n","        for group in self.param_groups:\n","            group.setdefault(\"amsgrad\", False)\n","\n","    def step(self, closure=None):\n","        \"\"\" Performs a single optimization step.\n","\n","        Arguments:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","\n","                # Perform weight-decay\n","                p.data.mul_(1 - group['lr'] * group['weight_decay'])\n","\n","                # Perform optimization step\n","                grad = p.grad.data\n","                if grad.is_sparse:\n","                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n","                amsgrad = group['amsgrad']\n","\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    # Exponential moving average of gradient values\n","                    state['exp_avg'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                    # Exponential moving average of squared gradient values\n","                    state['exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                    if amsgrad:\n","                        state['max_exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n","                if amsgrad:\n","                    max_exp_avg_sq = state['max_exp_avg_sq']\n","                beta1, beta2 = group['betas']\n","\n","                state['step'] += 1\n","                bias_correction1 = 1 - beta1 ** state['step']\n","                bias_correction2 = 1 - beta2 ** state['step']\n","\n","                # Decay the first and second moment running average coefficient\n","                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n","                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n","                if amsgrad:\n","                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n","                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n","                else:\n","                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n","\n","                step_size = group['lr'] / bias_correction1\n","\n","                p.data.addcdiv_(-step_size, exp_avg, denom)\n","\n","                if len(p.data.shape) == 2 or len(p.data.shape) == 4:\n","                    threshold = (54 ** (1/3) / 4) * ((2 * group['penalty'] * group['lr'] / denom) ** (2/3))\n","                    mask = p.data.abs() > threshold\n","                    mask = mask.float()\n","\n","                    zero_mask = p.data.abs() <= threshold\n","                    zero_mask = zero_mask.float() + 1e-4\n","\n","                    p.data.mul_(mask)\n","                    factor = (group['lr'] * group['penalty'] / denom) / 4\n","                    angle = factor * (((p.data.abs() + zero_mask) / 3) ** (-1.5))\n","                    angle = angle * mask\n","                    angle = torch.acos(angle)\n","\n","                    value = p.data * (2/3) * (1 + torch.cos(2/3 * (math.pi - angle)))\n","                    p.data = value * mask\n","\n","        return loss\n","\n","class AdamL23(Optimizer):\n","    r\"\"\" Implements Adam with L0 regularization\n","\n","    A General Family of Proximal Methods for Stochastic Preconditioned Gradient Descent\n","\n","    This family contains the Adam-type curvature estimate and\n","    L0 (non-convex, non-smooth) regularizer\n","\n","    For this optimizer, the update rule is somewhat adaptive hard-thresholding\n","    \"\"\"\n","    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n","                 weight_decay=1e-2, amsgrad=False, penalty=0.0):\n","        if not 0.0 <= lr:\n","            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n","        if not 0.0 <= eps:\n","            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n","        if not 0.0 <= betas[0] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n","        if not 0.0 <= betas[1] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n","        defaults = dict(lr=lr, betas=betas, eps=eps,\n","                        weight_decay=weight_decay, amsgrad=amsgrad,\n","                        penalty=penalty)\n","        super(AdamL23, self).__init__(params, defaults)\n","\n","    def __setstate__(self, state):\n","        super(AdamL23, self).__setstate__(state)\n","        for group in self.param_groups:\n","            group.setdefault(\"amsgrad\", False)\n","\n","    def step(self, closure=None):\n","        \"\"\" Performs a single optimization step.\n","\n","        Arguments:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","\n","                # Perform weight-decay\n","                p.data.mul_(1 - group['lr'] * group['weight_decay'])\n","\n","                # Perform optimization step\n","                grad = p.grad.data\n","                if grad.is_sparse:\n","                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n","                amsgrad = group['amsgrad']\n","\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    # Exponential moving average of gradient values\n","                    state['exp_avg'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                    # Exponential moving average of squared gradient values\n","                    state['exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                    if amsgrad:\n","                        state['max_exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n","                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n","                if amsgrad:\n","                    max_exp_avg_sq = state['max_exp_avg_sq']\n","                beta1, beta2 = group['betas']\n","\n","                state['step'] += 1\n","                bias_correction1 = 1 - beta1 ** state['step']\n","                bias_correction2 = 1 - beta2 ** state['step']\n","\n","                # Decay the first and second moment running average coefficient\n","                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n","                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n","                if amsgrad:\n","                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n","                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n","                else:\n","                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n","\n","                step_size = group['lr'] / bias_correction1\n","\n","                p.data.addcdiv_(-step_size, exp_avg, denom)\n","\n","                if len(p.data.shape) == 2 or len(p.data.shape) == 4:\n","                    eff_lam = 2 * group['lr'] * group['penalty'] / denom\n","                    threshold = (2/3) * (3 * eff_lam ** 3) ** (1/4)\n","                    mask = p.data.abs() > threshold\n","                    mask = mask.float()\n","\n","                    zero_mask = p.data.abs() <= threshold\n","                    zero_mask = zero_mask.float() * 100\n","\n","                    p.data.mul_(mask)\n","                    angle = acosh((27/16) * (p.data ** 2 + zero_mask) * (eff_lam ** (-1.5)))\n","                    angle = angle * mask\n","                    absA = (2/math.sqrt(3)) * (eff_lam ** (1/4)) * (torch.cosh(angle/3) ** (1/2))\n","\n","                    value = ((absA + torch.sqrt(2 * (p.data.abs() + zero_mask) / absA - absA ** 2)) / 2) ** 3\n","\n","                    p.data = p.data.sign() * value * mask\n","\n","        return loss"]},{"cell_type":"code","execution_count":null,"id":"e2984400-11e9-45e0-83bf-2e776dc0812d","metadata":{"tags":[],"id":"e2984400-11e9-45e0-83bf-2e776dc0812d","outputId":"42c59689-4a6a-47ce-dead-e601885656b0"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_15063/1815375738.py:77: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)\n","  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n"]},{"name":"stdout","output_type":"stream","text":["已更新保存模型\n","Epochs 0 loss_train 6.998524248600006 loss_test 114.35025024414062 每轮耗时： 0.6194684505462646\n","已更新保存模型\n","Epochs 1 loss_train 1.032635693366711 loss_test 1.5154844125111897 每轮耗时： 0.46654295921325684\n","已更新保存模型\n","Epochs 2 loss_train 1.005527477998 loss_test 1.023868242899577 每轮耗时： 0.4851210117340088\n","Epochs 3 loss_train 1.0233662036749034 loss_test 1.1288366715113323 每轮耗时： 0.38991475105285645\n","Epochs 4 loss_train 0.9974801402825576 loss_test 1.0503966013590496 每轮耗时： 0.396268367767334\n","已更新保存模型\n","Epochs 5 loss_train 0.960750570664039 loss_test 0.9933320681254069 每轮耗时： 0.47130513191223145\n","Epochs 6 loss_train 0.9200809781367962 loss_test 1.0224789182345073 每轮耗时： 0.3910651206970215\n","Epochs 7 loss_train 0.8244694540133843 loss_test 1.0071086287498474 每轮耗时： 0.369856595993042\n","Epochs 8 loss_train 0.7400380602249732 loss_test 1.9174148639043171 每轮耗时： 0.3264918327331543\n","已更新保存模型\n","Epochs 9 loss_train 0.6114327105192038 loss_test 0.9592364430427551 每轮耗时： 0.40979743003845215\n","Epochs 10 loss_train 0.5011874322707837 loss_test 1.7953084707260132 每轮耗时： 0.33757567405700684\n","已更新保存模型\n","Epochs 11 loss_train 0.47337841529112595 loss_test 0.9492153525352478 每轮耗时： 0.40921974182128906\n","Epochs 12 loss_train 0.42156405173815215 loss_test 0.9972525437672933 每轮耗时： 0.34634852409362793\n","Epochs 13 loss_train 0.37920214579655576 loss_test 1.014352520306905 每轮耗时： 0.34386658668518066\n","Epochs 14 loss_train 0.3174589517024847 loss_test 1.0543232957522075 每轮耗时： 0.35707950592041016\n","Epochs 15 loss_train 0.3055724364060622 loss_test 1.1255202293395996 每轮耗时： 0.34705257415771484\n","已更新保存模型\n","Epochs 16 loss_train 0.31058081411398375 loss_test 0.9474819898605347 每轮耗时： 0.4422159194946289\n","Epochs 17 loss_train 0.30482216064746565 loss_test 1.1546583771705627 每轮耗时： 0.3746359348297119\n","Epochs 18 loss_train 0.26323398603842807 loss_test 1.2744622627894084 每轮耗时： 0.3729360103607178\n","Epochs 19 loss_train 0.22305864898058084 loss_test 0.9562501907348633 每轮耗时： 0.3714268207550049\n","Epochs 20 loss_train 0.19641572752824196 loss_test 1.039360225200653 每轮耗时： 0.37383270263671875\n","Epochs 21 loss_train 0.16307444068101737 loss_test 1.3175595204035442 每轮耗时： 0.38315415382385254\n","Epochs 22 loss_train 0.16568915316691765 loss_test 1.0893577337265015 每轮耗时： 0.40853381156921387\n","Epochs 23 loss_train 0.17836100436173952 loss_test 1.0248976945877075 每轮耗时： 0.3301832675933838\n","Epochs 24 loss_train 0.15032633995780578 loss_test 1.2514880498250325 每轮耗时： 0.3351292610168457\n","Epochs 25 loss_train 0.14631432180221265 loss_test 1.2717809279759724 每轮耗时： 0.35127854347229004\n","Epochs 26 loss_train 0.13989245203825143 loss_test 1.1631855169932048 每轮耗时： 0.355243444442749\n","Epochs 27 loss_train 0.09918597627144593 loss_test 1.0785422722498577 每轮耗时： 0.38945984840393066\n","Epochs 28 loss_train 0.099910808297304 loss_test 1.2820767561594646 每轮耗时： 0.3928563594818115\n","Epochs 29 loss_train 0.08493343855325992 loss_test 1.1404392719268799 每轮耗时： 0.390472412109375\n","Epochs 30 loss_train 0.07709246444014403 loss_test 1.0392610430717468 每轮耗时： 0.3949875831604004\n","Epochs 31 loss_train 0.06707998393819882 loss_test 1.0837088425954182 每轮耗时： 0.3986358642578125\n","Epochs 32 loss_train 0.09672749873537284 loss_test 1.142371416091919 每轮耗时： 0.3857715129852295\n","Epochs 33 loss_train 0.08850803656073716 loss_test 1.1117757558822632 每轮耗时： 0.39245152473449707\n","Epochs 34 loss_train 0.06658008092871079 loss_test 1.1242488622665405 每轮耗时： 0.4005138874053955\n","Epochs 35 loss_train 0.05186806733791645 loss_test 1.0155089696248372 每轮耗时： 0.39412379264831543\n","Epochs 36 loss_train 0.052090087332404576 loss_test 1.0657430092493694 每轮耗时： 0.33245420455932617\n","Epochs 37 loss_train 0.049642680785976924 loss_test 1.1545000871022542 每轮耗时： 0.3427615165710449\n","Epochs 38 loss_train 0.05791948979290632 loss_test 1.1126091877619426 每轮耗时： 0.36190342903137207\n","Epochs 39 loss_train 0.06703247903631283 loss_test 1.0619126160939534 每轮耗时： 0.3425908088684082\n","Epochs 40 loss_train 0.053134833534176536 loss_test 1.0406427582105 每轮耗时： 0.34492063522338867\n","Epochs 41 loss_train 0.048822102638391346 loss_test 1.0429560939470928 每轮耗时： 0.40616464614868164\n","Epochs 42 loss_train 0.04355915879400877 loss_test 0.9892809391021729 每轮耗时： 0.3336453437805176\n","Epochs 43 loss_train 0.04626088231228865 loss_test 1.5510243972142537 每轮耗时： 0.34682583808898926\n","Epochs 44 loss_train 0.06195682731385414 loss_test 1.110514760017395 每轮耗时： 0.3567523956298828\n","Epochs 45 loss_train 0.09599107245986278 loss_test 1.1795122226079304 每轮耗时： 0.3550376892089844\n","Epochs 46 loss_train 0.08286078331562188 loss_test 1.3850957155227661 每轮耗时： 0.3225440979003906\n","Epochs 47 loss_train 0.061928701801941946 loss_test 0.995742936929067 每轮耗时： 0.33823633193969727\n","Epochs 48 loss_train 0.05798390192481188 loss_test 1.0680841207504272 每轮耗时： 0.3524749279022217\n","Epochs 49 loss_train 0.07636975525663449 loss_test 0.988181988398234 每轮耗时： 0.36756181716918945\n","Epochs 50 loss_train 0.07267219372666799 loss_test 1.2783604462941487 每轮耗时： 0.3464186191558838\n","Epochs 51 loss_train 0.08955132474119847 loss_test 1.0297834674517314 每轮耗时： 0.31986093521118164\n","Epochs 52 loss_train 0.04932848550379276 loss_test 1.0832631587982178 每轮耗时： 0.3244960308074951\n","Epochs 53 loss_train 0.040269836640128724 loss_test 1.204704483350118 每轮耗时： 0.34865236282348633\n","Epochs 54 loss_train 0.04062345817398567 loss_test 1.101609706878662 每轮耗时： 0.3379557132720947\n","Epochs 55 loss_train 0.039821583920946486 loss_test 1.1014848947525024 每轮耗时： 0.3300023078918457\n","Epochs 56 loss_train 0.04715492862921495 loss_test 1.097453276316325 每轮耗时： 0.3720238208770752\n","Epochs 57 loss_train 0.03444804967595981 loss_test 1.0591127077738445 每轮耗时： 0.32745957374572754\n","Epochs 58 loss_train 0.05469517123240691 loss_test 1.0804918607076008 每轮耗时： 0.33193111419677734\n","Epochs 59 loss_train 0.05114912141401034 loss_test 1.0614304542541504 每轮耗时： 0.3343074321746826\n","Epochs 60 loss_train 0.03549618646502495 loss_test 1.046832025051117 每轮耗时： 0.32348060607910156\n","Epochs 61 loss_train 0.03618153313604685 loss_test 1.0400912761688232 每轮耗时： 0.32193970680236816\n","Epochs 62 loss_train 0.04326146153303293 loss_test 1.1359002590179443 每轮耗时： 0.33386898040771484\n","Epochs 63 loss_train 0.03885362025063772 loss_test 1.0563098589579265 每轮耗时： 0.34465885162353516\n","Epochs 64 loss_train 0.034534538666216225 loss_test 1.064769983291626 每轮耗时： 0.36407971382141113\n","Epochs 65 loss_train 0.027744004144691504 loss_test 1.0269301136334736 每轮耗时： 0.34980130195617676\n","Epochs 66 loss_train 0.04073597254374853 loss_test 1.078421950340271 每轮耗时： 0.3287527561187744\n","Epochs 67 loss_train 0.04523365572094917 loss_test 0.9895897308985392 每轮耗时： 0.339202880859375\n","Epochs 68 loss_train 0.025879330646533232 loss_test 1.0392676790555317 每轮耗时： 0.3511204719543457\n","Epochs 69 loss_train 0.02984048404659216 loss_test 1.0568526983261108 每轮耗时： 0.34334826469421387\n","Epochs 70 loss_train 0.030390858506927125 loss_test 1.006365994612376 每轮耗时： 0.33147263526916504\n","Epochs 71 loss_train 0.04250589156380066 loss_test 1.137333591779073 每轮耗时： 0.33623409271240234\n","Epochs 72 loss_train 0.051422957330942154 loss_test 1.0208772818247478 每轮耗时： 0.35182952880859375\n","Epochs 73 loss_train 0.04005993429857951 loss_test 1.0543463230133057 每轮耗时： 0.3543112277984619\n","Epochs 74 loss_train 0.0404339310211631 loss_test 1.206408937772115 每轮耗时： 0.3379809856414795\n","Epochs 75 loss_train 0.03013379957813483 loss_test 1.0405728022257488 每轮耗时： 0.3422574996948242\n","Epochs 76 loss_train 0.028389090815415748 loss_test 1.1252241134643555 每轮耗时： 0.3329486846923828\n","Epochs 77 loss_train 0.023131356144753788 loss_test 0.9978139599164327 每轮耗时： 0.3636906147003174\n","Epochs 78 loss_train 0.02895679038304549 loss_test 1.0260848601659138 每轮耗时： 0.34110379219055176\n","Epochs 79 loss_train 0.0274677574634552 loss_test 1.0722258687019348 每轮耗时： 0.34183406829833984\n","Epochs 80 loss_train 0.03165818765186346 loss_test 1.1228065490722656 每轮耗时： 0.33547139167785645\n","Epochs 81 loss_train 0.024610386588252507 loss_test 1.0246888796488445 每轮耗时： 0.3342165946960449\n","Epochs 82 loss_train 0.024709907145454332 loss_test 1.0473108688990276 每轮耗时： 0.3489241600036621\n","Epochs 83 loss_train 0.027457316764272176 loss_test 1.0253289739290874 每轮耗时： 0.3309042453765869\n","Epochs 84 loss_train 0.024006034104296796 loss_test 1.0391069650650024 每轮耗时： 0.3308398723602295\n","Epochs 85 loss_train 0.027765316172288015 loss_test 1.0554815133412678 每轮耗时： 0.34558916091918945\n","Epochs 86 loss_train 0.028024333959015515 loss_test 1.0662389596303303 每轮耗时： 0.33725404739379883\n","Epochs 87 loss_train 0.03212750860704826 loss_test 0.9801643093427023 每轮耗时： 0.33405470848083496\n","Epochs 88 loss_train 0.02518945810599969 loss_test 1.0538758039474487 每轮耗时： 0.34020256996154785\n","Epochs 89 loss_train 0.030789191333147194 loss_test 1.0594271818796794 每轮耗时： 0.34065675735473633\n","Epochs 90 loss_train 0.029327128655635394 loss_test 0.9876965483029684 每轮耗时： 0.3351902961730957\n","Epochs 91 loss_train 0.022686404462617177 loss_test 1.057357370853424 每轮耗时： 0.33542704582214355\n","Epochs 92 loss_train 0.028704044194175646 loss_test 1.0138289332389832 每轮耗时： 0.35399913787841797\n","Epochs 93 loss_train 0.02736726489204627 loss_test 1.0415032704671223 每轮耗时： 0.3446834087371826\n","Epochs 94 loss_train 0.02416798142859569 loss_test 1.0709292888641357 每轮耗时： 0.3628873825073242\n","Epochs 95 loss_train 0.030445122374938086 loss_test 1.012388567129771 每轮耗时： 0.3787555694580078\n","Epochs 96 loss_train 0.025951376901223108 loss_test 1.0037482380867004 每轮耗时： 0.3570103645324707\n","Epochs 97 loss_train 0.018247810049125783 loss_test 1.0235681931177776 每轮耗时： 0.3687617778778076\n","Epochs 98 loss_train 0.017131074474981196 loss_test 1.0214788516362507 每轮耗时： 0.3798253536224365\n","Epochs 99 loss_train 0.014004750535465203 loss_test 1.0008240540822346 每轮耗时： 0.3617057800292969\n","总耗时: 36.1870014667511\n"]}],"source":["Epochs                = 100      # 训练轮数\n","l                     = 0.01\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","def seed_torch():\n","    seed=1029\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed) # 为了禁止hash随机化，使得实验可复现\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","seed_torch()\n","model = ResNet18()\n","model = model.to(device)\n","loss_function = nn.MSELoss()  # loss\n","optimizer = AdamL12(model.parameters(), lr=l)  # 优化器\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=9, gamma=0.95)\n","LOSS = float('inf')\n","result = {}\n","result['train-loss']= []\n","result['test-loss']= []\n","start_time1 = time.time()\n","for epochs in range(Epochs):\n","    start_time = time.time()\n","    model.train()\n","    train_loss = 0\n","    test_loss  = 0\n","    for data_l in train_loader:\n","        seq, labels = data_l\n","        seq, labels = seq.to(device), labels.to(device)\n","        optimizer.zero_grad()                          #    清空过往梯度\n","        y_pred = model(seq)\n","        single_loss = loss_function(y_pred, labels)    #    获取loss：输入预测值和标签，计算损失函数\n","        single_loss.backward()                         #    反向传播，计算当前梯度\n","        optimizer.step()                               #    根据梯度更新网络参数\n","        train_loss += single_loss.item()\n","    train_loss = train_loss/len(train_loader)\n","    scheduler.step()\n","    model.eval()\n","    for data_l in test_loader:\n","        seq, labels = data_l\n","        seq, labels = seq.to(device), labels.to(device)\n","        y_pred = model(seq)\n","        single_loss = loss_function(y_pred, labels)    #    获取loss：输入预测值和标签，计算损失函数\n","        test_loss += single_loss.item()\n","    test_loss = test_loss/len(test_loader)\n","    if LOSS > test_loss:\n","        LOSS =test_loss\n","        torch.save(model, 'model_cnn3.pth')\n","        print('已更新保存模型')\n","    result['train-loss'].append(train_loss)\n","    result['test-loss'].append(test_loss)\n","    del seq, labels ,y_pred #删除数据与变量\n","    gc.collect() #清除数据与变量相关的缓存\n","    torch.cuda.empty_cache() #缓存分配器分配出去的内存给释放掉\n","    epoch_time = time.time() - start_time\n","    print('Epochs',epochs,'loss_train',train_loss,'loss_test',test_loss,'每轮耗时：',epoch_time)\n","all_time = time.time() - start_time1\n","print('总耗时:',all_time)"]},{"cell_type":"code","execution_count":null,"id":"b363e603-46e2-4005-b455-d041d109dbe3","metadata":{"tags":[],"id":"b363e603-46e2-4005-b455-d041d109dbe3"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.figure(dpi=150,figsize=(7,4))\n","plt.plot(result['train-loss'][:], label='train')\n","plt.plot(result['test-loss'][:], label='test')\n","plt.xlabel('iteration')\n","plt.ylabel('loss')\n","plt.title('Training and Testing Loss')\n","plt.legend()\n","plt.savefig('resnet18loss3.jpg',dpi=150)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"591ad040-6a5f-417c-9da2-5b5957f77ded","metadata":{"tags":[],"id":"591ad040-6a5f-417c-9da2-5b5957f77ded"},"outputs":[],"source":["\n","def load_model(model_path):\n","\n","    if torch.cuda.is_available():\n","        model = torch.load(model_path)\n","    else:\n","        model = torch.load(model_path, map_location=torch.device('cpu'))\n","\n","    model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n","    return model\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = load_model('model_cnn3.pth')\n","pre = model(X_test.to(device))\n","pre = pre.cpu().detach().numpy()\n","Y_test1 = Y_test.cpu().detach().numpy().copy()\n","for i in range(pre.shape[1]):\n","    pre[:,i] = pre[:,i]*Std.values[i] +Mean.values[i]\n","    Y_test1[:,i] = Y_test1[:,i]*Std.values[i] +Mean.values[i]\n","    plt.figure(dpi=150,figsize=(7,4))\n","    plt.plot(Y_test1[:,i], label='True Values', color='blue')\n","    plt.plot(pre[:,i], label='Predictions', linestyle='--', color='red')\n","    plt.title('True Values vs Predictions')\n","    plt.xlabel('Data Point')\n","    plt.ylabel('Value')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig('cnn_对比图_{}_3.jpg'.format(i),dpi=150)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"id":"f6e5cdf9-2e9d-450e-8904-4c3e83e61e09","metadata":{"tags":[],"id":"f6e5cdf9-2e9d-450e-8904-4c3e83e61e09"},"outputs":[],"source":["def Evaluation_index(Y_test1,pre):\n","    from sklearn.metrics import r2_score, mean_squared_error,explained_variance_score,mean_absolute_error\n","    r2 = r2_score(Y_test1,pre)\n","    ev = explained_variance_score(Y_test1,pre)\n","    mse = mean_squared_error(Y_test1,pre)\n","    rmse = np.sqrt(mse)\n","    mae = mean_absolute_error(Y_test1,pre)\n","\n","    pre = pre.reshape(-1)\n","    Y_test1 = Y_test1.reshape(-1)\n","    INDEX = []\n","    page = 0\n","    for i in Y_test1:\n","        if i ==0:\n","            INDEX.append(page)\n","        page +=1\n","    if INDEX !=[]:\n","        Y_test1 = np.delete(Y_test1,INDEX,0)\n","        pre     = np.delete(pre,INDEX,0)\n","    mape = (sum(abs((pre - Y_test1)/(Y_test1)))/len(Y_test1))\n","    evaluation_index = pd.DataFrame()\n","    evaluation_index['评估指标名称'] = ['r2','ev','mse','rmse','mae','mape']\n","    evaluation_index['评估指标值'] = [r2,ev,mse,rmse,mae,mape]\n","    print('r2:',r2)\n","    print('ev:',ev)\n","    print('mse:',mse)\n","    print('rmse:',rmse)\n","    print('mae:',mae)\n","    print('mape:',mape)\n","    return evaluation_index\n","for i in range(3):\n","    evaluation_index = Evaluation_index(Y_test1[:,i],pre[:,i])\n","    evaluation_index.to_csv(f'evaluation_index{i}_3.csv',index = False)\n","    print('-----------------')"]},{"cell_type":"code","execution_count":null,"id":"edda82e3-ed71-4c80-b0c5-927f2717d8b7","metadata":{"id":"edda82e3-ed71-4c80-b0c5-927f2717d8b7"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"7cfa6612-37ac-49d8-8431-8049dc66bf64","metadata":{"id":"7cfa6612-37ac-49d8-8431-8049dc66bf64"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"ef0cfb90","metadata":{"id":"ef0cfb90"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}